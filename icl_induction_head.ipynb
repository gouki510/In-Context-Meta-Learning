{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4cb8GADCuFl_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzF5h6iNugUD"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pcrkWn3UuPgm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import einops\n",
        "\n",
        "\n",
        "class Unembed(nn.Module):\n",
        "    def __init__(self, d_vocab, d_model):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(d_vocab, d_model) / np.sqrt(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.einsum(\"pe,bse->bsp\", self.W, x)\n",
        "\n",
        "\n",
        "class HookPoint(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fwd_hooks = []\n",
        "        self.bwd_hooks = []\n",
        "\n",
        "    def give_name(self, name):\n",
        "        # Called by the model at initialisation\n",
        "        self.name = name\n",
        "\n",
        "    def add_hook(self, hook, dir=\"fwd\"):\n",
        "        # Hook format is fn(activation, hook_name)\n",
        "        # Change it into PyTorch hook format (this includes input and output,\n",
        "        # which are the same for a HookPoint)\n",
        "        def full_hook(module, module_input, module_output):\n",
        "            return hook(module_output, name=self.name)\n",
        "\n",
        "        if dir == \"fwd\":\n",
        "            handle = self.register_forward_hook(full_hook)\n",
        "            self.fwd_hooks.append(handle)\n",
        "        elif dir == \"bwd\":\n",
        "            handle = self.register_backward_hook(full_hook)\n",
        "            self.bwd_hooks.append(handle)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid direction {dir}\")\n",
        "\n",
        "    def remove_hooks(self, dir=\"fwd\"):\n",
        "        if (dir == \"fwd\") or (dir == \"both\"):\n",
        "            for hook in self.fwd_hooks:\n",
        "                hook.remove()\n",
        "            self.fwd_hooks = []\n",
        "        if (dir == \"bwd\") or (dir == \"both\"):\n",
        "            for hook in self.bwd_hooks:\n",
        "                hook.remove()\n",
        "            self.bwd_hooks = []\n",
        "        if dir not in [\"fwd\", \"bwd\", \"both\"]:\n",
        "            raise ValueError(f\"Invalid direction {dir}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "class PosEmbed(nn.Module):\n",
        "    def __init__(self, max_ctx, d_model, weight_scale=1):\n",
        "        super().__init__()\n",
        "        self.W_pos = nn.Parameter(torch.randn(max_ctx, d_model) * weight_scale)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.W_pos[: x.shape[-2]]\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, epsilon=1e-4, model=[None]):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.w_ln = nn.Parameter(torch.ones(d_model))\n",
        "        self.b_ln = nn.Parameter(torch.zeros(d_model))\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.model[0].use_ln:\n",
        "            x = x - x.mean(axis=-1)[..., None]\n",
        "            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n",
        "            x = x * self.w_ln\n",
        "            x = x + self.b_ln\n",
        "            return x\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "# Attention\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    b : batch size\n",
        "    d : embedding size of token\n",
        "    p : vocabraly size (113 or 3)\n",
        "    i : number of heads\n",
        "    h : embedding size of each heads\n",
        "    n_ctx : token size\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, d_head, n_ctx):\n",
        "        super().__init__()\n",
        "        self.W_K = nn.Parameter(\n",
        "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.W_Q = nn.Parameter(\n",
        "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.W_V = nn.Parameter(\n",
        "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.W_O = nn.Parameter(\n",
        "            torch.randn(d_model, d_head * num_heads) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones((n_ctx, n_ctx))))\n",
        "        self.d_head = d_head\n",
        "\n",
        "    def forward(self, x):\n",
        "        k = torch.einsum(\"ihd,bpd->biph\", self.W_K, x)\n",
        "        q = torch.einsum(\"ihd,bpd->biph\", self.W_Q, x)\n",
        "        v = torch.einsum(\"ihd,bpd->biph\", self.W_V, x)\n",
        "        attn_scores_pre = torch.einsum(\"biph,biqh->biqp\", k, q)\n",
        "        attn_scores_masked = torch.tril(attn_scores_pre) - 1e10 * (\n",
        "            1 - self.mask[: x.shape[-2], : x.shape[-2]]\n",
        "        )\n",
        "        attn_matrix = F.softmax(attn_scores_masked / np.sqrt(self.d_head), dim=-1)\n",
        "        z = torch.einsum(\"biph,biqp->biqh\", v, attn_matrix)\n",
        "        z_flat = einops.rearrange(z, \"b i q h -> b q (i h)\")\n",
        "        out = torch.einsum(\"df,bqf->bqd\", self.W_O, z_flat)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, d_in, d_out, act_type, weight_scale=1):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(d_out, d_in))\n",
        "        torch.nn.init.normal_(self.W, mean=0, std=weight_scale / np.sqrt(d_in))\n",
        "\n",
        "    def set_weight_ratio(self, weight_ratio):\n",
        "        self.W = nn.Parameter(self.W * weight_ratio)\n",
        "\n",
        "    def set_weight_ratio_l2(self, weight_ratio):\n",
        "        self.W = nn.Parameter(self.W * torch.sqrt(weight_ratio))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x @ self.W.T\n",
        "\n",
        "\n",
        "# for Transformer\n",
        "class MLPBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    b : batch size\n",
        "    d : embedding size of token\n",
        "    p : vocabraly size (114 or 3)\n",
        "    i : number of heads\n",
        "    h : embedding size of each heads\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_mlp, act_type):\n",
        "        super().__init__()\n",
        "        # bias & layer norm are removed.\n",
        "        self.W_in = nn.Parameter(torch.randn(d_mlp, d_model) / np.sqrt(d_model))\n",
        "        self.b_in = nn.Parameter(torch.zeros(d_mlp))\n",
        "        self.W_out = nn.Parameter(torch.randn(d_model, d_mlp) / np.sqrt(d_model))\n",
        "        self.b_out = nn.Parameter(torch.zeros(d_model))\n",
        "        self.act_type = act_type\n",
        "        # self.ln = LayerNorm(d_mlp, model=self.model)\n",
        "        assert act_type in [\"ReLU\", \"GeLU\"]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.einsum(\"md,bpd->bpm\", self.W_in, x) + self.b_in\n",
        "        if self.act_type == \"ReLU\":\n",
        "            x = F.relu(x)\n",
        "        elif self.act_type == \"GeLU\":\n",
        "            x = F.gelu(x)\n",
        "        x = torch.einsum(\"dm,bpm->bpd\", self.W_out, x) + self.b_out\n",
        "        return x\n",
        "\n",
        "    def set_weight_ratio(self, weight_ratio):\n",
        "        self.W_in = nn.Parameter(self.W_in * weight_ratio)\n",
        "        self.W_out = nn.Parameter(self.W_out * weight_ratio)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    b : batch size\n",
        "    d : embedding size of token\n",
        "    p : vocabraly size\n",
        "    i : number of heads\n",
        "    h : embedding size of each heads\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n",
        "        super().__init__()\n",
        "        # self.ln1 = LayerNorm(d_model, model=self.model)\n",
        "        self.model = model\n",
        "        self.attn = Attention(d_model, num_heads, d_head, n_ctx)\n",
        "        # self.ln2 = LayerNorm(d_model, model=self.model)\n",
        "        self.mlp = MLPBlock(d_model, d_mlp, act_type)\n",
        "        self.layer_norm = LayerNorm(d_model, model=self.model)\n",
        "        self.hook_attn_out = HookPoint()\n",
        "        self.hook_mlp_out = HookPoint()\n",
        "        self.hook_resid_pre = HookPoint()\n",
        "        self.hook_resid_mid = HookPoint()\n",
        "        self.hook_resid_post = HookPoint()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hook_resid_mid(\n",
        "            x + self.hook_attn_out(self.attn((self.hook_resid_pre(x))))\n",
        "        )\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.hook_resid_post(x + self.hook_mlp_out(self.mlp((x))))\n",
        "        return x\n",
        "\n",
        "    def set_weight_ratio(self, weight_ratio):\n",
        "        self.attn.set_weight_ratio(weight_ratio)\n",
        "        self.mlp.set_weight_ratio(weight_ratio)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HCuWL5OHHW2n"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class InputEmbedder(nn.Module):\n",
        "    \"\"\"Input embedder.\"\"\"\n",
        "\n",
        "    def __init__(self, conf):\n",
        "\n",
        "        \"\"\"Initialize the input embedder.\n",
        "\n",
        "    Args:\n",
        "      num_classes: Total number of output classes.\n",
        "      emb_dim: Dimensionality of example and label embeddings.\n",
        "      example_encoding: How to encode example inputs.\n",
        "        'resnet': simple resnet encoding\n",
        "        'linear': flatten and pass through a linear layer\n",
        "        'embedding': pass through an embedding layer\n",
        "      flatten_superpixels: Whether to flatten the output of the resnet (instead\n",
        "        of taking a mean over superpixels).\n",
        "      example_dropout_prob: Dropout probability on example embeddings. Note that\n",
        "        these are applied at both train and test.\n",
        "      concatenate_labels: Whether to concatenate example and label embeddings\n",
        "        into one token for each (example, label) pair, rather than being fed to\n",
        "        the transformer as two separate tokens.\n",
        "      use_positional_encodings: Whether to use positional encoding.\n",
        "      positional_dropout_prob: Positional dropout probability.\n",
        "      name: Optional name for the module.\n",
        "    \"\"\"\n",
        "        super(InputEmbedder, self).__init__()\n",
        "        self._num_classes = conf.num_classes\n",
        "        self._emb_dim = conf.d_emb\n",
        "        self.p_dim = conf.p_dim\n",
        "\n",
        "        self.Emb = nn.Linear(self._emb_dim, self._emb_dim)\n",
        "\n",
        "        self.label_embs = nn.Parameter(\n",
        "            torch.randn(self._num_classes, self._emb_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, examples, labels, is_training=True):\n",
        "        \"\"\"Call to the input embedder.\n",
        "\n",
        "        Args:\n",
        "          examples: input sequence of shape\n",
        "            [batch_size, seq_len, height, width, channels]\n",
        "          labels: input sequence of shape [batch_size, seq_len]\n",
        "          is_training: if is currently training.\n",
        "\n",
        "        Returns:\n",
        "          outputs: output of the transformer tower\n",
        "            of shape [batch_size, seq_len, channels].\n",
        "        \"\"\"\n",
        "        # Encode the example inputs into shape (B, SS, E)\n",
        "        B, SS, D = examples.shape\n",
        "        # pos encoding\n",
        "        pos_enc = F.one_hot(torch.arange(SS), num_classes=self.p_dim).repeat(32,1,1).to(examples.device)\n",
        "        h_example = torch.cat([examples, pos_enc], dim=2)\n",
        "\n",
        "        # Embed the labels.\n",
        "        n_emb_classes = self._num_classes\n",
        "        labels_to_embed = labels\n",
        "        embs = self.get_parameter(\"label_embs\")\n",
        "        h_label = embs[labels_to_embed]  # (B, SS, E)\n",
        "        hh = torch.empty(\n",
        "            (h_example.shape[0], h_example.shape[1] * 2 - 1, h_example.shape[2]),\n",
        "            dtype=h_example.dtype,\n",
        "        ).to(h_example.device)\n",
        "        hh[:, 0::2] = h_example\n",
        "        hh[:, 1::2] = h_label[:, :-1]\n",
        "\n",
        "        return hh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TOZd6k4mu2nB"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedder, config):\n",
        "        super().__init__()\n",
        "        num_layers = config.num_layers\n",
        "        d_model = config.d_emb\n",
        "        d_mlp = config.d_emb * 4\n",
        "        d_head = config.d_emb // config.num_heads\n",
        "        num_heads = config.num_heads\n",
        "        n_ctx = config.n_ctx\n",
        "        act_type = config.act_type\n",
        "        use_cache = config.use_cache\n",
        "        use_ln = config.use_ln\n",
        "        self.cache = {}\n",
        "        self.use_cache = use_cache\n",
        "        d_vocab = config.num_classes\n",
        "\n",
        "        self.embedder = embedder\n",
        "        # self.pos_embed = PosEmbed(n_ctx, d_model)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model=[self]\n",
        "                )\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        # self.ln = LayerNorm(d_model, model=[self])\n",
        "        self.unembed = Unembed(d_vocab, d_model)\n",
        "        self.use_ln = use_ln\n",
        "\n",
        "        for name, module in self.named_modules():\n",
        "            if type(module) == HookPoint:\n",
        "                module.give_name(name)\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        x = self.embedder(x, labels,)\n",
        "        # x = self.pos_embed(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.unembed(x)\n",
        "        return x\n",
        "\n",
        "    def set_use_cache(self, use_cache):\n",
        "        self.use_cache = use_cache\n",
        "\n",
        "    def hook_points(self):\n",
        "        return [module for name, module in self.named_modules() if \"hook\" in name]\n",
        "\n",
        "    def remove_all_hooks(self):\n",
        "        for hp in self.hook_points():\n",
        "            hp.remove_hooks(\"fwd\")\n",
        "            hp.remove_hooks(\"bwd\")\n",
        "\n",
        "    def cache_all(self, cache, incl_bwd=False):\n",
        "        # Caches all activations wrapped in a HookPoint\n",
        "        def save_hook(tensor, name):\n",
        "            cache[name] = tensor.detach()\n",
        "\n",
        "        def save_hook_back(tensor, name):\n",
        "            cache[name + \"_grad\"] = tensor[0].detach()\n",
        "\n",
        "        for hp in self.hook_points():\n",
        "            hp.add_hook(save_hook, \"fwd\")\n",
        "            if incl_bwd:\n",
        "                hp.add_hook(save_hook_back, \"bwd\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "6iNOyJRwwV1h"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
        "\n",
        "class SamplingDataset(object):\n",
        "  def __init__(self,conf):\n",
        "    self.num_classes = conf.num_classes\n",
        "    self.dim = conf.dim\n",
        "    self.num_labels = conf.num_labels\n",
        "    self.eps = conf.eps\n",
        "    self.alpha = conf.alpha\n",
        "    self.ways = conf.ways\n",
        "    self.p_bursty = conf.p_bursty\n",
        "    self.data_type = conf.data_type # \"bursty\", no_support, holdout, flip\n",
        "    self.mu, self.labels = self._get_data()\n",
        "\n",
        "  def _get_data(self):\n",
        "    mu = torch.normal(mean=0, std=1/self.dim, size=(self.num_classes,self.dim))\n",
        "    labels = torch.randint(self.num_labels, size=(self.num_classes,1))\n",
        "    return mu, labels\n",
        "\n",
        "class SamplingLoader(DataLoader):\n",
        "\n",
        "  def __init__(self,conf):\n",
        "    self.dataset = SamplingDataset(conf)\n",
        "    self.mu, self.labels = self.dataset._get_data()\n",
        "    self.data_type = conf.data_type\n",
        "    self.num_seq = conf.num_seq\n",
        "    self.alpha = conf.alpha\n",
        "    self.num_classes = conf.num_classes\n",
        "    self.num_labels = conf.num_labels\n",
        "    self.ways = conf.ways\n",
        "    self.p_bursty = conf.p_bursty\n",
        "    self.eps = conf.eps\n",
        "    self.dim = conf.dim\n",
        "    self.num_holdout_classes = conf.num_holdout_classes\n",
        "    self.holdout_classes = np.arange(self.num_classes-self.num_holdout_classes, self.num_classes)\n",
        "    assert self.num_seq % self.ways ==0\n",
        "    prob = np.array([1/(k+1)**self.alpha for k in range(self.num_classes-self.num_holdout_classes)])\n",
        "    self.prob = prob/prob.sum()\n",
        "\n",
        "  def get_seq(self):\n",
        "    while True:\n",
        "      if self.data_type==\"bursty\":\n",
        "        if self.p_bursty > np.random.rand():\n",
        "          # choise few shot example\n",
        "          num_few_shot_class = self.num_seq//self.ways\n",
        "          few_shot_class = np.random.choice(self.num_classes-self.num_holdout_classes, num_few_shot_class, replace=False)\n",
        "          mus = self.mu[few_shot_class]\n",
        "          mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
        "          labels = self.labels[few_shot_class]\n",
        "          labels = np.repeat(labels, self.ways, axis=0) # expand ways\n",
        "          classes = np.repeat(few_shot_class, self.ways)\n",
        "          # add noise\n",
        "          x = self.add_noise(mus)\n",
        "          # permutation shuffle\n",
        "          ordering = np.random.permutation(self.num_seq)\n",
        "          x = x[ordering]\n",
        "          labels = labels[ordering]\n",
        "          classes = classes[ordering]\n",
        "          # select query labels\n",
        "          query_class = np.random.choice(few_shot_class, 1)\n",
        "          query_label = self.labels[query_class]\n",
        "          query_mu = self.mu[query_class]\n",
        "          query_x = self.add_noise(query_mu)\n",
        "          # concat\n",
        "          x = torch.cat([x, query_x])\n",
        "          labels = torch.cat([labels.flatten(), torch.tensor(query_label).flatten()])\n",
        "          yield {\n",
        "              \"examples\":x.to(torch.float32),\n",
        "              \"labels\":labels,\n",
        "              \"classes\" : torch.cat([torch.tensor(classes).flatten(), torch.tensor(query_class).flatten()])\n",
        "          }\n",
        "        else:\n",
        "          # rank frequency\n",
        "          classes = np.random.choice(self.num_classes-self.num_holdout_classes, self.num_seq+1, p=self.prob)\n",
        "          mus = self.mu[classes]\n",
        "          labels = self.labels[classes]\n",
        "          x = self.add_noise(mus)\n",
        "          # permutation shuffle\n",
        "          ordering = np.random.permutation(self.num_seq+1)\n",
        "          x = x[ordering]\n",
        "          labels = labels[ordering]\n",
        "          classes = classes[ordering]\n",
        "\n",
        "          yield {\n",
        "              \"examples\":x.to(torch.float32),\n",
        "              \"labels\":labels.flatten(),\n",
        "              \"classes\" : torch.tensor(classes)\n",
        "          }\n",
        "\n",
        "      elif self.data_type == \"no_support\":\n",
        "          # rank frequency\n",
        "          classes = np.random.choice(self.num_classes-self.num_holdout_classes, self.num_seq+1, p=self.prob, replace=False)\n",
        "          mus = self.mu[classes]\n",
        "          labels = self.labels[classes]\n",
        "          x = self.add_noise(mus)\n",
        "          # permutation shuffle\n",
        "          ordering = np.random.permutation(self.num_seq+1)\n",
        "          x = x[ordering]\n",
        "          labels = labels[ordering]\n",
        "          classes = classes[ordering]\n",
        "\n",
        "          yield {\n",
        "              \"examples\":x.to(torch.float32),\n",
        "              \"labels\":labels.flatten(),\n",
        "              \"classes\" : torch.tensor(classes)\n",
        "          }\n",
        "          \n",
        "      elif self.data_type == \"holdout\":\n",
        "          # rank frequency\n",
        "          classes = np.random.choice(self.holdout_classes, self.num_seq)\n",
        "          mus = self.mu[classes]\n",
        "          labels = self.labels[classes]\n",
        "          x = self.add_noise(mus)\n",
        "          # permutation shuffle\n",
        "          ordering = np.random.permutation(self.num_seq)\n",
        "          x = x[ordering]\n",
        "          labels = labels[ordering]\n",
        "          classes = classes[ordering]\n",
        "          # query\n",
        "          query_class = np.random.choice(classes, 1)\n",
        "          query_label = self.labels[query_class]\n",
        "          query_mu = self.mu[query_class]\n",
        "          query_x = self.add_noise(query_mu)\n",
        "          # concat\n",
        "          x = torch.cat([x, query_x])\n",
        "          labels = torch.cat([labels.flatten(), torch.tensor(query_label).flatten()])\n",
        "\n",
        "          yield {\n",
        "              \"examples\":x.to(torch.float32),\n",
        "              \"labels\":labels,\n",
        "              \"classes\" : torch.cat([torch.tensor(classes).flatten(), torch.tensor(query_class)])\n",
        "          }\n",
        "\n",
        "      elif self.data_type == \"flip\":\n",
        "                # rank frequency\n",
        "          classes = np.random.choice(self.num_classes, self.num_seq)\n",
        "          mus = self.mu[classes]\n",
        "          # label flip\n",
        "          labels = (self.labels[classes] + 1) % self.num_labels\n",
        "          x = self.add_noise(mus)\n",
        "          # permutation shuffle\n",
        "          ordering = np.random.permutation(self.num_seq)\n",
        "          x = x[ordering]\n",
        "          labels = labels[ordering]\n",
        "          classes = classes[ordering]\n",
        "          # query\n",
        "          query_class = np.random.choice(classes, 1)\n",
        "          query_label = (self.labels[query_class]+1) % self.num_labels\n",
        "          query_mu = self.mu[query_class]\n",
        "          query_x = self.add_noise(query_mu)\n",
        "          # concat\n",
        "          x = torch.cat([x, query_x])\n",
        "          labels = torch.cat([labels.flatten(), torch.tensor(query_label).flatten()])\n",
        "          yield {\n",
        "              \"examples\":x.to(torch.float32),\n",
        "              \"labels\":labels.flatten(),\n",
        "              \"classes\" : torch.cat([torch.tensor(classes).flatten(), torch.tensor(query_class)])\n",
        "          }\n",
        "\n",
        "  def add_noise(self, x):\n",
        "    x = (x+self.eps*np.random.normal(0, 1/self.dim, size=x.shape))/(np.sqrt(1+self.eps**2))\n",
        "    return x\n",
        "\n",
        "class IterDataset(IterableDataset):\n",
        "    def __init__(self, generator):\n",
        "        self.generator = generator\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.generator()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "gX3jnutBzfVe"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, asdict\n",
        "@dataclass\n",
        "class TransformerConfig:\n",
        "        num_layers: int = 2\n",
        "        d_vocab: int = 32\n",
        "        d_model: int = 128\n",
        "        d_mlp: int = 128\n",
        "        d_head: int = 128\n",
        "        num_heads: int = 1\n",
        "        n_ctx: int = (8*2+1)\n",
        "        act_type: str = \"ReLU\"\n",
        "        use_cache: bool = False\n",
        "        use_ln: bool = True\n",
        "        p_dim: int = 65\n",
        "        d_emb: int = 128\n",
        "        num_classes = 32\n",
        "\n",
        "@dataclass\n",
        "class TrainDataConfig:\n",
        "  num_classes: int = 512\n",
        "  dim: int = 63\n",
        "  num_labels: int = 32\n",
        "  eps: float = 0.1\n",
        "  alpha: float = 0\n",
        "  ways: int = 2\n",
        "  p_bursty: float = 1\n",
        "  data_type: str = \"bursty\" # bursty, holdout, no_support, flip\n",
        "  num_seq: int = 8\n",
        "  num_holdout_classes: int = 10\n",
        "\n",
        "@dataclass\n",
        "class IWLDataConfig:\n",
        "  num_classes: int = 512\n",
        "  dim: int = 63\n",
        "  num_labels: int = 32\n",
        "  eps: float = 0.1\n",
        "  alpha: float = 0\n",
        "  ways: int = 2\n",
        "  p_bursty: float = 1\n",
        "  data_type: str = \"no_support\" # bursty, holdout, no_support, flip\n",
        "  num_seq: int = 8\n",
        "  num_holdout_classes: int = 10\n",
        "\n",
        "@dataclass\n",
        "class ICLDataConfig:\n",
        "  num_classes: int = 512\n",
        "  dim: int = 63\n",
        "  num_labels: int = 32\n",
        "  eps: float = 0.1\n",
        "  alpha: float = 0\n",
        "  ways: int = 2\n",
        "  p_bursty: float = 1\n",
        "  data_type: str = \"holdout\" # bursty, holdout, no_support, flip\n",
        "  num_seq: int = 8\n",
        "  num_holdout_classes: int = 10\n",
        "\n",
        "@dataclass\n",
        "class ICL2DataConfig:\n",
        "  num_classes: int = 512\n",
        "  dim: int = 63\n",
        "  num_labels: int = 32\n",
        "  eps: float = 0.1\n",
        "  alpha: float = 0\n",
        "  ways: int = 2\n",
        "  p_bursty: float = 1\n",
        "  data_type: str = \"flip\" # bursty, holdout, no_support, flip\n",
        "  num_seq: int = 8\n",
        "  num_holdout_classes: int = 10\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "  batch_size: int = 32\n",
        "  optimize_step: int = int(5e3)\n",
        "  lr: float = 0.01\n",
        "  optimizer: str = \"sgd\"\n",
        "  eval_every: int = 100\n",
        "  eval_steps: int = 100\n",
        "\n",
        "@dataclass\n",
        "class MainConfig:\n",
        "  traindataconfig : TrainDataConfig = TrainDataConfig()\n",
        "  icldataconfig: ICLDataConfig = ICLDataConfig()\n",
        "  iwldataconfig: IWLDataConfig = IWLDataConfig()\n",
        "  icl2dataconfig: ICL2DataConfig = ICL2DataConfig()\n",
        "  modelconfig: TransformerConfig = TransformerConfig()\n",
        "  trainconfig: TrainConfig = TrainConfig()\n",
        "# define config\n",
        "traindataconfig = TrainDataConfig()\n",
        "icldataconfig = ICLDataConfig()\n",
        "iwldataconfig = IWLDataConfig()\n",
        "icl2dataconfig = ICL2DataConfig()\n",
        "modelconfig = TransformerConfig()\n",
        "trainconfig = TrainConfig()\n",
        "\n",
        "# data\n",
        "trainloader = SamplingLoader(traindataconfig)\n",
        "train_seq_generator = trainloader.get_seq\n",
        "train_dataset = IterDataset(train_seq_generator)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=trainconfig.batch_size)\n",
        "\n",
        "iclloader = SamplingLoader(icldataconfig)\n",
        "icl_seq_generator = iclloader.get_seq\n",
        "icl_dataset = IterDataset(icl_seq_generator)\n",
        "icl_dataloader = torch.utils.data.DataLoader(icl_dataset, batch_size=trainconfig.batch_size)\n",
        "\n",
        "iwlloader = SamplingLoader(iwldataconfig)\n",
        "iwl_seq_generator = iwlloader.get_seq\n",
        "iwl_dataset = IterDataset(iwl_seq_generator)\n",
        "iwl_dataloader = torch.utils.data.DataLoader(iwl_dataset, batch_size=trainconfig.batch_size)\n",
        "\n",
        "icl2loader = SamplingLoader(icl2dataconfig)\n",
        "icl2_seq_generator = icl2loader.get_seq\n",
        "icl2_dataset = IterDataset(icl2_seq_generator)\n",
        "icl2_dataloader = torch.utils.data.DataLoader(icl2_dataset, batch_size=trainconfig.batch_size)\n",
        "\n",
        "# model\n",
        "embedder = InputEmbedder(modelconfig)\n",
        "model = Transformer(embedder, modelconfig)\n",
        "model.to(\"cuda:1\")\n",
        "\n",
        "# optimizer\n",
        "optimizer =  torch.optim.SGD(model.parameters(), lr=trainconfig.lr, momentum=0.9)\n",
        "\n",
        "# loss\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[436,  72, 456,  12, 242, 296, 240, 146, 146],\n",
            "        [375,  12, 493, 146, 283, 350,  64, 404, 350],\n",
            "        [105, 421, 135, 180,  92, 180, 214, 229, 421],\n",
            "        [128,   9, 346, 482, 305, 128, 319,  72,   9],\n",
            "        [351, 135, 407, 352, 279,  68, 240,  96, 351],\n",
            "        [386, 324,  12, 314, 327, 350, 127, 454,  12],\n",
            "        [299,  31, 366,  76,  49, 510,  27, 268, 510],\n",
            "        [491, 238, 211, 487, 349, 377, 214, 248, 214],\n",
            "        [ 87, 443, 409, 123,  80, 180, 196, 186,  87],\n",
            "        [ 92, 492, 430, 463, 401, 119,  73, 209, 209],\n",
            "        [498,  34, 485, 493, 286, 474, 460, 288, 474],\n",
            "        [320,  56, 259,  48,  42, 265, 500, 310,  48],\n",
            "        [280, 375, 189, 155, 340, 353, 378, 242, 189],\n",
            "        [351, 118, 339, 412, 462,  25, 208,  68, 118],\n",
            "        [215, 271, 342, 248, 114,  37, 371, 113, 113],\n",
            "        [296,  46, 425, 446, 422,  22, 402, 416, 422],\n",
            "        [ 54, 404, 141, 281, 400, 235, 448, 114, 400],\n",
            "        [379, 339, 341,   6, 349, 108,  55, 437, 349],\n",
            "        [339, 167, 101,   1, 463, 412, 398, 463, 463],\n",
            "        [431, 332, 401, 254, 203, 206, 131, 155, 203],\n",
            "        [491,  29, 237,  14, 250, 417, 207, 264, 250],\n",
            "        [390, 413, 202,  44,  45, 158, 135, 345, 158],\n",
            "        [179, 122, 193, 308, 174, 252, 269, 216, 122],\n",
            "        [ 73, 328, 310, 261, 386, 299, 182, 419, 182],\n",
            "        [473, 421, 156, 314, 450, 265, 139, 333, 333],\n",
            "        [  2, 433, 300, 424, 103, 392, 293, 259, 293],\n",
            "        [ 81, 381, 300, 189, 477, 216, 139, 242, 300],\n",
            "        [231, 375, 101, 466,  19,  38,  50, 264, 375],\n",
            "        [447, 127, 393, 411, 132,  61, 176, 117, 127],\n",
            "        [ 69, 422,  20, 456, 317, 108,  96, 314, 422],\n",
            "        [ 41, 457, 196, 326, 162, 372, 311, 262, 196],\n",
            "        [  5, 397,   6,  40, 137, 269, 357, 333,   5]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_560266/3124704240.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.cat([labels.flatten(), torch.tensor(query_label).flatten()])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
          ]
        }
      ],
      "source": [
        "for data_dict in icl2_dataloader:\n",
        "  examples = data_dict[\"examples\"]\n",
        "  labels = data_dict[\"labels\"]\n",
        "  classes = data_dict[\"classes\"]\n",
        "  # print(examples)\n",
        "  # print(labels)\n",
        "  print(classes)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Feb 28 12:24:02 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA RTX A6000               On  | 00000000:21:00.0 Off |                  Off |\n",
            "| 37%   65C    P8              26W / 200W |   6298MiB / 49140MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA RTX A6000               On  | 00000000:41:00.0 Off |                  Off |\n",
            "| 45%   69C    P2              56W / 200W |    407MiB / 49140MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   2  NVIDIA RTX A6000               On  | 00000000:43:00.0 Off |                  Off |\n",
            "| 55%   80C    P2             176W / 200W |   3402MiB / 49140MiB |     48%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "U4wKEdZkNkC6"
      },
      "outputs": [],
      "source": [
        "def cal_acc(t,p):\n",
        "    p_arg = torch.argmax(p,dim=1)\n",
        "    return torch.sum(t == p_arg) / p.shape[0]\n",
        "def to_gpu_dict(dic):\n",
        "    dic = {k:v.to(\"cuda:1\") for k,v in dic.items()}\n",
        "    return dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "12621ac3883347eba74b077797cda2b9",
            "5b7930a79e344c358f1ca175e9be4fd1",
            "d4e89f0d98b44eb4b76e59caa06aea14",
            "494267ae30c44a3bb446eaee6dc43115",
            "1a6e230516ff4996bb34f8a4992e1a20",
            "065c4cf26dd24c0d8caaa7165e800ca7",
            "aea29e116b304946b7d98692deb2addb",
            "9b3c46b8ff12462b9c4d2f0fbf12c767"
          ]
        },
        "id": "jwic1RdJGRDT",
        "outputId": "4663b1f4-a8bd-4983-f03d-8d0b9ac74208"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:tw94jb5g) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>train/loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid/icl2_acc</td><td>▆▄▃▅▃▃▇▄▄▄▂▃▇▅▆▄▂▅▄▃▅▃▆█▂▂▁▆▇▆▃▄▂▆▃▃▇▇▃▃</td></tr><tr><td>valid/icl_acc</td><td>▆▄█▅▃▁▃▆▅▄▃▃▄▇▃▆▃▆▄▄▆▃▇▇▅▆▅▅▆▆▁▇▂▆▅▅▅▂▅▅</td></tr><tr><td>valid/iwl_acc</td><td>█▅▅▅▅▁▅▁▅▅▁▁▁▅█▁▅▁▁█▁▁▁██▅▅█▅█▅▁█▁▅▁█▁██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>1.0</td></tr><tr><td>train/loss</td><td>0.0</td></tr><tr><td>valid/icl2_acc</td><td>0.21875</td></tr><tr><td>valid/icl_acc</td><td>0.28125</td></tr><tr><td>valid/iwl_acc</td><td>0.0</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">comfy-dust-20</strong> at: <a href='https://wandb.ai/gouki/icl-induction-head/runs/tw94jb5g' target=\"_blank\">https://wandb.ai/gouki/icl-induction-head/runs/tw94jb5g</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240228_112935-tw94jb5g/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:tw94jb5g). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/workspace/MultiTask-ICL/wandb/run-20240228_122443-k2qc8c5r</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gouki/icl-induction-head/runs/k2qc8c5r' target=\"_blank\">snowy-jazz-21</a></strong> to <a href='https://wandb.ai/gouki/icl-induction-head' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gouki/icl-induction-head' target=\"_blank\">https://wandb.ai/gouki/icl-induction-head</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gouki/icl-induction-head/runs/k2qc8c5r' target=\"_blank\">https://wandb.ai/gouki/icl-induction-head/runs/k2qc8c5r</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_560266/3509681106.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.cat([labels.flatten(), torch.tensor(query_label).flatten()])\n",
            "/tmp/ipykernel_560266/3509681106.py:141: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.cat([labels.flatten(), torch.tensor(query_label).flatten()])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
            "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "config = MainConfig()\n",
        "wandb.init(project=\"icl-induction-head\", config=asdict(config))\n",
        "\n",
        "step = 0\n",
        "for (data_dict, icl_data_dict, iwl_data_dict, icl2_data_dict) in zip(train_dataloader, icl_dataloader, iwl_dataloader, icl2_dataloader):\n",
        "  model.train()   \n",
        "  data_dict = to_gpu_dict(data_dict)\n",
        "  icl_data_dict = to_gpu_dict(icl_data_dict)\n",
        "  iwl_data_dict = to_gpu_dict(iwl_data_dict)\n",
        "  icl2_data_dict = to_gpu_dict(icl2_data_dict)\n",
        "  \n",
        "  logits = model(data_dict[\"examples\"], data_dict[\"labels\"])\n",
        "  query_logit = logits[:,-1,:]\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  # print(data_dict[\"labels\"][:,-1])\n",
        "  loss = criterion(query_logit, data_dict[\"labels\"][:,-1],)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  train_acc = cal_acc(data_dict[\"labels\"][:, -1], query_logit)\n",
        "  wandb.log({\"train/acc\":train_acc,\"train/loss\":loss}, step=step)\n",
        "  with torch.no_grad():\n",
        "\n",
        "          logits = model(icl_data_dict[\"examples\"], icl_data_dict[\"labels\"])\n",
        "          query_logit = logits[:,-1,:]\n",
        "          icl_acc = cal_acc(icl_data_dict[\"labels\"][:, -1], query_logit)\n",
        "          wandb.log({\"valid/icl_acc\":icl_acc}, step=step)\n",
        "\n",
        "          logits = model(iwl_data_dict[\"examples\"], iwl_data_dict[\"labels\"])\n",
        "          query_logit = logits[:,-1,:]\n",
        "          iwl_acc = cal_acc(iwl_data_dict[\"labels\"][:, -1], query_logit)\n",
        "          wandb.log({\"valid/iwl_acc\":iwl_acc}, step=step)\n",
        "\n",
        "          logits = model(icl2_data_dict[\"examples\"], icl2_data_dict[\"labels\"])\n",
        "          query_logit = logits[:,-1,:]\n",
        "          icl2_acc = cal_acc(icl2_data_dict[\"labels\"][:, -1], query_logit)\n",
        "          wandb.log({\"valid/icl2_acc\":icl2_acc}, step=step)\n",
        "  step+=1\n",
        "  if step > trainconfig.optimize_step:\n",
        "    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4i_bXu6L5yF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "065c4cf26dd24c0d8caaa7165e800ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12621ac3883347eba74b077797cda2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b7930a79e344c358f1ca175e9be4fd1",
              "IPY_MODEL_d4e89f0d98b44eb4b76e59caa06aea14"
            ],
            "layout": "IPY_MODEL_494267ae30c44a3bb446eaee6dc43115"
          }
        },
        "1a6e230516ff4996bb34f8a4992e1a20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494267ae30c44a3bb446eaee6dc43115": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b7930a79e344c358f1ca175e9be4fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a6e230516ff4996bb34f8a4992e1a20",
            "placeholder": "​",
            "style": "IPY_MODEL_065c4cf26dd24c0d8caaa7165e800ca7",
            "value": "0.010 MB of 0.010 MB uploaded\r"
          }
        },
        "9b3c46b8ff12462b9c4d2f0fbf12c767": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aea29e116b304946b7d98692deb2addb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4e89f0d98b44eb4b76e59caa06aea14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aea29e116b304946b7d98692deb2addb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b3c46b8ff12462b9c4d2f0fbf12c767",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
