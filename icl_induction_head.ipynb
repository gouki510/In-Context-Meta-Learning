{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4cb8GADCuFl_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Mar  8 03:55:59 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA RTX A6000               On  | 00000000:21:00.0 Off |                  Off |\n",
            "| 30%   55C    P8              25W / 200W |   6295MiB / 49140MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA RTX A6000               On  | 00000000:41:00.0 Off |                  Off |\n",
            "| 30%   51C    P8              23W / 200W |   6402MiB / 49140MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   2  NVIDIA RTX A6000               On  | 00000000:43:00.0 Off |                  Off |\n",
            "| 30%   46C    P8              24W / 200W |   3851MiB / 49140MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzF5h6iNugUD"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pcrkWn3UuPgm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import einops\n",
        "\n",
        "\n",
        "class Unembed(nn.Module):\n",
        "    def __init__(self, d_vocab, d_model):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(d_vocab, d_model) / np.sqrt(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.einsum(\"pe,bse->bsp\", self.W, x)\n",
        "\n",
        "\n",
        "class HookPoint(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fwd_hooks = []\n",
        "        self.bwd_hooks = []\n",
        "\n",
        "    def give_name(self, name):\n",
        "        # Called by the model at initialisation\n",
        "        self.name = name\n",
        "\n",
        "    def add_hook(self, hook, dir=\"fwd\"):\n",
        "        # Hook format is fn(activation, hook_name)\n",
        "        # Change it into PyTorch hook format (this includes input and output,\n",
        "        # which are the same for a HookPoint)\n",
        "        def full_hook(module, module_input, module_output):\n",
        "            return hook(module_output, name=self.name)\n",
        "\n",
        "        if dir == \"fwd\":\n",
        "            handle = self.register_forward_hook(full_hook)\n",
        "            self.fwd_hooks.append(handle)\n",
        "        elif dir == \"bwd\":\n",
        "            handle = self.register_backward_hook(full_hook)\n",
        "            self.bwd_hooks.append(handle)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid direction {dir}\")\n",
        "\n",
        "    def remove_hooks(self, dir=\"fwd\"):\n",
        "        if (dir == \"fwd\") or (dir == \"both\"):\n",
        "            for hook in self.fwd_hooks:\n",
        "                hook.remove()\n",
        "            self.fwd_hooks = []\n",
        "        if (dir == \"bwd\") or (dir == \"both\"):\n",
        "            for hook in self.bwd_hooks:\n",
        "                hook.remove()\n",
        "            self.bwd_hooks = []\n",
        "        if dir not in [\"fwd\", \"bwd\", \"both\"]:\n",
        "            raise ValueError(f\"Invalid direction {dir}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "class PosEmbed(nn.Module):\n",
        "    def __init__(self, max_ctx, d_model, weight_scale=1):\n",
        "        super().__init__()\n",
        "        self.W_pos = nn.Parameter(torch.randn(max_ctx, d_model) * weight_scale)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.W_pos[: x.shape[-2]]\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, epsilon=1e-4, model=[None]):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.w_ln = nn.Parameter(torch.ones(d_model))\n",
        "        self.b_ln = nn.Parameter(torch.zeros(d_model))\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.model[0].use_ln:\n",
        "            x = x - x.mean(axis=-1)[..., None]\n",
        "            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n",
        "            x = x * self.w_ln\n",
        "            x = x + self.b_ln\n",
        "            return x\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "# Attention\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    b : batch size\n",
        "    d : embedding size of token\n",
        "    p : vocabraly size (113 or 3)\n",
        "    i : number of heads\n",
        "    h : embedding size of each heads\n",
        "    n_ctx : token size\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, d_head, n_ctx):\n",
        "        super().__init__()\n",
        "        self.W_K = nn.Parameter(\n",
        "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.W_Q = nn.Parameter(\n",
        "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.W_V = nn.Parameter(\n",
        "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.W_O = nn.Parameter(\n",
        "            torch.randn(d_model, d_head * num_heads) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones((n_ctx, n_ctx))))\n",
        "        self.d_head = d_head\n",
        "\n",
        "    def forward(self, x):\n",
        "        k = torch.einsum(\"ihd,bpd->biph\", self.W_K, x)\n",
        "        q = torch.einsum(\"ihd,bpd->biph\", self.W_Q, x)\n",
        "        v = torch.einsum(\"ihd,bpd->biph\", self.W_V, x)\n",
        "        attn_scores_pre = torch.einsum(\"biph,biqh->biqp\", k, q)\n",
        "        attn_scores_masked = torch.tril(attn_scores_pre) - 1e10 * (\n",
        "            1 - self.mask[: x.shape[-2], : x.shape[-2]]\n",
        "        )\n",
        "        attn_matrix = F.softmax(attn_scores_masked / np.sqrt(self.d_head), dim=-1)\n",
        "        z = torch.einsum(\"biph,biqp->biqh\", v, attn_matrix)\n",
        "        z_flat = einops.rearrange(z, \"b i q h -> b q (i h)\")\n",
        "        out = torch.einsum(\"df,bqf->bqd\", self.W_O, z_flat)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, d_in, d_out, act_type, weight_scale=1):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(d_out, d_in))\n",
        "        torch.nn.init.normal_(self.W, mean=0, std=weight_scale / np.sqrt(d_in))\n",
        "\n",
        "    def set_weight_ratio(self, weight_ratio):\n",
        "        self.W = nn.Parameter(self.W * weight_ratio)\n",
        "\n",
        "    def set_weight_ratio_l2(self, weight_ratio):\n",
        "        self.W = nn.Parameter(self.W * torch.sqrt(weight_ratio))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x @ self.W.T\n",
        "\n",
        "\n",
        "# for Transformer\n",
        "class MLPBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    b : batch size\n",
        "    d : embedding size of token\n",
        "    p : vocabraly size (114 or 3)\n",
        "    i : number of heads\n",
        "    h : embedding size of each heads\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_mlp, act_type):\n",
        "        super().__init__()\n",
        "        # bias & layer norm are removed.\n",
        "        self.W_in = nn.Parameter(torch.randn(d_mlp, d_model) / np.sqrt(d_model))\n",
        "        self.b_in = nn.Parameter(torch.zeros(d_mlp))\n",
        "        self.W_out = nn.Parameter(torch.randn(d_model, d_mlp) / np.sqrt(d_model))\n",
        "        self.b_out = nn.Parameter(torch.zeros(d_model))\n",
        "        self.act_type = act_type\n",
        "        # self.ln = LayerNorm(d_mlp, model=self.model)\n",
        "        assert act_type in [\"ReLU\", \"GeLU\"]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.einsum(\"md,bpd->bpm\", self.W_in, x) + self.b_in\n",
        "        if self.act_type == \"ReLU\":\n",
        "            x = F.relu(x)\n",
        "        elif self.act_type == \"GeLU\":\n",
        "            x = F.gelu(x)\n",
        "        x = torch.einsum(\"dm,bpm->bpd\", self.W_out, x) + self.b_out\n",
        "        return x\n",
        "\n",
        "    def set_weight_ratio(self, weight_ratio):\n",
        "        self.W_in = nn.Parameter(self.W_in * weight_ratio)\n",
        "        self.W_out = nn.Parameter(self.W_out * weight_ratio)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    b : batch size\n",
        "    d : embedding size of token\n",
        "    p : vocabraly size\n",
        "    i : number of heads\n",
        "    h : embedding size of each heads\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n",
        "        super().__init__()\n",
        "        # self.ln1 = LayerNorm(d_model, model=self.model)\n",
        "        self.model = model\n",
        "        self.attn = Attention(d_model, num_heads, d_head, n_ctx)\n",
        "        # self.ln2 = LayerNorm(d_model, model=self.model)\n",
        "        self.mlp = MLPBlock(d_model, d_mlp, act_type)\n",
        "        self.layer_norm = LayerNorm(d_model, model=self.model)\n",
        "        self.hook_attn_out = HookPoint()\n",
        "        self.hook_mlp_out = HookPoint()\n",
        "        self.hook_resid_pre = HookPoint()\n",
        "        self.hook_resid_mid = HookPoint()\n",
        "        self.hook_resid_post = HookPoint()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hook_resid_mid(\n",
        "            x + self.hook_attn_out(self.attn((self.hook_resid_pre(x))))\n",
        "        )\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.hook_resid_post(x + self.hook_mlp_out(self.mlp((x))))\n",
        "        return x\n",
        "\n",
        "    def set_weight_ratio(self, weight_ratio):\n",
        "        self.attn.set_weight_ratio(weight_ratio)\n",
        "        self.mlp.set_weight_ratio(weight_ratio)\n",
        "\n",
        "\n",
        "class InputEmbedder(nn.Module):\n",
        "    \"\"\"Input embedder.\"\"\"\n",
        "\n",
        "    def __init__(self, conf):\n",
        "\n",
        "        \"\"\"Initialize the input embedder.\n",
        "\n",
        "    Args:\n",
        "      num_classes: Total number of output classes.\n",
        "      emb_dim: Dimensionality of example and label embeddings.\n",
        "      example_encoding: How to encode example inputs.\n",
        "        'resnet': simple resnet encoding\n",
        "        'linear': flatten and pass through a linear layer\n",
        "        'embedding': pass through an embedding layer\n",
        "      flatten_superpixels: Whether to flatten the output of the resnet (instead\n",
        "        of taking a mean over superpixels).\n",
        "      example_dropout_prob: Dropout probability on example embeddings. Note that\n",
        "        these are applied at both train and test.\n",
        "      concatenate_labels: Whether to concatenate example and label embeddings\n",
        "        into one token for each (example, label) pair, rather than being fed to\n",
        "        the transformer as two separate tokens.\n",
        "      use_positional_encodings: Whether to use positional encoding.\n",
        "      positional_dropout_prob: Positional dropout probability.\n",
        "      name: Optional name for the module.\n",
        "    \"\"\"\n",
        "        super(InputEmbedder, self).__init__()\n",
        "        self._num_classes = conf.d_vocab\n",
        "        self._emb_dim = conf.d_emb\n",
        "        self.p_dim = conf.p_dim\n",
        "\n",
        "        self.Emb = nn.Linear(self._emb_dim, self._emb_dim)\n",
        "\n",
        "        self.label_embs = nn.Parameter(\n",
        "            torch.randn(self._num_classes, self._emb_dim) / np.sqrt(self._emb_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, examples, labels, is_training=True):\n",
        "        \"\"\"Call to the input embedder.\n",
        "\n",
        "        Args:\n",
        "          examples: input sequence of shape\n",
        "            [batch_size, seq_len, height, width, channels]\n",
        "          labels: input sequence of shape [batch_size, seq_len]\n",
        "          is_training: if is currently training.\n",
        "\n",
        "        Returns:\n",
        "          outputs: output of the transformer tower\n",
        "            of shape [batch_size, seq_len, channels].\n",
        "        \"\"\"\n",
        "        # Encode the example inputs into shape (B, SS, E)\n",
        "        B, SS, D = examples.shape\n",
        "        # pos encoding\n",
        "        pos_enc = F.one_hot(torch.arange(SS), num_classes=self.p_dim).repeat(B,1,1).to(examples.device)\n",
        "        h_example = torch.cat([examples, pos_enc], dim=2)\n",
        "\n",
        "        # Embed the labels.\n",
        "        n_emb_classes = self._num_classes\n",
        "        labels_to_embed = labels\n",
        "        h_label = self.label_embs[labels_to_embed]  # (B, SS, E)\n",
        "        hh = torch.empty(\n",
        "            (h_example.shape[0], h_example.shape[1] * 2 - 1, h_example.shape[2]),\n",
        "            dtype=h_example.dtype,\n",
        "        ).to(h_example.device)\n",
        "        hh[:, 0::2] = h_example\n",
        "        hh[:, 1::2] = h_label[:, :-1]\n",
        "\n",
        "        return hh\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedder, config):\n",
        "        super().__init__()\n",
        "        num_layers = config.num_layers\n",
        "        d_model = config.d_emb\n",
        "        d_mlp = config.d_emb * 4\n",
        "        d_head = config.d_emb // config.num_heads\n",
        "        num_heads = config.num_heads\n",
        "        n_ctx = config.n_ctx\n",
        "        act_type = config.act_type\n",
        "        use_cache = config.use_cache\n",
        "        use_ln = config.use_ln\n",
        "        self.cache = {}\n",
        "        self.use_cache = use_cache\n",
        "        d_vocab = config.d_vocab\n",
        "\n",
        "        self.embedder = embedder\n",
        "        # self.pos_embed = PosEmbed(n_ctx, d_model)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model=[self]\n",
        "                )\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        # self.ln = LayerNorm(d_model, model=[self])\n",
        "        self.unembed = Unembed(d_vocab, d_model)\n",
        "        self.use_ln = use_ln\n",
        "\n",
        "        for name, module in self.named_modules():\n",
        "            if type(module) == HookPoint:\n",
        "                module.give_name(name)\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        x = self.embedder(x, labels,)\n",
        "        # x = self.pos_embed(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.unembed(x)\n",
        "        return x\n",
        "\n",
        "    def set_use_cache(self, use_cache):\n",
        "        self.use_cache = use_cache\n",
        "\n",
        "    def hook_points(self):\n",
        "        return [module for name, module in self.named_modules() if \"hook\" in name]\n",
        "\n",
        "    def remove_all_hooks(self):\n",
        "        for hp in self.hook_points():\n",
        "            hp.remove_hooks(\"fwd\")\n",
        "            hp.remove_hooks(\"bwd\")\n",
        "\n",
        "    def cache_all(self, cache, incl_bwd=False):\n",
        "        # Caches all activations wrapped in a HookPoint\n",
        "        def save_hook(tensor, name):\n",
        "            cache[name] = tensor.detach()\n",
        "\n",
        "        def save_hook_back(tensor, name):\n",
        "            cache[name + \"_grad\"] = tensor[0].detach()\n",
        "\n",
        "        for hp in self.hook_points():\n",
        "            hp.add_hook(save_hook, \"fwd\")\n",
        "            if incl_bwd:\n",
        "                hp.add_hook(save_hook_back, \"bwd\")\n",
        "\n",
        "class TransformerICL(nn.Module):\n",
        "    def __init__(self, embedder, config):\n",
        "        super().__init__()\n",
        "        num_layers = config.num_layers\n",
        "        d_model = config.d_emb\n",
        "        d_mlp = config.d_emb * 4\n",
        "        d_head = config.d_emb // config.num_heads\n",
        "        num_heads = config.num_heads\n",
        "        n_ctx = config.n_ctx\n",
        "        act_type = config.act_type\n",
        "        use_cache = config.use_cache\n",
        "        use_ln = config.use_ln\n",
        "        self.cache = {}\n",
        "        self.use_cache = use_cache\n",
        "        d_vocab = config.d_vocab\n",
        "\n",
        "        self.embedder = embedder\n",
        "        # self.pos_embed = PosEmbed(n_ctx, d_model)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                Attention(d_model, num_heads, d_head, n_ctx),\n",
        "                Attention(d_model, num_heads, d_head, n_ctx),\n",
        "                Dense(d_model, d_model, act_type),\n",
        "                Dense(d_model, d_model, act_type),\n",
        "                Dense(d_model, d_model, act_type),\n",
        "            ]\n",
        "        )\n",
        "        # self.ln = LayerNorm(d_model, model=[self])\n",
        "        self.unembed = Unembed(d_vocab, d_model)\n",
        "        self.use_ln = use_ln\n",
        "\n",
        "        for name, module in self.named_modules():\n",
        "            if type(module) == HookPoint:\n",
        "                module.give_name(name)\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        x = self.embedder(x, labels)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.unembed(x)\n",
        "        return x\n",
        "\n",
        "    def set_use_cache(self, use_cache):\n",
        "        self.use_cache = use_cache\n",
        "\n",
        "    def hook_points(self):\n",
        "        return [module for name, module in self.named_modules() if \"hook\" in name]\n",
        "\n",
        "    def remove_all_hooks(self):\n",
        "        for hp in self.hook_points():\n",
        "            hp.remove_hooks(\"fwd\")\n",
        "            hp.remove_hooks(\"bwd\")\n",
        "\n",
        "    def cache_all(self, cache, incl_bwd=False):\n",
        "        # Caches all activations wrapped in a HookPoint\n",
        "        def save_hook(tensor, name):\n",
        "            cache[name] = tensor.detach()\n",
        "\n",
        "        def save_hook_back(tensor, name):\n",
        "            cache[name + \"_grad\"] = tensor[0].detach()\n",
        "\n",
        "        for hp in self.hook_points():\n",
        "            hp.add_hook(save_hook, \"fwd\")\n",
        "            if incl_bwd:\n",
        "                hp.add_hook(save_hook_back, \"bwd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6iNOyJRwwV1h"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class SamplingDataset(object):\n",
        "  def __init__(self,conf):\n",
        "    self.num_classes = conf.num_classes\n",
        "    self.dim = conf.dim\n",
        "    self.num_labels = conf.num_labels\n",
        "    self.mu, self.labels = self._get_data()\n",
        "\n",
        "  def _get_data(self):\n",
        "    mu = torch.normal(mean=0, std=math.sqrt(1/self.dim), size=(self.num_classes,self.dim))\n",
        "    labels = torch.randint(self.num_labels, size=(self.num_classes,1))\n",
        "    return mu, labels\n",
        "\n",
        "class SamplingLoader(DataLoader):\n",
        "\n",
        "  def __init__(self,conf, dataset):\n",
        "    self.dataset = dataset\n",
        "    self.mu, self.labels = self.dataset.mu, self.dataset.labels\n",
        "    self.data_type = conf.data_type\n",
        "    self.num_seq = conf.num_seq\n",
        "    self.alpha = conf.alpha\n",
        "    self.num_classes = conf.num_classes\n",
        "    self.num_labels = conf.num_labels\n",
        "    self.ways = conf.ways\n",
        "    self.p_bursty = conf.p_bursty\n",
        "    self.eps = conf.eps\n",
        "    self.dim = conf.dim\n",
        "    self.num_holdout_classes = conf.num_holdout_classes\n",
        "    self.holdout_classes = np.arange(self.num_classes-self.num_holdout_classes, self.num_classes)\n",
        "    self.burstiness_class = conf.burstiness_classes\n",
        "    if self.ways != 0:\n",
        "      assert self.num_seq % self.ways ==0\n",
        "    if self.ways == 0:\n",
        "      self.p_bursty = 0\n",
        "    prob = np.array([1/((k+1)**self.alpha) for k in range(self.num_classes-self.num_holdout_classes)])\n",
        "    self.prob = prob/prob.sum()\n",
        "\n",
        "  def get_seq(self):\n",
        "    while True:\n",
        "      if self.data_type==\"bursty\":\n",
        "        if self.p_bursty > np.random.rand():\n",
        "          # choise few shot example\n",
        "          num_few_shot_class = self.num_seq//self.ways\n",
        "          few_shot_class = np.random.choice(self.burstiness_class, num_few_shot_class, replace=False)\n",
        "          mus = self.mu[few_shot_class]\n",
        "          mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
        "          labels = self.labels[few_shot_class]\n",
        "          labels = np.repeat(labels, self.ways, axis=0) # expand ways\n",
        "          classes = np.repeat(few_shot_class, self.ways)\n",
        "          # add noise\n",
        "          x = self.add_noise(mus)\n",
        "          # permutation shuffle\n",
        "          ordering = np.random.permutation(self.num_seq)\n",
        "          x = x[ordering]\n",
        "          labels = labels[ordering]\n",
        "          classes = classes[ordering]\n",
        "          # select query labels\n",
        "          query_class = np.random.choice(few_shot_class, 1)\n",
        "          query_label = self.labels[query_class]\n",
        "          query_mu = self.mu[query_class]\n",
        "          query_x = self.add_noise(query_mu)\n",
        "          # concat\n",
        "          x = torch.cat([x, query_x])\n",
        "          labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
        "          yield {\n",
        "              \"examples\":x.to(torch.float32),\n",
        "              \"labels\":labels,\n",
        "              \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
        "          }\n",
        "        else:\n",
        "          # rank frequency\n",
        "          classes = np.random.choice(self.num_classes-self.num_holdout_classes, self.num_seq+1, p=self.prob)\n",
        "          mus = self.mu[classes]\n",
        "          labels = self.labels[classes]\n",
        "          x = self.add_noise(mus)\n",
        "          # permutation shuffle\n",
        "          ordering = np.random.permutation(self.num_seq+1)\n",
        "          x = x[ordering]\n",
        "          labels = labels[ordering]\n",
        "          classes = classes[ordering]\n",
        "\n",
        "          yield {\n",
        "              \"examples\":x.to(torch.float32),\n",
        "              \"labels\":labels.flatten(),\n",
        "              \"classes\" : torch.from_numpy(classes)\n",
        "          }\n",
        "\n",
        "      elif self.data_type == \"no_support\":\n",
        "          # rank frequency\n",
        "          classes = np.random.choice(self.num_classes-self.num_holdout_classes, self.num_seq+1, p=self.prob)\n",
        "          mus = self.mu[classes]\n",
        "          labels = self.labels[classes]\n",
        "          x = self.add_noise(mus)\n",
        "          # permutation shuffle\n",
        "          ordering = np.random.permutation(self.num_seq+1)\n",
        "          x = x[ordering]\n",
        "          labels = labels[ordering]\n",
        "          classes = classes[ordering]\n",
        "\n",
        "          yield {\n",
        "              \"examples\":x.to(torch.float32),\n",
        "              \"labels\":labels.flatten(),\n",
        "              \"classes\" : torch.from_numpy(classes)\n",
        "          }\n",
        "          \n",
        "      elif self.data_type == \"holdout\":\n",
        "          if 1 > np.random.rand():\n",
        "            # choise few shot example\n",
        "            num_few_shot_class = self.num_seq//self.ways\n",
        "            few_shot_class = np.random.choice(self.holdout_classes, num_few_shot_class, replace=False)\n",
        "            mus = self.mu[few_shot_class]\n",
        "            mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
        "            labels = self.labels[few_shot_class]\n",
        "            labels = np.repeat(labels, self.ways, axis=0) # expand ways\n",
        "            classes = np.repeat(few_shot_class, self.ways)\n",
        "            # add noise\n",
        "            x = self.add_noise(mus)\n",
        "            # permutation shuffle\n",
        "            ordering = np.random.permutation(self.num_seq)\n",
        "            x = x[ordering]\n",
        "            labels = labels[ordering]\n",
        "            classes = classes[ordering]\n",
        "            # select query labels\n",
        "            query_class = np.random.choice(few_shot_class, 1)\n",
        "            query_label = self.labels[query_class]\n",
        "            query_mu = self.mu[query_class]\n",
        "            query_x = self.add_noise(query_mu)\n",
        "            # concat\n",
        "            x = torch.cat([x, query_x])\n",
        "            labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
        "            \n",
        "            yield {\n",
        "                \"examples\":x.to(torch.float32),\n",
        "                \"labels\":labels,\n",
        "                \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
        "            }\n",
        "            \n",
        "          else:\n",
        "          # rank frequency\n",
        "            classes = np.random.choice(self.holdout_classes, self.num_seq)\n",
        "            mus = self.mu[classes]\n",
        "            labels = self.labels[classes]\n",
        "            x = self.add_noise(mus)\n",
        "            # permutation shuffle\n",
        "            ordering = np.random.permutation(self.num_seq)\n",
        "            x = x[ordering]\n",
        "            labels = labels[ordering]\n",
        "            classes = classes[ordering]\n",
        "            # query\n",
        "            query_class = np.random.choice(classes, 1)\n",
        "            query_label = self.labels[query_class]\n",
        "            query_mu = self.mu[query_class]\n",
        "            query_x = self.add_noise(query_mu)\n",
        "            # concat\n",
        "            x = torch.cat([x, query_x])\n",
        "            labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
        "\n",
        "            yield {\n",
        "                \"examples\":x.to(torch.float32),\n",
        "                \"labels\":labels,\n",
        "                \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class)])\n",
        "            }\n",
        "\n",
        "      elif self.data_type == \"flip\":\n",
        "          if self.p_bursty > np.random.rand():\n",
        "            # choise few shot example\n",
        "            num_few_shot_class = self.num_seq//self.ways\n",
        "            few_shot_class = np.random.choice(self.burstiness_class, num_few_shot_class, replace=False)\n",
        "            mus = self.mu[few_shot_class]\n",
        "            mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
        "            classes = np.repeat(few_shot_class, self.ways)\n",
        "            # label flip\n",
        "            labels = (self.labels[classes] + 1) % self.num_labels\n",
        "            # add noise\n",
        "            x = self.add_noise(mus)\n",
        "            # permutation shuffle\n",
        "            ordering = np.random.permutation(self.num_seq)\n",
        "            x = x[ordering]\n",
        "            labels = labels[ordering]\n",
        "            classes = classes[ordering]\n",
        "            # select query labels\n",
        "            query_class = np.random.choice(few_shot_class, 1)\n",
        "            query_label = (self.labels[query_class] + 1) % self.num_labels\n",
        "            query_mu = self.mu[query_class]\n",
        "            query_x = self.add_noise(query_mu)\n",
        "            # concat\n",
        "            x = torch.cat([x, query_x])\n",
        "            labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
        "            \n",
        "            yield {\n",
        "                \"examples\":x.to(torch.float32),\n",
        "                \"labels\":labels,\n",
        "                \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
        "            }\n",
        "            \n",
        "          else:\n",
        "            # rank frequency\n",
        "            classes = np.random.choice(self.num_classes-self.num_holdout_classes, self.num_seq)\n",
        "            mus = self.mu[classes]\n",
        "            # label flip\n",
        "            labels = (self.labels[classes] + 1) % self.num_labels\n",
        "            x = self.add_noise(mus)\n",
        "            # permutation shuffle\n",
        "            ordering = np.random.permutation(self.num_seq)\n",
        "            x = x[ordering]\n",
        "            labels = labels[ordering]\n",
        "            classes = classes[ordering]\n",
        "            # query\n",
        "            query_class = np.random.choice(classes, 1)\n",
        "            query_label = (self.labels[query_class]+1) % self.num_labels\n",
        "            query_mu = self.mu[query_class]\n",
        "            query_x = self.add_noise(query_mu)\n",
        "            # concat\n",
        "            x = torch.cat([x, query_x])\n",
        "            labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
        "            yield {\n",
        "                \"examples\":x.to(torch.float32),\n",
        "                \"labels\":labels.flatten(),\n",
        "                \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class)])\n",
        "            }\n",
        "        \n",
        "\n",
        "  def add_noise(self, x):\n",
        "    x = (x+self.eps*torch.normal(mean=0, std=math.sqrt(1/self.dim), size=(x.shape[0],1)))/(np.sqrt(1+self.eps**2))\n",
        "    # x = (x+self.eps*np.random.normal(mean=0, std=np.sqrt(1/self.dim), size=(x.shape[0],1)))/(np.sqrt(1+self.eps**2))\n",
        "    return x\n",
        "\n",
        "class IterDataset(IterableDataset):\n",
        "    def __init__(self, generator):\n",
        "        self.generator = generator\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.generator()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gX3jnutBzfVe"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, asdict\n",
        "\n",
        "@dataclass\n",
        "class TransformerConfig:\n",
        "  num_layers: int = 2\n",
        "  d_vocab: int = 32\n",
        "  d_model: int = 128\n",
        "  d_mlp: int = 128\n",
        "  d_head: int = 128\n",
        "  num_heads: int = 1\n",
        "  n_ctx: int = int(8*2+1)\n",
        "  act_type: str = \"ReLU\"\n",
        "  use_cache: bool = False\n",
        "  use_ln: bool = True\n",
        "  p_dim: int = 65\n",
        "  d_emb: int = 128\n",
        "  num_classes = 64\n",
        "\n",
        "@dataclass\n",
        "class TrainDataConfig:\n",
        "  num_classes: int = 64\n",
        "  dim: int = 63\n",
        "  num_labels: int = 32\n",
        "  eps: float = 0.1\n",
        "  alpha: float = 0\n",
        "  ways: int = 4\n",
        "  num_seq: int = 8\n",
        "  burstiness_classes: int = range(0, num_seq//ways)\n",
        "  p_bursty: float = 0.7\n",
        "  data_type: str = \"bursty\" # bursty, holdout, no_support, flip\n",
        "  num_holdout_classes: int = 10\n",
        "\n",
        "@dataclass\n",
        "class IWLDataConfig(TrainDataConfig):\n",
        "  data_type: str = \"no_support\" # bursty, holdout, no_support, flip\n",
        "\n",
        "@dataclass\n",
        "class ICLDataConfig(TrainDataConfig):\n",
        "  data_type: str = \"holdout\" # bursty, holdout, no_support, flip\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ICL2DataConfig(TrainDataConfig):\n",
        "  data_type: str = \"flip\" # bursty, holdout, no_support, flip\n",
        "  \n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "  batch_size: int = 1\n",
        "  optimize_step: int = int(5e4)\n",
        "  lr: float = 0.01\n",
        "  optimizer: str = \"sgd\" # adam, sgd, adamw\n",
        "\n",
        "@dataclass\n",
        "class MainConfig:\n",
        "  traindataconfig : TrainDataConfig = TrainDataConfig()\n",
        "  icldataconfig: ICLDataConfig = ICLDataConfig()\n",
        "  iwldataconfig: IWLDataConfig = IWLDataConfig()\n",
        "  icl2dataconfig: ICL2DataConfig = ICL2DataConfig()\n",
        "  modelconfig: TransformerConfig = TransformerConfig()\n",
        "  trainconfig: TrainConfig = TrainConfig()\n",
        "  device: str = \"cuda:1\"\n",
        "# define config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "U4wKEdZkNkC6"
      },
      "outputs": [],
      "source": [
        "def cal_acc(t,p):\n",
        "    p_arg = torch.argmax(p,dim=1)\n",
        "    return torch.sum(t == p_arg) / p.shape[0]\n",
        "def to_gpu_dict(dic):\n",
        "    dic = {k:v.to(\"cuda:1\") for k,v in dic.items()}\n",
        "    return dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_class tensor([[31, 34,  6, 28, 42, 45,  3, 27, 42]])\n",
            "train_label tensor([[14, 27, 15, 29, 30, 18,  7,  6, 30]])\n",
            "train_class tensor([[0, 0, 0, 1, 1, 0, 1, 1, 0]])\n",
            "train_label tensor([[16, 16, 16,  5,  5, 16,  5,  5, 16]])\n",
            "train_class tensor([[1, 0, 0, 1, 1, 0, 0, 1, 1]])\n",
            "train_label tensor([[ 5, 16, 16,  5,  5, 16, 16,  5,  5]])\n",
            "iwl_class tensor([[45, 42,  1,  4, 42, 50, 48, 39, 27]])\n",
            "iwl_label tensor([[18, 30,  5, 28, 30, 20,  4, 31,  6]])\n",
            "iwl_class tensor([[14, 49,  8, 17, 37, 42,  8, 50, 47]])\n",
            "iwl_label tensor([[28, 15,  6, 25, 14, 30,  6, 20,  4]])\n",
            "iwl_class tensor([[11, 38, 11,  9, 43, 39, 45, 10, 42]])\n",
            "iwl_label tensor([[23, 26, 23, 26,  7, 31, 18, 16, 30]])\n",
            "icl2_class tensor([[0, 0, 0, 1, 1, 0, 1, 1, 1]])\n",
            "icl2_label tensor([[17, 17, 17,  6,  6, 17,  6,  6,  6]])\n",
            "icl2_class tensor([[0, 1, 1, 0, 0, 1, 0, 1, 0]])\n",
            "icl2_label tensor([[17,  6,  6, 17, 17,  6, 17,  6, 17]])\n",
            "icl2_class tensor([[0, 0, 1, 1, 1, 1, 0, 0, 1]])\n",
            "icl2_label tensor([[17, 17,  6,  6,  6,  6, 17, 17,  6]])\n",
            "icl_class tensor([[59, 59, 54, 59, 54, 54, 59, 54, 59]])\n",
            "icl_label tensor([[12, 12, 11, 12, 11, 11, 12, 11, 12]])\n",
            "icl_class tensor([[60, 63, 63, 60, 63, 63, 60, 60, 60]])\n",
            "icl_label tensor([[16,  0,  0, 16,  0,  0, 16, 16, 16]])\n",
            "icl_class tensor([[59, 60, 60, 59, 59, 59, 60, 60, 59]])\n",
            "icl_label tensor([[12, 16, 16, 12, 12, 12, 16, 16, 12]])\n"
          ]
        }
      ],
      "source": [
        "traindataconfig = MainConfig.traindataconfig\n",
        "icldataconfig = MainConfig.icldataconfig\n",
        "iwldataconfig = MainConfig.iwldataconfig\n",
        "icl2dataconfig = MainConfig.icl2dataconfig\n",
        "trainconfig = MainConfig.trainconfig\n",
        "\n",
        "Dataset = SamplingDataset(traindataconfig)\n",
        "\n",
        "trainloader = SamplingLoader(traindataconfig, dataset=Dataset)\n",
        "train_seq_generator = trainloader.get_seq\n",
        "train_dataset = IterDataset(train_seq_generator)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "iclloader = SamplingLoader(icldataconfig, dataset=Dataset)\n",
        "icl_seq_generator = iclloader.get_seq\n",
        "icl_dataset = IterDataset(icl_seq_generator)\n",
        "icl_dataloader = torch.utils.data.DataLoader(icl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "iwlloader = SamplingLoader(iwldataconfig, dataset=Dataset)\n",
        "iwl_seq_generator = iwlloader.get_seq\n",
        "iwl_dataset = IterDataset(iwl_seq_generator)\n",
        "iwl_dataloader = torch.utils.data.DataLoader(iwl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "icl2loader = SamplingLoader(icl2dataconfig, dataset=Dataset)\n",
        "icl2_seq_generator = icl2loader.get_seq\n",
        "icl2_dataset = IterDataset(icl2_seq_generator)\n",
        "icl2_dataloader = torch.utils.data.DataLoader(icl2_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "cnt = 0\n",
        "for data in train_dataloader:\n",
        "    examples = data[\"examples\"]\n",
        "    labels = data[\"labels\"]\n",
        "    classes = data[\"classes\"]\n",
        "    # print(examples)\n",
        "    print(\"train_class\", classes)\n",
        "    print(\"train_label\", labels)\n",
        "    cnt += 1\n",
        "    if cnt > 2:\n",
        "        break\n",
        "cnt = 0\n",
        "for data in iwl_dataloader:\n",
        "    examples = data[\"examples\"]\n",
        "    labels = data[\"labels\"]\n",
        "    classes = data[\"classes\"]\n",
        "    # print(examples)\n",
        "    print(\"iwl_class\", classes)\n",
        "    print(\"iwl_label\", labels)\n",
        "    cnt += 1\n",
        "    if cnt > 2:\n",
        "        break\n",
        "cnt = 0\n",
        "for data in icl2_dataloader:\n",
        "    examples = data[\"examples\"]\n",
        "    labels = data[\"labels\"]\n",
        "    classes = data[\"classes\"]\n",
        "    # print(examples)\n",
        "    print(\"icl2_class\", classes)\n",
        "    print(\"icl2_label\", labels)\n",
        "    cnt += 1\n",
        "    if cnt > 2:\n",
        "        break\n",
        "cnt = 0\n",
        "for data in icl_dataloader:\n",
        "    examples = data[\"examples\"]\n",
        "    labels = data[\"labels\"]\n",
        "    classes = data[\"classes\"]\n",
        "    # print(examples\n",
        "    print(\"icl_class\", classes)\n",
        "    print(\"icl_label\", labels)\n",
        "    cnt += 1\n",
        "    if cnt > 2:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "12621ac3883347eba74b077797cda2b9",
            "5b7930a79e344c358f1ca175e9be4fd1",
            "d4e89f0d98b44eb4b76e59caa06aea14",
            "494267ae30c44a3bb446eaee6dc43115",
            "1a6e230516ff4996bb34f8a4992e1a20",
            "065c4cf26dd24c0d8caaa7165e800ca7",
            "aea29e116b304946b7d98692deb2addb",
            "9b3c46b8ff12462b9c4d2f0fbf12c767"
          ]
        },
        "id": "jwic1RdJGRDT",
        "outputId": "4663b1f4-a8bd-4983-f03d-8d0b9ac74208"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgouki\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/workspace/induction-head/wandb/run-20240305_163359-thz03x9z</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gouki/icl-minima/runs/thz03x9z' target=\"_blank\">deep-gorge-1</a></strong> to <a href='https://wandb.ai/gouki/icl-minima' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gouki/icl-minima' target=\"_blank\">https://wandb.ai/gouki/icl-minima</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gouki/icl-minima/runs/thz03x9z' target=\"_blank\">https://wandb.ai/gouki/icl-minima/runs/thz03x9z</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 1, 0, 1, 2, 0, 0, 2, 2]], device='cuda:1') tensor([[2, 0, 2, 0, 3, 2, 2, 3, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 2, 0, 2, 2, 0, 2, 1, 0]], device='cuda:1') tensor([[1, 0, 3, 0, 0, 3, 0, 1, 3]], device='cuda:1')\n",
            " 0 0.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 0, 2, 1, 2, 0, 0, 0, 0]], device='cuda:1') tensor([[3, 2, 3, 0, 3, 2, 2, 2, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 0, 2, 1, 0, 0, 1, 2, 0]], device='cuda:1') tensor([[3, 3, 0, 1, 3, 3, 1, 0, 3]], device='cuda:1')\n",
            " 1 0.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 2, 1, 0, 1, 0, 0, 0, 2]], device='cuda:1') tensor([[3, 3, 0, 2, 0, 2, 2, 2, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 2, 2, 1, 0, 1, 0, 0]], device='cuda:1') tensor([[3, 1, 0, 0, 1, 3, 1, 3, 3]], device='cuda:1')\n",
            " 2 1.0 1.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 0, 2, 1, 1, 0, 0, 0, 0]], device='cuda:1') tensor([[3, 2, 3, 0, 0, 2, 2, 2, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 2, 1, 0, 0, 0, 0, 1, 1]], device='cuda:1') tensor([[1, 0, 1, 3, 3, 3, 3, 1, 1]], device='cuda:1')\n",
            " 3 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 0, 2, 0, 0, 0, 0, 2, 0]], device='cuda:1') tensor([[2, 2, 3, 2, 2, 2, 2, 3, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 1, 2, 0, 0, 1, 1, 0, 2]], device='cuda:1') tensor([[1, 1, 0, 3, 3, 1, 1, 3, 0]], device='cuda:1')\n",
            " 4 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 2, 2, 1, 1, 0, 0, 0, 1]], device='cuda:1') tensor([[3, 3, 3, 0, 0, 2, 2, 2, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 2, 2, 2, 2, 1, 1, 2]], device='cuda:1') tensor([[3, 1, 0, 0, 0, 0, 1, 1, 0]], device='cuda:1')\n",
            " 5 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 1, 0, 1, 1, 2, 1, 1, 0]], device='cuda:1') tensor([[2, 0, 2, 0, 0, 3, 0, 0, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 0, 2, 0, 0, 2, 1, 0]], device='cuda:1') tensor([[3, 1, 3, 0, 3, 3, 0, 1, 3]], device='cuda:1')\n",
            " 6 0.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 1, 1, 2, 0, 1, 0, 2, 0]], device='cuda:1') tensor([[3, 0, 0, 3, 2, 0, 2, 3, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 2, 1, 1, 0, 2, 2, 2, 2]], device='cuda:1') tensor([[3, 0, 1, 1, 3, 0, 0, 0, 0]], device='cuda:1')\n",
            " 7 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 1, 2, 2, 0, 0, 1, 0, 2]], device='cuda:1') tensor([[3, 0, 3, 3, 2, 2, 0, 2, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 2, 1, 2, 2, 1, 0, 0, 1]], device='cuda:1') tensor([[3, 0, 1, 0, 0, 1, 3, 3, 1]], device='cuda:1')\n",
            " 8 0.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 0, 0, 1, 0, 0, 0, 2, 1]], device='cuda:1') tensor([[2, 2, 2, 0, 2, 2, 2, 3, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 2, 2, 0, 1, 0, 0, 1]], device='cuda:1') tensor([[3, 1, 0, 0, 3, 1, 3, 3, 1]], device='cuda:1')\n",
            " 9 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 1, 0, 1, 1, 1, 2, 0, 1]], device='cuda:1') tensor([[3, 0, 2, 0, 0, 0, 3, 2, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 2, 2, 0, 1, 1, 1, 2, 2]], device='cuda:1') tensor([[1, 0, 0, 3, 1, 1, 1, 0, 0]], device='cuda:1')\n",
            " 10 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 0, 0, 0, 1, 1, 0, 0, 1]], device='cuda:1') tensor([[0, 2, 2, 2, 0, 0, 2, 2, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 0, 2, 0, 2, 0, 2, 2, 2]], device='cuda:1') tensor([[0, 3, 0, 3, 0, 3, 0, 0, 0]], device='cuda:1')\n",
            " 11 0.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 1, 1, 2, 1, 2, 1, 1, 1]], device='cuda:1') tensor([[0, 0, 0, 3, 0, 3, 0, 0, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 0, 0, 1, 0, 2, 2, 1, 2]], device='cuda:1') tensor([[3, 3, 3, 1, 3, 0, 0, 1, 0]], device='cuda:1')\n",
            " 12 0.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 0, 1, 0, 1, 1, 0, 2, 1]], device='cuda:1') tensor([[2, 2, 0, 2, 0, 0, 2, 3, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 0, 0, 2, 1, 1, 2, 0]], device='cuda:1') tensor([[0, 0, 3, 3, 0, 1, 1, 0, 3]], device='cuda:1')\n",
            " 13 1.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 2, 0, 2, 0, 2, 0, 0, 1]], device='cuda:1') tensor([[0, 3, 2, 3, 2, 3, 2, 2, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 2, 1, 1, 2, 0, 2, 0, 2]], device='cuda:1') tensor([[3, 0, 1, 1, 0, 3, 0, 3, 0]], device='cuda:1')\n",
            " 14 1.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 0, 2, 2, 2, 2, 1, 0, 1]], device='cuda:1') tensor([[3, 2, 3, 3, 3, 3, 0, 2, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 2, 1, 2, 1, 0, 1, 1, 1]], device='cuda:1') tensor([[3, 0, 1, 0, 1, 3, 1, 1, 1]], device='cuda:1')\n",
            " 15 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 0, 2, 1, 0, 2, 1, 0, 2]], device='cuda:1') tensor([[0, 2, 3, 0, 2, 3, 0, 2, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 2, 1, 1, 1, 2, 2, 0]], device='cuda:1') tensor([[3, 1, 0, 1, 1, 1, 0, 0, 3]], device='cuda:1')\n",
            " 16 0.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 2, 0, 1, 1, 0, 0, 0, 0]], device='cuda:1') tensor([[0, 3, 2, 0, 0, 2, 2, 2, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 1, 2, 1, 0, 2, 0, 2, 2]], device='cuda:1') tensor([[1, 1, 0, 1, 3, 0, 3, 0, 0]], device='cuda:1')\n",
            " 17 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 0, 1, 0, 1, 2, 1, 2, 0]], device='cuda:1') tensor([[3, 2, 0, 2, 0, 3, 0, 3, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 2, 1, 1, 0, 0, 0, 1]], device='cuda:1') tensor([[0, 0, 0, 1, 1, 3, 3, 3, 1]], device='cuda:1')\n",
            " 18 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 2, 2, 0, 1, 1, 2, 2, 0]], device='cuda:1') tensor([[3, 3, 3, 2, 0, 0, 3, 3, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 1, 2, 0, 0, 0, 1, 2, 0]], device='cuda:1') tensor([[1, 1, 0, 3, 3, 3, 1, 0, 3]], device='cuda:1')\n",
            " 19 0.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 2, 2, 2, 1, 2, 1, 0, 0]], device='cuda:1') tensor([[3, 3, 3, 3, 0, 3, 0, 2, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 0, 0, 2, 2, 1, 0, 2, 2]], device='cuda:1') tensor([[0, 3, 3, 0, 0, 1, 3, 0, 0]], device='cuda:1')\n",
            " 20 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 1, 0, 1, 0, 1, 1, 2, 0]], device='cuda:1') tensor([[3, 0, 2, 0, 2, 0, 0, 3, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 1, 0, 1, 0, 0, 2, 0, 2]], device='cuda:1') tensor([[0, 1, 3, 1, 3, 3, 0, 3, 0]], device='cuda:1')\n",
            " 21 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0]], device='cuda:1') tensor([[2, 2, 0, 2, 2, 2, 2, 2, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 0, 0, 1, 0, 2, 0, 0, 0]], device='cuda:1') tensor([[3, 3, 3, 1, 3, 0, 3, 3, 3]], device='cuda:1')\n",
            " 22 0.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 2, 2, 1, 2, 0, 2, 0, 1]], device='cuda:1') tensor([[3, 3, 3, 0, 3, 2, 3, 2, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 1, 0, 0, 0, 2, 0, 2, 1]], device='cuda:1') tensor([[1, 1, 3, 3, 3, 0, 3, 0, 1]], device='cuda:1')\n",
            " 23 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 0, 0, 2, 2, 2, 2, 2, 2]], device='cuda:1') tensor([[3, 2, 2, 3, 3, 3, 3, 3, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 0, 2, 2, 0, 2, 2, 0, 0]], device='cuda:1') tensor([[3, 3, 0, 0, 3, 0, 0, 3, 3]], device='cuda:1')\n",
            " 24 0.0 1.0 1.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 1, 0, 2, 0, 2, 0, 0, 2]], device='cuda:1') tensor([[2, 0, 2, 3, 2, 3, 2, 2, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 0, 2, 2, 1, 2, 0, 0, 0]], device='cuda:1') tensor([[3, 3, 0, 0, 1, 0, 3, 3, 3]], device='cuda:1')\n",
            " 25 1.0 1.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 2, 0, 2, 1, 0, 2, 0, 0]], device='cuda:1') tensor([[3, 3, 2, 3, 0, 2, 3, 2, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 1, 1, 0, 0, 1, 1, 0, 1]], device='cuda:1') tensor([[1, 1, 1, 3, 3, 1, 1, 3, 1]], device='cuda:1')\n",
            " 26 1.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 2, 0, 1, 1, 0, 2, 2, 0]], device='cuda:1') tensor([[0, 3, 2, 0, 0, 2, 3, 3, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 0, 1, 0, 0, 2, 2, 0]], device='cuda:1') tensor([[0, 0, 3, 1, 3, 3, 0, 0, 3]], device='cuda:1')\n",
            " 27 0.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 1, 2, 0, 1, 0, 0, 1, 2]], device='cuda:1') tensor([[0, 0, 3, 2, 0, 2, 2, 0, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 2, 2, 1, 0, 2, 1, 2]], device='cuda:1') tensor([[0, 0, 0, 0, 1, 3, 0, 1, 0]], device='cuda:1')\n",
            " 28 1.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 2, 2, 1, 0, 2, 1, 0, 0]], device='cuda:1') tensor([[2, 3, 3, 0, 2, 3, 0, 2, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 2, 1, 0, 0, 2, 0, 2, 0]], device='cuda:1') tensor([[3, 0, 1, 3, 3, 0, 3, 0, 3]], device='cuda:1')\n",
            " 29 1.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 2, 0, 0, 0, 1, 2, 1, 0]], device='cuda:1') tensor([[3, 3, 2, 2, 2, 0, 3, 0, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 0, 0, 0, 1, 1, 0, 0, 2]], device='cuda:1') tensor([[0, 3, 3, 3, 1, 1, 3, 3, 0]], device='cuda:1')\n",
            " 30 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 1, 2, 2, 1, 0, 1, 0, 0]], device='cuda:1') tensor([[2, 0, 3, 3, 0, 2, 0, 2, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 2, 0, 2, 1, 1, 2, 0, 0]], device='cuda:1') tensor([[1, 0, 3, 0, 1, 1, 0, 3, 3]], device='cuda:1')\n",
            " 31 1.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 0, 2, 1, 0, 0, 1, 0, 1]], device='cuda:1') tensor([[0, 2, 3, 0, 2, 2, 0, 2, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 1, 0, 1, 2, 1, 1, 1, 0]], device='cuda:1') tensor([[1, 1, 3, 1, 0, 1, 1, 1, 3]], device='cuda:1')\n",
            " 32 0.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 0, 0, 1, 0, 0, 0, 1, 1]], device='cuda:1') tensor([[3, 2, 2, 0, 2, 2, 2, 0, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 0, 0, 0, 1, 0, 2, 2, 0]], device='cuda:1') tensor([[3, 3, 3, 3, 1, 3, 0, 0, 3]], device='cuda:1')\n",
            " 33 1.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 0, 2, 2, 1, 0, 2, 0, 0]], device='cuda:1') tensor([[3, 2, 3, 3, 0, 2, 3, 2, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 1, 0, 0, 2, 1, 1, 2, 0]], device='cuda:1') tensor([[0, 1, 3, 3, 0, 1, 1, 0, 3]], device='cuda:1')\n",
            " 34 1.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 2, 2, 2, 2, 2, 2, 1, 2]], device='cuda:1') tensor([[2, 3, 3, 3, 3, 3, 3, 0, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 1, 1, 2, 0, 1, 0, 1]], device='cuda:1') tensor([[3, 1, 1, 1, 0, 3, 1, 3, 1]], device='cuda:1')\n",
            " 35 0.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 1, 1, 0, 0, 1, 0, 2, 0]], device='cuda:1') tensor([[3, 0, 0, 2, 2, 0, 2, 3, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 2, 0, 0, 2, 1, 2, 0, 0]], device='cuda:1') tensor([[1, 0, 3, 3, 0, 1, 0, 3, 3]], device='cuda:1')\n",
            " 36 0.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 0, 0, 2, 1, 2, 0, 0, 2]], device='cuda:1') tensor([[2, 2, 2, 3, 0, 3, 2, 2, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 0, 1, 2, 0, 0, 1, 1, 0]], device='cuda:1') tensor([[1, 3, 1, 0, 3, 3, 1, 1, 3]], device='cuda:1')\n",
            " 37 0.0 1.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 2, 2, 2, 2, 1, 1, 1, 1]], device='cuda:1') tensor([[0, 3, 3, 3, 3, 0, 0, 0, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 1, 2, 1, 1, 0, 2, 2, 1]], device='cuda:1') tensor([[1, 1, 0, 1, 1, 3, 0, 0, 1]], device='cuda:1')\n",
            " 38 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 1, 1, 0, 2, 2, 2, 1, 0]], device='cuda:1') tensor([[3, 0, 0, 2, 3, 3, 3, 0, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 2, 2, 0, 2, 2, 2, 0, 0]], device='cuda:1') tensor([[3, 0, 0, 3, 0, 0, 0, 3, 3]], device='cuda:1')\n",
            " 39 0.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 2, 2, 2, 0, 1, 2, 2, 1]], device='cuda:1') tensor([[0, 3, 3, 3, 2, 0, 3, 3, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 0, 1, 1, 1, 2, 2, 1]], device='cuda:1') tensor([[3, 1, 3, 1, 1, 1, 0, 0, 1]], device='cuda:1')\n",
            " 40 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 0, 1, 2, 2, 2, 2, 1, 1]], device='cuda:1') tensor([[3, 2, 0, 3, 3, 3, 3, 0, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 1, 1, 0, 2, 2, 2, 2]], device='cuda:1') tensor([[3, 1, 1, 1, 3, 0, 0, 0, 0]], device='cuda:1')\n",
            " 41 1.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 1, 2, 1, 1, 0, 1, 2, 2]], device='cuda:1') tensor([[3, 0, 3, 0, 0, 2, 0, 3, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 1, 0, 1, 1, 1, 2, 1]], device='cuda:1') tensor([[3, 1, 1, 3, 1, 1, 1, 0, 1]], device='cuda:1')\n",
            " 42 1.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 2, 0, 0, 1, 2, 0, 2, 0]], device='cuda:1') tensor([[3, 3, 2, 2, 0, 3, 2, 3, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 1, 0, 1, 0, 0, 1, 1, 1]], device='cuda:1') tensor([[1, 1, 3, 1, 3, 3, 1, 1, 1]], device='cuda:1')\n",
            " 43 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 1, 1, 0, 1, 0, 2, 1, 2]], device='cuda:1') tensor([[2, 0, 0, 2, 0, 2, 3, 0, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 1, 0, 1, 0, 0, 2, 2, 0]], device='cuda:1') tensor([[0, 1, 3, 1, 3, 3, 0, 0, 3]], device='cuda:1')\n",
            " 44 1.0 1.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 2, 0, 2, 0, 2, 1, 1, 1]], device='cuda:1') tensor([[3, 3, 2, 3, 2, 3, 0, 0, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 1, 2, 2, 2, 1, 1, 2]], device='cuda:1') tensor([[3, 1, 1, 0, 0, 0, 1, 1, 0]], device='cuda:1')\n",
            " 45 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 2, 2, 0, 1, 1, 0, 0, 1]], device='cuda:1') tensor([[0, 3, 3, 2, 0, 0, 2, 2, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 1, 0, 0, 1, 1, 1, 1, 1]], device='cuda:1') tensor([[1, 1, 3, 3, 1, 1, 1, 1, 1]], device='cuda:1')\n",
            " 46 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 1, 0, 1, 2, 1, 2, 2, 2]], device='cuda:1') tensor([[0, 0, 2, 0, 3, 0, 3, 3, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 1, 1, 1, 0, 0, 0, 2]], device='cuda:1') tensor([[0, 0, 1, 1, 1, 3, 3, 3, 0]], device='cuda:1')\n",
            " 47 0.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 1, 2, 0, 1, 0, 1, 1, 2]], device='cuda:1') tensor([[3, 0, 3, 2, 0, 2, 0, 0, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 0, 2, 0, 0, 2, 0, 2, 2]], device='cuda:1') tensor([[3, 3, 0, 3, 3, 0, 3, 0, 0]], device='cuda:1')\n",
            " 48 1.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 0, 1, 1, 1, 2, 2, 1, 2]], device='cuda:1') tensor([[2, 2, 0, 0, 0, 3, 3, 0, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 1, 2, 2, 2, 0, 0, 0, 2]], device='cuda:1') tensor([[1, 1, 0, 0, 0, 3, 3, 3, 0]], device='cuda:1')\n",
            " 49 0.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 0, 0, 1, 1, 1, 1, 1, 0]], device='cuda:1') tensor([[3, 2, 2, 0, 0, 0, 0, 0, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 1, 1, 1, 0, 2, 0, 1]], device='cuda:1') tensor([[0, 0, 1, 1, 1, 3, 0, 3, 1]], device='cuda:1')\n",
            " 50 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 0, 1, 1, 1, 1, 1, 1, 2]], device='cuda:1') tensor([[2, 2, 0, 0, 0, 0, 0, 0, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 0, 1, 0, 1, 1, 1, 1]], device='cuda:1') tensor([[3, 1, 3, 1, 3, 1, 1, 1, 1]], device='cuda:1')\n",
            " 51 1.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 2, 2, 1, 1, 0, 1, 2, 2]], device='cuda:1') tensor([[0, 3, 3, 0, 0, 2, 0, 3, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 0, 1, 2, 1, 0, 1, 1]], device='cuda:1') tensor([[0, 0, 3, 1, 0, 1, 3, 1, 1]], device='cuda:1')\n",
            " 52 0.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 0, 1, 2, 1, 2, 2, 2, 2]], device='cuda:1') tensor([[0, 2, 0, 3, 0, 3, 3, 3, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 0, 2, 0, 0, 2, 1, 2]], device='cuda:1') tensor([[0, 0, 3, 0, 3, 3, 0, 1, 0]], device='cuda:1')\n",
            " 53 0.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 1, 0, 1, 2, 0, 1, 2, 0]], device='cuda:1') tensor([[0, 0, 2, 0, 3, 2, 0, 3, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 1, 0, 1, 1, 2, 0, 1]], device='cuda:1') tensor([[0, 0, 1, 3, 1, 1, 0, 3, 1]], device='cuda:1')\n",
            " 54 0.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 2, 0, 0, 1, 1, 1, 0, 1]], device='cuda:1') tensor([[3, 3, 2, 2, 0, 0, 0, 2, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 1, 2, 1, 0, 2, 1, 2, 0]], device='cuda:1') tensor([[0, 1, 0, 1, 3, 0, 1, 0, 3]], device='cuda:1')\n",
            " 55 0.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 1, 1, 1, 0, 2, 1, 2, 2]], device='cuda:1') tensor([[3, 0, 0, 0, 2, 3, 0, 3, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 2, 0, 1, 2, 0, 0, 2, 2]], device='cuda:1') tensor([[3, 0, 3, 1, 0, 3, 3, 0, 0]], device='cuda:1')\n",
            " 56 0.0 0.0 1.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 0, 1, 0, 2, 2, 1, 1, 0]], device='cuda:1') tensor([[0, 2, 0, 2, 3, 3, 0, 0, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 2, 2, 2, 2, 1, 1, 0, 0]], device='cuda:1') tensor([[3, 0, 0, 0, 0, 1, 1, 3, 3]], device='cuda:1')\n",
            " 57 1.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 0, 1, 0, 0, 2, 1, 0, 1]], device='cuda:1') tensor([[2, 2, 0, 2, 2, 3, 0, 2, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 0, 0, 1, 1, 2, 1, 1]], device='cuda:1') tensor([[0, 0, 3, 3, 1, 1, 0, 1, 1]], device='cuda:1')\n",
            " 58 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 2, 0, 2, 0, 1, 1, 2, 2]], device='cuda:1') tensor([[0, 3, 2, 3, 2, 0, 0, 3, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 0, 0, 1, 1, 0, 2, 0, 0]], device='cuda:1') tensor([[1, 3, 3, 1, 1, 3, 0, 3, 3]], device='cuda:1')\n",
            " 59 1.0 1.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 0, 1, 2, 0, 1, 1, 1, 2]], device='cuda:1') tensor([[0, 2, 0, 3, 2, 0, 0, 0, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 2, 1, 0, 0, 2, 2, 1, 2]], device='cuda:1') tensor([[1, 0, 1, 3, 3, 0, 0, 1, 0]], device='cuda:1')\n",
            " 60 0.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 2, 1, 1, 0, 2, 1, 2, 1]], device='cuda:1') tensor([[0, 3, 0, 0, 2, 3, 0, 3, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 0, 0, 1, 2, 1, 1, 0, 1]], device='cuda:1') tensor([[3, 3, 3, 1, 0, 1, 1, 3, 1]], device='cuda:1')\n",
            " 61 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 1, 2, 1, 0, 1, 2, 0, 2]], device='cuda:1') tensor([[3, 0, 3, 0, 2, 0, 3, 2, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 0, 1, 0, 0, 1, 2, 0, 0]], device='cuda:1') tensor([[3, 3, 1, 3, 3, 1, 0, 3, 3]], device='cuda:1')\n",
            " 62 0.0 1.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 2, 2, 1, 0, 1, 1, 1, 2]], device='cuda:1') tensor([[3, 3, 3, 0, 2, 0, 0, 0, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 0, 0, 2, 2, 2, 0, 2, 2]], device='cuda:1') tensor([[1, 3, 3, 0, 0, 0, 3, 0, 0]], device='cuda:1')\n",
            " 63 0.0 1.0 1.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 2, 1, 0, 1, 1, 1, 1, 0]], device='cuda:1') tensor([[3, 3, 0, 2, 0, 0, 0, 0, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 1, 2, 2, 2, 0, 0, 2]], device='cuda:1') tensor([[3, 1, 1, 0, 0, 0, 3, 3, 0]], device='cuda:1')\n",
            " 64 0.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 0, 0, 1, 1, 0, 0, 1, 1]], device='cuda:1') tensor([[2, 2, 2, 0, 0, 2, 2, 0, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 1, 1, 0, 2, 1, 1, 2, 1]], device='cuda:1') tensor([[1, 1, 1, 3, 0, 1, 1, 0, 1]], device='cuda:1')\n",
            " 65 0.0 1.0 1.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 0, 0, 0, 0, 0, 1, 2, 2]], device='cuda:1') tensor([[0, 2, 2, 2, 2, 2, 0, 3, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 2, 2, 2, 1, 0, 2, 2]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 1, 3, 0, 0]], device='cuda:1')\n",
            " 66 0.0 1.0 1.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 0, 0, 1, 1, 2, 2, 1, 2]], device='cuda:1') tensor([[0, 2, 2, 0, 0, 3, 3, 0, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 2, 2, 1, 0, 1, 0, 0, 1]], device='cuda:1') tensor([[1, 0, 0, 1, 3, 1, 3, 3, 1]], device='cuda:1')\n",
            " 67 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 1, 2, 1, 1, 0, 1, 0, 0]], device='cuda:1') tensor([[0, 0, 3, 0, 0, 2, 0, 2, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 2, 0, 2, 2, 1, 1, 0, 2]], device='cuda:1') tensor([[3, 0, 3, 0, 0, 1, 1, 3, 0]], device='cuda:1')\n",
            " 68 0.0 0.0 1.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 1, 2, 0, 0, 0, 1, 2, 0]], device='cuda:1') tensor([[0, 0, 3, 2, 2, 2, 0, 3, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 2, 2, 1, 2, 0, 0, 1, 1]], device='cuda:1') tensor([[1, 0, 0, 1, 0, 3, 3, 1, 1]], device='cuda:1')\n",
            " 69 1.0 0.0 1.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 0, 0, 1, 0, 0, 0, 2, 0]], device='cuda:1') tensor([[0, 2, 2, 0, 2, 2, 2, 3, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 2, 0, 2, 0, 0, 1, 0]], device='cuda:1') tensor([[3, 1, 0, 3, 0, 3, 3, 1, 3]], device='cuda:1')\n",
            " 70 1.0 0.0 1.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 0, 1, 1, 0, 2, 2, 1, 0]], device='cuda:1') tensor([[3, 2, 0, 0, 2, 3, 3, 0, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 2, 1, 2, 2, 2, 1, 1, 2]], device='cuda:1') tensor([[3, 0, 1, 0, 0, 0, 1, 1, 0]], device='cuda:1')\n",
            " 71 0.0 0.0 1.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 0, 0, 1, 1, 2, 1, 1, 1]], device='cuda:1') tensor([[3, 2, 2, 0, 0, 3, 0, 0, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 2, 1, 2, 0, 1, 1, 1, 1]], device='cuda:1') tensor([[1, 0, 1, 0, 3, 1, 1, 1, 1]], device='cuda:1')\n",
            " 72 1.0 1.0 1.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 2, 2, 2, 2, 1, 2, 0, 2]], device='cuda:1') tensor([[0, 3, 3, 3, 3, 0, 3, 2, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 2, 1, 2, 1, 1, 2, 2, 1]], device='cuda:1') tensor([[3, 0, 1, 0, 1, 1, 0, 0, 1]], device='cuda:1')\n",
            " 73 1.0 0.0 1.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 2, 2, 1, 2, 2, 0, 1, 1]], device='cuda:1') tensor([[2, 3, 3, 0, 3, 3, 2, 0, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 1, 0, 0, 1, 1, 2, 2]], device='cuda:1') tensor([[0, 0, 1, 3, 3, 1, 1, 0, 0]], device='cuda:1')\n",
            " 74 1.0 1.0 1.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 2, 2, 0, 0, 2, 1, 1, 0]], device='cuda:1') tensor([[0, 3, 3, 2, 2, 3, 0, 0, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 1, 0, 0, 1, 0, 2, 2]], device='cuda:1') tensor([[0, 0, 1, 3, 3, 1, 3, 0, 0]], device='cuda:1')\n",
            " 75 1.0 0.0 1.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 1, 1, 2, 1, 2, 2, 1, 0]], device='cuda:1') tensor([[0, 0, 0, 3, 0, 3, 3, 0, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 2, 0, 0, 1, 0, 1, 1]], device='cuda:1') tensor([[0, 0, 0, 3, 3, 1, 3, 1, 1]], device='cuda:1')\n",
            " 76 1.0 0.0 1.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 1, 1, 0, 0, 1, 0, 0, 2]], device='cuda:1') tensor([[3, 0, 0, 2, 2, 0, 2, 2, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 2, 0, 2, 1, 0, 2, 2, 2]], device='cuda:1') tensor([[1, 0, 3, 0, 1, 3, 0, 0, 0]], device='cuda:1')\n",
            " 77 0.0 0.0 1.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 2, 0, 1, 0, 2, 1, 1, 1]], device='cuda:1') tensor([[2, 3, 2, 0, 2, 3, 0, 0, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 2, 0, 1, 0, 0, 0, 0]], device='cuda:1') tensor([[3, 1, 0, 3, 1, 3, 3, 3, 3]], device='cuda:1')\n",
            " 78 0.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 0, 0, 0, 1, 0, 2, 1, 1]], device='cuda:1') tensor([[2, 2, 2, 2, 0, 2, 3, 0, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 1, 1, 0, 0, 0, 0, 1]], device='cuda:1') tensor([[3, 1, 1, 1, 3, 3, 3, 3, 1]], device='cuda:1')\n",
            " 79 0.0 1.0 1.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 0, 0, 2, 0, 1, 1, 0, 0]], device='cuda:1') tensor([[3, 2, 2, 3, 2, 0, 0, 2, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 2, 2, 2, 1, 1, 1, 1]], device='cuda:1') tensor([[3, 1, 0, 0, 0, 1, 1, 1, 1]], device='cuda:1')\n",
            " 80 1.0 0.0 1.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 2, 1, 1, 1, 1, 0, 1, 0]], device='cuda:1') tensor([[0, 3, 0, 0, 0, 0, 2, 0, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 1, 0, 2, 1, 2, 2, 0]], device='cuda:1') tensor([[3, 1, 1, 3, 0, 1, 0, 0, 3]], device='cuda:1')\n",
            " 81 0.0 0.0 1.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 1, 2, 2, 2, 0, 2, 2, 1]], device='cuda:1') tensor([[3, 0, 3, 3, 3, 2, 3, 3, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 0, 2, 0, 2, 1, 2, 1, 1]], device='cuda:1') tensor([[1, 3, 0, 3, 0, 1, 0, 1, 1]], device='cuda:1')\n",
            " 82 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 0, 2, 1, 2, 2, 2, 2, 2]], device='cuda:1') tensor([[3, 2, 3, 0, 3, 3, 3, 3, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 2, 2, 0, 0, 0, 2, 2]], device='cuda:1') tensor([[3, 1, 0, 0, 3, 3, 3, 0, 0]], device='cuda:1')\n",
            " 83 1.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 0, 1, 1, 1, 2, 0, 0, 0]], device='cuda:1') tensor([[2, 2, 0, 0, 0, 3, 2, 2, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 2, 2, 0, 1, 1, 2, 1]], device='cuda:1') tensor([[0, 0, 0, 0, 3, 1, 1, 0, 1]], device='cuda:1')\n",
            " 84 0.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 1, 0, 2, 0, 0, 2, 0, 2]], device='cuda:1') tensor([[2, 0, 2, 3, 2, 2, 3, 2, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[2, 2, 2, 1, 2, 1, 0, 2, 2]], device='cuda:1') tensor([[0, 0, 0, 1, 0, 1, 3, 0, 0]], device='cuda:1')\n",
            " 85 1.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 2, 0, 0, 2, 0, 2, 2, 0]], device='cuda:1') tensor([[0, 3, 2, 2, 3, 2, 3, 3, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 1, 2, 0, 0, 2, 2, 2, 1]], device='cuda:1') tensor([[1, 1, 0, 3, 3, 0, 0, 0, 1]], device='cuda:1')\n",
            " 86 1.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 2, 2, 0, 2, 1, 1, 0, 2]], device='cuda:1') tensor([[2, 3, 3, 2, 3, 0, 0, 2, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 2, 0, 0, 2, 1, 0, 0, 2]], device='cuda:1') tensor([[3, 0, 3, 3, 0, 1, 3, 3, 0]], device='cuda:1')\n",
            " 87 0.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 1, 2, 0, 1, 0, 1, 2, 0]], device='cuda:1') tensor([[0, 0, 3, 2, 0, 2, 0, 3, 2]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 2, 1, 0, 2, 2, 2, 2, 2]], device='cuda:1') tensor([[1, 0, 1, 3, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            " 88 1.0 0.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[0, 1, 2, 1, 1, 1, 2, 1, 1]], device='cuda:1') tensor([[2, 0, 3, 0, 0, 0, 3, 0, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 2, 1, 0, 2, 0, 2, 2, 0]], device='cuda:1') tensor([[3, 0, 1, 3, 0, 3, 0, 0, 3]], device='cuda:1')\n",
            " 89 1.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 2, 2, 2, 0, 1, 0, 1, 1]], device='cuda:1') tensor([[0, 3, 3, 3, 2, 0, 2, 0, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 0, 1, 2, 0, 0, 2, 2, 0]], device='cuda:1') tensor([[3, 3, 1, 0, 3, 3, 0, 0, 3]], device='cuda:1')\n",
            " 90 1.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 0, 2, 1, 1, 1, 2, 0, 1]], device='cuda:1') tensor([[0, 2, 3, 0, 0, 0, 3, 2, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 0, 0, 1, 1, 0, 0, 0]], device='cuda:1') tensor([[3, 1, 3, 3, 1, 1, 3, 3, 3]], device='cuda:1')\n",
            " 91 0.0 0.0 0.0 1.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[1, 1, 2, 0, 2, 2, 2, 1, 2]], device='cuda:1') tensor([[0, 0, 3, 2, 3, 3, 3, 0, 3]], device='cuda:1')\n",
            "icl2_sample tensor([[1, 1, 1, 1, 0, 1, 2, 0, 1]], device='cuda:1') tensor([[1, 1, 1, 1, 3, 1, 0, 3, 1]], device='cuda:1')\n",
            " 92 0.0 1.0 0.0 0.0icl_sample tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3]], device='cuda:1') tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:1')\n",
            "iwl_sample tensor([[2, 1, 1, 1, 2, 1, 0, 0, 1]], device='cuda:1') tensor([[3, 0, 0, 0, 3, 0, 2, 2, 0]], device='cuda:1')\n",
            "icl2_sample tensor([[0, 1, 0, 1, 2, 2, 1, 1, 1]], device='cuda:1') tensor([[3, 1, 3, 1, 0, 0, 1, 1, 1]], device='cuda:1')\n",
            " 93 1.0 0.0 0.0 0.0"
          ]
        }
      ],
      "source": [
        "# train\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "config = MainConfig()\n",
        "wandb.init(project=\"icl-minima\", config=asdict(config))\n",
        "# data\n",
        "Dataset = SamplingDataset(traindataconfig)\n",
        "\n",
        "trainloader = SamplingLoader(traindataconfig, dataset=Dataset)\n",
        "train_seq_generator = trainloader.get_seq\n",
        "train_dataset = IterDataset(train_seq_generator)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "iclloader = SamplingLoader(icldataconfig, dataset=Dataset)\n",
        "icl_seq_generator = iclloader.get_seq\n",
        "icl_dataset = IterDataset(icl_seq_generator)\n",
        "icl_dataloader = torch.utils.data.DataLoader(icl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "iwlloader = SamplingLoader(iwldataconfig, dataset=Dataset)\n",
        "iwl_seq_generator = iwlloader.get_seq\n",
        "iwl_dataset = IterDataset(iwl_seq_generator)\n",
        "iwl_dataloader = torch.utils.data.DataLoader(iwl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "icl2loader = SamplingLoader(icl2dataconfig, dataset=Dataset)\n",
        "icl2_seq_generator = icl2loader.get_seq\n",
        "icl2_dataset = IterDataset(icl2_seq_generator)\n",
        "icl2_dataloader = torch.utils.data.DataLoader(icl2_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "# model\n",
        "embedder = InputEmbedder(config.modelconfig)\n",
        "model = TransformerICL(embedder, config.modelconfig)\n",
        "model.to(config.device)\n",
        "\n",
        "# optimizer\n",
        "if config.trainconfig.optimizer == \"adam\":\n",
        "  optimizer =  torch.optim.Adam(model.parameters(), lr=config.trainconfig.lr)\n",
        "elif config.trainconfig.optimizer == \"sgd\":\n",
        "  optimizer =  torch.optim.SGD(model.parameters(), lr=config.trainconfig.lr)\n",
        "elif config.trainconfig.optimizer == \"adamw\":\n",
        "  optimizer =  torch.optim.AdamW(model.parameters(), lr=config.trainconfig.lr)\n",
        "\n",
        "# loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "step = 0\n",
        "for (data_dict, icl_data_dict, iwl_data_dict, icl2_data_dict) in zip(train_dataloader, icl_dataloader, iwl_dataloader, icl2_dataloader):\n",
        "  model.train()   \n",
        "  data_dict = to_gpu_dict(data_dict)\n",
        "  icl_data_dict = to_gpu_dict(icl_data_dict)\n",
        "  iwl_data_dict = to_gpu_dict(iwl_data_dict)\n",
        "  icl2_data_dict = to_gpu_dict(icl2_data_dict)\n",
        "  \n",
        "  logits = model(data_dict[\"examples\"], data_dict[\"labels\"])\n",
        "  query_logit = logits[:,-1,:]\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  # print(data_dict[\"labels\"][:,-1])\n",
        "  loss = criterion(query_logit, data_dict[\"labels\"][:,-1],)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  train_acc = cal_acc(data_dict[\"labels\"][:, -1], query_logit)\n",
        "  # print(\"train_sample\", data_dict[\"classes\"], data_dict[\"labels\"])\n",
        "  wandb.log({\"train/acc\":train_acc,\"train/loss\":loss}, step=step)\n",
        "  with torch.no_grad():\n",
        "          model.eval()\n",
        "          logits = model(icl_data_dict[\"examples\"], icl_data_dict[\"labels\"])\n",
        "          query_logit = logits[:,-1,:]\n",
        "          icl_acc = cal_acc(icl_data_dict[\"labels\"][:, -1], query_logit)\n",
        "          # wandb.log({\"valid/icl_acc\":icl_acc}, step=step)\n",
        "          # print(\"\\r\",step, icl_acc, end=\"\")\n",
        "          print(\"icl_sample\", icl_data_dict[\"classes\"], icl_data_dict[\"labels\"])\n",
        "\n",
        "          logits = model(iwl_data_dict[\"examples\"], iwl_data_dict[\"labels\"])\n",
        "          query_logit = logits[:,-1,:]\n",
        "          iwl_acc = cal_acc(iwl_data_dict[\"labels\"][:, -1], query_logit)\n",
        "          # wandb.log({\"valid/iwl_acc\":iwl_acc}, step=step)\n",
        "          # print(\"\\r\",step, iwl_acc, end=\"\")\n",
        "          print(\"iwl_sample\", iwl_data_dict[\"classes\"], iwl_data_dict[\"labels\"])\n",
        "\n",
        "          logits = model(icl2_data_dict[\"examples\"], icl2_data_dict[\"labels\"])\n",
        "          query_logit = logits[:,-1,:]\n",
        "          icl2_acc = cal_acc(icl2_data_dict[\"labels\"][:, -1], query_logit)\n",
        "          # wandb.log({\"valid/icl2_acc\":icl2_acc}, step=step)\n",
        "          # print(\"\\r\",step, icl2_acc, end=\"\")\n",
        "          print(\"icl2_sample\", icl2_data_dict[\"classes\"], icl2_data_dict[\"labels\"])\n",
        "          \n",
        "  print(\"\\r\",step, train_acc.item(), iwl_acc.item(), icl_acc.item(), icl2_acc.item(), end=\"\")\n",
        "  step+=1\n",
        "  if step > config.trainconfig.optimize_step:\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4i_bXu6L5yF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "065c4cf26dd24c0d8caaa7165e800ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12621ac3883347eba74b077797cda2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b7930a79e344c358f1ca175e9be4fd1",
              "IPY_MODEL_d4e89f0d98b44eb4b76e59caa06aea14"
            ],
            "layout": "IPY_MODEL_494267ae30c44a3bb446eaee6dc43115"
          }
        },
        "1a6e230516ff4996bb34f8a4992e1a20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494267ae30c44a3bb446eaee6dc43115": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b7930a79e344c358f1ca175e9be4fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a6e230516ff4996bb34f8a4992e1a20",
            "placeholder": "",
            "style": "IPY_MODEL_065c4cf26dd24c0d8caaa7165e800ca7",
            "value": "0.010 MB of 0.010 MB uploaded\r"
          }
        },
        "9b3c46b8ff12462b9c4d2f0fbf12c767": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aea29e116b304946b7d98692deb2addb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4e89f0d98b44eb4b76e59caa06aea14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aea29e116b304946b7d98692deb2addb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b3c46b8ff12462b9c4d2f0fbf12c767",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
