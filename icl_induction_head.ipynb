{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4cb8GADCuFl_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Mar 11 06:37:53 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA RTX A6000               On  | 00000000:21:00.0 Off |                  Off |\n",
            "| 37%   65C    P8              26W / 200W |   6298MiB / 49140MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA RTX A6000               On  | 00000000:41:00.0 Off |                  Off |\n",
            "| 46%   72C    P2             101W / 200W |    397MiB / 49140MiB |     13%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   2  NVIDIA RTX A6000               On  | 00000000:43:00.0 Off |                  Off |\n",
            "| 57%   80C    P2             193W / 200W |  43989MiB / 49140MiB |     80%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzF5h6iNugUD"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pcrkWn3UuPgm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import einops\n",
        "\n",
        "\n",
        "class Unembed(nn.Module):\n",
        "    def __init__(self, d_vocab, d_model):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(d_vocab, d_model) / np.sqrt(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.einsum(\"pe,bse->bsp\", self.W, x)\n",
        "\n",
        "\n",
        "class HookPoint(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fwd_hooks = []\n",
        "        self.bwd_hooks = []\n",
        "\n",
        "    def give_name(self, name):\n",
        "        # Called by the model at initialisation\n",
        "        self.name = name\n",
        "\n",
        "    def add_hook(self, hook, dir=\"fwd\"):\n",
        "        # Hook format is fn(activation, hook_name)\n",
        "        # Change it into PyTorch hook format (this includes input and output,\n",
        "        # which are the same for a HookPoint)\n",
        "        def full_hook(module, module_input, module_output):\n",
        "            return hook(module_output, name=self.name)\n",
        "\n",
        "        if dir == \"fwd\":\n",
        "            handle = self.register_forward_hook(full_hook)\n",
        "            self.fwd_hooks.append(handle)\n",
        "        elif dir == \"bwd\":\n",
        "            handle = self.register_backward_hook(full_hook)\n",
        "            self.bwd_hooks.append(handle)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid direction {dir}\")\n",
        "\n",
        "    def remove_hooks(self, dir=\"fwd\"):\n",
        "        if (dir == \"fwd\") or (dir == \"both\"):\n",
        "            for hook in self.fwd_hooks:\n",
        "                hook.remove()\n",
        "            self.fwd_hooks = []\n",
        "        if (dir == \"bwd\") or (dir == \"both\"):\n",
        "            for hook in self.bwd_hooks:\n",
        "                hook.remove()\n",
        "            self.bwd_hooks = []\n",
        "        if dir not in [\"fwd\", \"bwd\", \"both\"]:\n",
        "            raise ValueError(f\"Invalid direction {dir}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "class PosEmbed(nn.Module):\n",
        "    def __init__(self, max_ctx, d_model, weight_scale=1):\n",
        "        super().__init__()\n",
        "        self.W_pos = nn.Parameter(torch.randn(max_ctx, d_model) * weight_scale)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.W_pos[: x.shape[-2]]\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, epsilon=1e-4, model=[None]):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.w_ln = nn.Parameter(torch.ones(d_model))\n",
        "        self.b_ln = nn.Parameter(torch.zeros(d_model))\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.model[0].use_ln:\n",
        "            x = x - x.mean(axis=-1)[..., None]\n",
        "            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n",
        "            x = x * self.w_ln\n",
        "            x = x + self.b_ln\n",
        "            return x\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "# Attention\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    b : batch size\n",
        "    d : embedding size of token\n",
        "    p : vocabraly size (113 or 3)\n",
        "    i : number of heads\n",
        "    h : embedding size of each heads\n",
        "    n_ctx : token size\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, d_head, n_ctx):\n",
        "        super().__init__()\n",
        "        self.W_K = nn.Parameter(\n",
        "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.W_Q = nn.Parameter(\n",
        "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.W_V = nn.Parameter(\n",
        "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.W_O = nn.Parameter(\n",
        "            torch.randn(d_model, d_head * num_heads) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones((n_ctx, n_ctx))))\n",
        "        self.d_head = d_head\n",
        "\n",
        "    def forward(self, x):\n",
        "        k = torch.einsum(\"ihd,bpd->biph\", self.W_K, x)\n",
        "        q = torch.einsum(\"ihd,bpd->biph\", self.W_Q, x)\n",
        "        v = torch.einsum(\"ihd,bpd->biph\", self.W_V, x)\n",
        "        attn_scores_pre = torch.einsum(\"biph,biqh->biqp\", k, q)\n",
        "        attn_scores_masked = torch.tril(attn_scores_pre) - 1e10 * (\n",
        "            1 - self.mask[: x.shape[-2], : x.shape[-2]]\n",
        "        )\n",
        "        attn_matrix = F.softmax(attn_scores_masked / np.sqrt(self.d_head), dim=-1)\n",
        "        z = torch.einsum(\"biph,biqp->biqh\", v, attn_matrix)\n",
        "        z_flat = einops.rearrange(z, \"b i q h -> b q (i h)\")\n",
        "        out = torch.einsum(\"df,bqf->bqd\", self.W_O, z_flat)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, d_in, d_out, act_type, weight_scale=1):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(d_out, d_in))\n",
        "        torch.nn.init.normal_(self.W, mean=0, std=weight_scale / np.sqrt(d_in))\n",
        "\n",
        "    def set_weight_ratio(self, weight_ratio):\n",
        "        self.W = nn.Parameter(self.W * weight_ratio)\n",
        "\n",
        "    def set_weight_ratio_l2(self, weight_ratio):\n",
        "        self.W = nn.Parameter(self.W * torch.sqrt(weight_ratio))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x @ self.W.T\n",
        "\n",
        "\n",
        "# for Transformer\n",
        "class MLPBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    b : batch size\n",
        "    d : embedding size of token\n",
        "    p : vocabraly size (114 or 3)\n",
        "    i : number of heads\n",
        "    h : embedding size of each heads\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_mlp, act_type):\n",
        "        super().__init__()\n",
        "        # bias & layer norm are removed.\n",
        "        self.W_in = nn.Parameter(torch.randn(d_mlp, d_model) / np.sqrt(d_model))\n",
        "        self.b_in = nn.Parameter(torch.zeros(d_mlp))\n",
        "        self.W_out = nn.Parameter(torch.randn(d_model, d_mlp) / np.sqrt(d_model))\n",
        "        self.b_out = nn.Parameter(torch.zeros(d_model))\n",
        "        self.act_type = act_type\n",
        "        # self.ln = LayerNorm(d_mlp, model=self.model)\n",
        "        assert act_type in [\"ReLU\", \"GeLU\"]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.einsum(\"md,bpd->bpm\", self.W_in, x) + self.b_in\n",
        "        if self.act_type == \"ReLU\":\n",
        "            x = F.relu(x)\n",
        "        elif self.act_type == \"GeLU\":\n",
        "            x = F.gelu(x)\n",
        "        x = torch.einsum(\"dm,bpm->bpd\", self.W_out, x) + self.b_out\n",
        "        return x\n",
        "\n",
        "    def set_weight_ratio(self, weight_ratio):\n",
        "        self.W_in = nn.Parameter(self.W_in * weight_ratio)\n",
        "        self.W_out = nn.Parameter(self.W_out * weight_ratio)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    b : batch size\n",
        "    d : embedding size of token\n",
        "    p : vocabraly size\n",
        "    i : number of heads\n",
        "    h : embedding size of each heads\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n",
        "        super().__init__()\n",
        "        # self.ln1 = LayerNorm(d_model, model=self.model)\n",
        "        self.model = model\n",
        "        self.attn = Attention(d_model, num_heads, d_head, n_ctx)\n",
        "        # self.ln2 = LayerNorm(d_model, model=self.model)\n",
        "        self.mlp = MLPBlock(d_model, d_mlp, act_type)\n",
        "        self.layer_norm = LayerNorm(d_model, model=self.model)\n",
        "        self.hook_attn_out = HookPoint()\n",
        "        self.hook_mlp_out = HookPoint()\n",
        "        self.hook_resid_pre = HookPoint()\n",
        "        self.hook_resid_mid = HookPoint()\n",
        "        self.hook_resid_post = HookPoint()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hook_resid_mid(\n",
        "            x + self.hook_attn_out(self.attn((self.hook_resid_pre(x))))\n",
        "        )\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.hook_resid_post(x + self.hook_mlp_out(self.mlp((x))))\n",
        "        return x\n",
        "\n",
        "    def set_weight_ratio(self, weight_ratio):\n",
        "        self.attn.set_weight_ratio(weight_ratio)\n",
        "        self.mlp.set_weight_ratio(weight_ratio)\n",
        "\n",
        "\n",
        "class InputEmbedder(nn.Module):\n",
        "    \"\"\"Input embedder.\"\"\"\n",
        "\n",
        "    def __init__(self, conf):\n",
        "\n",
        "        \"\"\"Initialize the input embedder.\n",
        "\n",
        "    Args:\n",
        "      num_classes: Total number of output classes.\n",
        "      emb_dim: Dimensionality of example and label embeddings.\n",
        "      example_encoding: How to encode example inputs.\n",
        "        'resnet': simple resnet encoding\n",
        "        'linear': flatten and pass through a linear layer\n",
        "        'embedding': pass through an embedding layer\n",
        "      flatten_superpixels: Whether to flatten the output of the resnet (instead\n",
        "        of taking a mean over superpixels).\n",
        "      example_dropout_prob: Dropout probability on example embeddings. Note that\n",
        "        these are applied at both train and test.\n",
        "      concatenate_labels: Whether to concatenate example and label embeddings\n",
        "        into one token for each (example, label) pair, rather than being fed to\n",
        "        the transformer as two separate tokens.\n",
        "      use_positional_encodings: Whether to use positional encoding.\n",
        "      positional_dropout_prob: Positional dropout probability.\n",
        "      name: Optional name for the module.\n",
        "    \"\"\"\n",
        "        super(InputEmbedder, self).__init__()\n",
        "        self.num_labels = conf.d_vocab\n",
        "        self.emb_dim = conf.d_emb\n",
        "        self.p_dim = conf.p_dim\n",
        "        self.emb_dim_content = self.emb_dim - self.p_dim\n",
        "        self.n_ctx = conf.n_ctx\n",
        "\n",
        "        self.Emb = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.label_embs = nn.Parameter(\n",
        "            torch.randn(self.num_labels, self.emb_dim_content) / np.sqrt(self.emb_dim_content)\n",
        "        )\n",
        "\n",
        "    def forward(self, examples, labels, is_training=True):\n",
        "        \"\"\"Call to the input embedder.\n",
        "\n",
        "        Args:\n",
        "          examples: input sequence of shape\n",
        "            [batch_size, seq_len, height, width, channels]\n",
        "          labels: input sequence of shape [batch_size, seq_len]\n",
        "          is_training: if is currently training.\n",
        "\n",
        "        Returns:\n",
        "          outputs: output of the transformer tower\n",
        "            of shape [batch_size, seq_len, channels].\n",
        "        \"\"\"\n",
        "        # Encode the example inputs into shape (B, SS, E)\n",
        "        B, SS, D = examples.shape\n",
        "        # pos encoding\n",
        "        pos_enc = F.one_hot(torch.arange(start=0,end=self.n_ctx+1,step=2), num_classes=self.p_dim).repeat(B,1,1).to(examples.device)\n",
        "        h_example = torch.cat([examples, pos_enc], dim=2)\n",
        "\n",
        "        # Embed the labels.\n",
        "        labels_to_embed = labels\n",
        "        h_label = self.label_embs[labels_to_embed]  # (B, SS, D)\n",
        "        pos_enc = F.one_hot(torch.arange(start=1,end=self.n_ctx+1,step=2), num_classes=self.p_dim).repeat(B,1,1).to(examples.device)\n",
        "        h_label = torch.cat([h_label, pos_enc], dim=2) # (B, SS, E)\n",
        "        \n",
        "        hh = torch.empty(\n",
        "            (h_example.shape[0], h_example.shape[1] * 2 - 1, h_example.shape[2]),\n",
        "            dtype=h_example.dtype,\n",
        "        ).to(h_example.device)\n",
        "        \n",
        "        hh[:, 0::2] = h_example\n",
        "        hh[:, 1::2] = h_label[:, :-1]\n",
        "\n",
        "        return hh\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedder, config):\n",
        "        super().__init__()\n",
        "        num_layers = config.num_layers\n",
        "        d_model = config.d_emb\n",
        "        d_mlp = config.d_emb * 4\n",
        "        d_head = config.d_emb // config.num_heads\n",
        "        num_heads = config.num_heads\n",
        "        n_ctx = config.n_ctx\n",
        "        act_type = config.act_type\n",
        "        use_cache = config.use_cache\n",
        "        use_ln = config.use_ln\n",
        "        self.cache = {}\n",
        "        self.use_cache = use_cache\n",
        "        d_vocab = config.d_vocab\n",
        "\n",
        "        self.embedder = embedder\n",
        "        # self.pos_embed = PosEmbed(n_ctx, d_model)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model=[self]\n",
        "                )\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        # self.ln = LayerNorm(d_model, model=[self])\n",
        "        self.unembed = Unembed(d_vocab, d_model)\n",
        "        self.use_ln = use_ln\n",
        "\n",
        "        for name, module in self.named_modules():\n",
        "            if type(module) == HookPoint:\n",
        "                module.give_name(name)\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        x = self.embedder(x, labels,)\n",
        "        # x = self.pos_embed(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.unembed(x)\n",
        "        return x\n",
        "\n",
        "    def set_use_cache(self, use_cache):\n",
        "        self.use_cache = use_cache\n",
        "\n",
        "    def hook_points(self):\n",
        "        return [module for name, module in self.named_modules() if \"hook\" in name]\n",
        "\n",
        "    def remove_all_hooks(self):\n",
        "        for hp in self.hook_points():\n",
        "            hp.remove_hooks(\"fwd\")\n",
        "            hp.remove_hooks(\"bwd\")\n",
        "\n",
        "    def cache_all(self, cache, incl_bwd=False):\n",
        "        # Caches all activations wrapped in a HookPoint\n",
        "        def save_hook(tensor, name):\n",
        "            cache[name] = tensor.detach()\n",
        "\n",
        "        def save_hook_back(tensor, name):\n",
        "            cache[name + \"_grad\"] = tensor[0].detach()\n",
        "\n",
        "        for hp in self.hook_points():\n",
        "            hp.add_hook(save_hook, \"fwd\")\n",
        "            if incl_bwd:\n",
        "                hp.add_hook(save_hook_back, \"bwd\")\n",
        "\n",
        "class TransformerICL(nn.Module):\n",
        "    def __init__(self, embedder, config):\n",
        "        super().__init__()\n",
        "        num_layers = config.num_layers\n",
        "        d_model = config.d_emb\n",
        "        d_mlp = config.d_emb * 4\n",
        "        d_head = config.d_emb // config.num_heads\n",
        "        num_heads = config.num_heads\n",
        "        n_ctx = config.n_ctx\n",
        "        act_type = config.act_type\n",
        "        use_cache = config.use_cache\n",
        "        use_ln = config.use_ln\n",
        "        self.cache = {}\n",
        "        self.use_cache = use_cache\n",
        "        d_vocab = config.d_vocab\n",
        "\n",
        "        self.embedder = embedder\n",
        "        # self.pos_embed = PosEmbed(n_ctx, d_model)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                Attention(d_model, num_heads, d_head, n_ctx),\n",
        "                Attention(d_model, num_heads, d_head, n_ctx),\n",
        "                Dense(d_model, d_model, act_type),\n",
        "                Dense(d_model, d_model, act_type),\n",
        "                Dense(d_model, d_model, act_type),\n",
        "            ]\n",
        "        )\n",
        "        # self.ln = LayerNorm(d_model, model=[self])\n",
        "        self.unembed = Unembed(d_vocab, d_model)\n",
        "        self.use_ln = use_ln\n",
        "\n",
        "        for name, module in self.named_modules():\n",
        "            if type(module) == HookPoint:\n",
        "                module.give_name(name)\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        x = self.embedder(x, labels)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.unembed(x)\n",
        "        return x\n",
        "\n",
        "    def set_use_cache(self, use_cache):\n",
        "        self.use_cache = use_cache\n",
        "\n",
        "    def hook_points(self):\n",
        "        return [module for name, module in self.named_modules() if \"hook\" in name]\n",
        "\n",
        "    def remove_all_hooks(self):\n",
        "        for hp in self.hook_points():\n",
        "            hp.remove_hooks(\"fwd\")\n",
        "            hp.remove_hooks(\"bwd\")\n",
        "\n",
        "    def cache_all(self, cache, incl_bwd=False):\n",
        "        # Caches all activations wrapped in a HookPoint\n",
        "        def save_hook(tensor, name):\n",
        "            cache[name] = tensor.detach()\n",
        "\n",
        "        def save_hook_back(tensor, name):\n",
        "            cache[name + \"_grad\"] = tensor[0].detach()\n",
        "\n",
        "        for hp in self.hook_points():\n",
        "            hp.add_hook(save_hook, \"fwd\")\n",
        "            if incl_bwd:\n",
        "                hp.add_hook(save_hook_back, \"bwd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6iNOyJRwwV1h"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class SamplingDataset(object):\n",
        "  def __init__(self,conf):\n",
        "    self.num_classes = conf.num_classes\n",
        "    self.dim = conf.dim\n",
        "    self.num_labels = conf.num_labels\n",
        "    self.mu, self.labels = self._get_data()\n",
        "\n",
        "  def _get_data(self):\n",
        "    mu = torch.normal(mean=0, std=math.sqrt(1/self.dim), size=(self.num_classes,self.dim))\n",
        "    labels = torch.randint(self.num_labels, size=(self.num_classes,1))\n",
        "    return mu, labels\n",
        "\n",
        "class SamplingLoader(DataLoader):\n",
        "\n",
        "  def __init__(self,conf, dataset):\n",
        "    self.dataset = dataset\n",
        "    self.mu, self.labels = self.dataset.mu, self.dataset.labels\n",
        "    self.data_type = conf.data_type\n",
        "    self.num_seq = conf.num_seq\n",
        "    self.alpha = conf.alpha\n",
        "    self.num_classes = conf.num_classes\n",
        "    self.num_labels = conf.num_labels\n",
        "    self.ways = conf.ways\n",
        "    self.p_bursty = conf.p_bursty\n",
        "    self.p_icl = conf.p_icl\n",
        "    self.eps = conf.eps\n",
        "    self.dim = conf.dim\n",
        "    if self.ways != 0:\n",
        "      assert self.num_seq % self.ways == 0\n",
        "    if self.ways == 0:\n",
        "      self.p_bursty = 0\n",
        "    prob = np.array([1/((k+1)**self.alpha) for k in range(self.num_classes)])\n",
        "    self.prob = prob/prob.sum()\n",
        "\n",
        "  def get_seq(self):\n",
        "    while True:\n",
        "      if self.data_type==\"bursty\":\n",
        "        if self.p_icl > np.random.rand():\n",
        "            # choise few shot example\n",
        "            num_few_shot_class = self.num_seq//self.ways\n",
        "            mus, labels = self._get_novel_class_seq(num_few_shot_class)\n",
        "            # mus = self.mu[few_shot_class]\n",
        "            mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
        "            # labels = self.labels[few_shot_class]\n",
        "            labels = np.repeat(labels, self.ways, axis=0) # expand ways\n",
        "            classes = np.arange(num_few_shot_class)\n",
        "            classes = np.repeat(classes, self.ways)\n",
        "            # add noise\n",
        "            x = self.add_noise(mus)\n",
        "            # permutation shuffle\n",
        "            ordering = np.random.permutation(self.num_seq)\n",
        "            x = x[ordering]\n",
        "            labels = labels[ordering]\n",
        "            classes = classes[ordering]\n",
        "            # select query labels\n",
        "            query_class_idx = np.random.choice(len(classes), 1)\n",
        "            query_class = classes[query_class_idx]\n",
        "            query_label = labels[query_class_idx]\n",
        "            query_mu = mus[query_class_idx]\n",
        "            query_x = self.add_noise(query_mu)\n",
        "            # concat\n",
        "            x = torch.cat([x, query_x])\n",
        "            labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
        "            \n",
        "            yield {\n",
        "                \"examples\":x.to(torch.float32),\n",
        "                \"labels\":labels,\n",
        "                \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
        "            }\n",
        "            \n",
        "        else:\n",
        "          if self.p_bursty > np.random.rand():\n",
        "            # choise few shot example\n",
        "            num_few_shot_class = self.num_seq//self.ways\n",
        "            few_shot_class = np.random.choice(self.num_classes, num_few_shot_class, replace=False)\n",
        "            mus = self.mu[few_shot_class]\n",
        "            mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
        "            labels = self.labels[few_shot_class]\n",
        "            labels = np.repeat(labels, self.ways, axis=0) # expand ways\n",
        "            classes = np.repeat(few_shot_class, self.ways)\n",
        "            # add noise\n",
        "            x = self.add_noise(mus)\n",
        "            # permutation shuffle\n",
        "            ordering = np.random.permutation(self.num_seq)\n",
        "            x = x[ordering]\n",
        "            labels = labels[ordering]\n",
        "            classes = classes[ordering]\n",
        "            # select query labels\n",
        "            query_class = np.random.choice(few_shot_class, 1)\n",
        "            query_label = self.labels[query_class]\n",
        "            query_mu = self.mu[query_class]\n",
        "            query_x = self.add_noise(query_mu)\n",
        "            # concat\n",
        "            x = torch.cat([x, query_x])\n",
        "            labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
        "            yield {\n",
        "                \"examples\":x.to(torch.float32),\n",
        "                \"labels\":labels,\n",
        "                \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
        "            }\n",
        "          else:\n",
        "            # rank frequency\n",
        "            classes = np.random.choice(self.num_classes, self.num_seq+1, p=self.prob)\n",
        "            mus = self.mu[classes]\n",
        "            labels = self.labels[classes]\n",
        "            x = self.add_noise(mus)\n",
        "            # permutation shuffle\n",
        "            ordering = np.random.permutation(self.num_seq+1)\n",
        "            x = x[ordering]\n",
        "            labels = labels[ordering]\n",
        "            classes = classes[ordering]\n",
        "\n",
        "            yield {\n",
        "                \"examples\":x.to(torch.float32),\n",
        "                \"labels\":labels.flatten(),\n",
        "                \"classes\" : torch.from_numpy(classes)\n",
        "            }\n",
        "\n",
        "      elif self.data_type == \"no_support\":\n",
        "          # rank frequency\n",
        "          classes = np.random.choice(self.num_classes, self.num_seq+1, p=self.prob)\n",
        "          mus = self.mu[classes]\n",
        "          labels = self.labels[classes]\n",
        "          x = self.add_noise(mus)\n",
        "          # permutation shuffle\n",
        "          ordering = np.random.permutation(self.num_seq+1)\n",
        "          x = x[ordering]\n",
        "          labels = labels[ordering]\n",
        "          classes = classes[ordering]\n",
        "\n",
        "          yield {\n",
        "              \"examples\":x.to(torch.float32),\n",
        "              \"labels\":labels.flatten(),\n",
        "              \"classes\" : torch.from_numpy(classes)\n",
        "          }\n",
        "          \n",
        "      elif self.data_type == \"holdout\":\n",
        "        # choise few shot example\n",
        "        num_few_shot_class = self.num_seq//self.ways\n",
        "        mus, labels = self._get_novel_class_seq(num_few_shot_class)\n",
        "        # mus = self.mu[few_shot_class]\n",
        "        mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
        "        # labels = self.labels[few_shot_class]\n",
        "        labels = np.repeat(labels, self.ways, axis=0) # expand ways\n",
        "        classes = np.arange(num_few_shot_class)\n",
        "        classes = np.repeat(classes, self.ways)\n",
        "        # add noise\n",
        "        x = self.add_noise(mus)\n",
        "        # permutation shuffle\n",
        "        ordering = np.random.permutation(self.num_seq)\n",
        "        x = x[ordering]\n",
        "        labels = labels[ordering]\n",
        "        classes = classes[ordering]\n",
        "        # select query labels\n",
        "        query_class_idx = np.random.choice(len(classes), 1)\n",
        "        query_class = classes[query_class_idx]\n",
        "        query_label = labels[query_class_idx]\n",
        "        query_mu = mus[query_class_idx]\n",
        "        query_x = self.add_noise(query_mu)\n",
        "        # concat\n",
        "        x = torch.cat([x, query_x])\n",
        "        labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
        "        \n",
        "        yield {\n",
        "            \"examples\":x.to(torch.float32),\n",
        "            \"labels\":labels,\n",
        "            \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
        "        }\n",
        "\n",
        "      elif self.data_type == \"flip\":\n",
        "        # choise few shot example\n",
        "        num_few_shot_class = self.num_seq//self.ways\n",
        "        few_shot_class = np.random.choice(self.num_classes, num_few_shot_class, replace=False)\n",
        "        mus = self.mu[few_shot_class]\n",
        "        mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
        "        classes = np.repeat(few_shot_class, self.ways)\n",
        "        # label flip\n",
        "        labels = (self.labels[classes] + 1) % self.num_labels\n",
        "        # add noise\n",
        "        x = self.add_noise(mus)\n",
        "        # permutation shuffle\n",
        "        ordering = np.random.permutation(self.num_seq)\n",
        "        x = x[ordering]\n",
        "        labels = labels[ordering]\n",
        "        classes = classes[ordering]\n",
        "        # select query labels\n",
        "        query_class = np.random.choice(few_shot_class, 1)\n",
        "        query_label = (self.labels[query_class] + 1) % self.num_labels\n",
        "        query_mu = self.mu[query_class]\n",
        "        query_x = self.add_noise(query_mu)\n",
        "        # concat\n",
        "        x = torch.cat([x, query_x])\n",
        "        labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
        "        \n",
        "        yield {\n",
        "            \"examples\":x.to(torch.float32),\n",
        "            \"labels\":labels,\n",
        "            \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
        "        }\n",
        "    \n",
        "  \n",
        "\n",
        "  def add_noise(self, x):\n",
        "    x = (x+self.eps*torch.normal(mean=0, std=math.sqrt(1/self.dim), size=(x.shape)))/(np.sqrt(1+self.eps**2))\n",
        "    # x = (x+self.eps*np.random.normal(mean=0, std=np.sqrt(1/self.dim), size=(x.shape[0],1)))/(np.sqrt(1+self.eps**2))\n",
        "    return x\n",
        "  \n",
        "  def _get_novel_class_seq(self,num_class):\n",
        "    mu = torch.normal(mean=0, std=math.sqrt(1/self.dim), size=(num_class,self.dim))\n",
        "    labels = torch.randint(self.num_labels, size=(num_class,1))\n",
        "    return mu, labels\n",
        "\n",
        "class IterDataset(IterableDataset):\n",
        "    def __init__(self, generator):\n",
        "        self.generator = generator\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.generator()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gX3jnutBzfVe"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, asdict\n",
        "\n",
        "@dataclass\n",
        "class TransformerConfig:\n",
        "  num_layers: int = 2\n",
        "  d_vocab: int = 32 # same as num_labels\n",
        "  d_model: int = 128 \n",
        "  d_mlp: int = 128\n",
        "  d_head: int = 128\n",
        "  num_heads: int = 1\n",
        "  n_ctx: int = int(8*2+1)\n",
        "  act_type: str = \"ReLU\"\n",
        "  use_cache: bool = False\n",
        "  use_ln: bool = True\n",
        "  p_dim: int = 65\n",
        "  d_emb: int = 128\n",
        "\n",
        "@dataclass\n",
        "class TrainDataConfig:\n",
        "  num_classes: int = 512\n",
        "  dim: int = 63\n",
        "  num_labels: int = 32\n",
        "  eps: float = 0.1\n",
        "  alpha: float = 0\n",
        "  ways: int = 2\n",
        "  num_seq: int = 8\n",
        "  p_bursty: float = 0.75\n",
        "  p_icl: float = 0\n",
        "  data_type: str = \"bursty\" # bursty, holdout, no_support, flip\n",
        "\n",
        "@dataclass\n",
        "class IWLDataConfig(TrainDataConfig):\n",
        "  data_type: str = \"no_support\" # bursty, holdout, no_support, flip\n",
        "\n",
        "@dataclass\n",
        "class ICLDataConfig(TrainDataConfig):\n",
        "  data_type: str = \"holdout\" # bursty, holdout, no_support, flip\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ICL2DataConfig(TrainDataConfig):\n",
        "  data_type: str = \"flip\" # bursty, holdout, no_support, flip\n",
        "  \n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "  batch_size: int = 1\n",
        "  optimize_step: int = int(2e5)\n",
        "  lr: float = 0.01\n",
        "  optimizer: str = \"sgd\" # adam, sgd, adamw\n",
        "\n",
        "@dataclass\n",
        "class MainConfig:\n",
        "  traindataconfig : TrainDataConfig = TrainDataConfig()\n",
        "  icldataconfig: ICLDataConfig = ICLDataConfig()\n",
        "  iwldataconfig: IWLDataConfig = IWLDataConfig()\n",
        "  icl2dataconfig: ICL2DataConfig = ICL2DataConfig()\n",
        "  modelconfig: TransformerConfig = TransformerConfig()\n",
        "  trainconfig: TrainConfig = TrainConfig()\n",
        "  device: str = \"cuda:1\"\n",
        "# define config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "U4wKEdZkNkC6"
      },
      "outputs": [],
      "source": [
        "def cal_acc(t,p):\n",
        "    p_arg = torch.argmax(p,dim=1)\n",
        "    return torch.sum(t == p_arg) / p.shape[0]\n",
        "def to_gpu_dict(dic):\n",
        "    dic = {k:v.to(\"cuda:1\") for k,v in dic.items()}\n",
        "    return dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_class tensor([[245,  84, 251,  84, 245, 251,  80,  80,  84]])\n",
            "train_label tensor([[ 7,  2, 26,  2,  7, 26, 21, 21,  2]])\n",
            "train_class tensor([[ 50, 417, 493,  50,  12, 417,  12, 493, 493]])\n",
            "train_label tensor([[14, 14,  3, 14, 28, 14, 28,  3,  3]])\n",
            "train_class tensor([[413, 284, 284, 166, 166, 413,   6,   6, 284]])\n",
            "train_label tensor([[ 2, 31, 31, 21, 21,  2, 30, 30, 31]])\n",
            "iwl_class tensor([[237, 243,  91, 172, 133, 218, 206, 128, 315]])\n",
            "iwl_label tensor([[18,  0, 12, 10, 25, 19, 26,  7, 27]])\n",
            "iwl_class tensor([[ 69, 334, 407, 370,  97, 459,  51, 106,  10]])\n",
            "iwl_label tensor([[ 5,  9, 14,  5, 18, 19,  0, 12, 27]])\n",
            "iwl_class tensor([[337,  72,  40, 444,  39, 338,  46, 425, 183]])\n",
            "iwl_label tensor([[28, 10,  1,  9, 28, 19, 23, 24, 28]])\n",
            "icl2_class tensor([[333, 270, 333,  23, 471,  23, 471, 270,  23]])\n",
            "icl2_label tensor([[19,  4, 19,  3, 16,  3, 16,  4,  3]])\n",
            "icl2_class tensor([[144, 497, 497,  70, 127,  70, 144, 127,  70]])\n",
            "icl2_label tensor([[ 1, 27, 27,  1, 17,  1,  1, 17,  1]])\n",
            "icl2_class tensor([[287, 110,  20,  20, 110, 472, 472, 287,  20]])\n",
            "icl2_label tensor([[22, 11, 25, 25, 11, 17, 17, 22, 25]])\n",
            "icl_class tensor([[3, 1, 1, 2, 0, 3, 2, 0, 0]])\n",
            "icl_label tensor([[21, 26, 26, 22, 28, 21, 22, 28, 28]])\n",
            "icl_class tensor([[3, 1, 1, 0, 2, 3, 2, 0, 2]])\n",
            "icl_label tensor([[ 8, 28, 28,  5, 22,  8, 22,  5, 22]])\n",
            "icl_class tensor([[1, 3, 0, 3, 2, 0, 1, 2, 1]])\n",
            "icl_label tensor([[16, 21, 25, 21, 17, 25, 16, 17, 16]])\n",
            "icl_class tensor([[0, 2, 2, 1, 3, 3, 0, 1, 0]])\n",
            "icl_label tensor([[11, 22, 22, 20, 10, 10, 11, 20, 11]])\n",
            "icl_class tensor([[1, 0, 2, 3, 1, 0, 2, 3, 3]])\n",
            "icl_label tensor([[31, 22, 22, 19, 31, 22, 22, 19, 19]])\n",
            "icl_class tensor([[0, 2, 1, 1, 3, 0, 2, 3, 2]])\n",
            "icl_label tensor([[26, 24, 11, 11, 18, 26, 24, 18, 24]])\n"
          ]
        }
      ],
      "source": [
        "traindataconfig = MainConfig.traindataconfig\n",
        "icldataconfig = MainConfig.icldataconfig\n",
        "iwldataconfig = MainConfig.iwldataconfig\n",
        "icl2dataconfig = MainConfig.icl2dataconfig\n",
        "trainconfig = MainConfig.trainconfig\n",
        "\n",
        "Dataset = SamplingDataset(traindataconfig)\n",
        "\n",
        "trainloader = SamplingLoader(traindataconfig, dataset=Dataset)\n",
        "train_seq_generator = trainloader.get_seq\n",
        "train_dataset = IterDataset(train_seq_generator)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "iclloader = SamplingLoader(icldataconfig, dataset=Dataset)\n",
        "icl_seq_generator = iclloader.get_seq\n",
        "icl_dataset = IterDataset(icl_seq_generator)\n",
        "icl_dataloader = torch.utils.data.DataLoader(icl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "iwlloader = SamplingLoader(iwldataconfig, dataset=Dataset)\n",
        "iwl_seq_generator = iwlloader.get_seq\n",
        "iwl_dataset = IterDataset(iwl_seq_generator)\n",
        "iwl_dataloader = torch.utils.data.DataLoader(iwl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "icl2loader = SamplingLoader(icl2dataconfig, dataset=Dataset)\n",
        "icl2_seq_generator = icl2loader.get_seq\n",
        "icl2_dataset = IterDataset(icl2_seq_generator)\n",
        "icl2_dataloader = torch.utils.data.DataLoader(icl2_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "cnt = 0\n",
        "for data in train_dataloader:\n",
        "    examples = data[\"examples\"]\n",
        "    labels = data[\"labels\"]\n",
        "    classes = data[\"classes\"]\n",
        "    # print(examples)\n",
        "    print(\"train_class\", classes)\n",
        "    print(\"train_label\", labels)\n",
        "    cnt += 1\n",
        "    if cnt > 2:\n",
        "        break\n",
        "cnt = 0\n",
        "for data in iwl_dataloader:\n",
        "    examples = data[\"examples\"]\n",
        "    labels = data[\"labels\"]\n",
        "    classes = data[\"classes\"]\n",
        "    # print(examples)\n",
        "    print(\"iwl_class\", classes)\n",
        "    print(\"iwl_label\", labels)\n",
        "    cnt += 1\n",
        "    if cnt > 2:\n",
        "        break\n",
        "cnt = 0\n",
        "for data in icl2_dataloader:\n",
        "    examples = data[\"examples\"]\n",
        "    labels = data[\"labels\"]\n",
        "    classes = data[\"classes\"]\n",
        "    # print(examples)\n",
        "    print(\"icl2_class\", classes)\n",
        "    print(\"icl2_label\", labels)\n",
        "    cnt += 1\n",
        "    if cnt > 2:\n",
        "        break\n",
        "cnt = 0\n",
        "for data in icl_dataloader:\n",
        "    examples = data[\"examples\"]\n",
        "    labels = data[\"labels\"]\n",
        "    classes = data[\"classes\"]\n",
        "    # print(examples\n",
        "    print(\"icl_class\", classes)\n",
        "    print(\"icl_label\", labels)\n",
        "    cnt += 1\n",
        "    if cnt > 5:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "12621ac3883347eba74b077797cda2b9",
            "5b7930a79e344c358f1ca175e9be4fd1",
            "d4e89f0d98b44eb4b76e59caa06aea14",
            "494267ae30c44a3bb446eaee6dc43115",
            "1a6e230516ff4996bb34f8a4992e1a20",
            "065c4cf26dd24c0d8caaa7165e800ca7",
            "aea29e116b304946b7d98692deb2addb",
            "9b3c46b8ff12462b9c4d2f0fbf12c767"
          ]
        },
        "id": "jwic1RdJGRDT",
        "outputId": "4663b1f4-a8bd-4983-f03d-8d0b9ac74208"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgouki\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/workspace/induction-head/wandb/run-20240311_015028-zdb5qxto</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gouki/icl-minima/runs/zdb5qxto' target=\"_blank\">drawn-pond-39</a></strong> to <a href='https://wandb.ai/gouki/icl-minima' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gouki/icl-minima' target=\"_blank\">https://wandb.ai/gouki/icl-minima</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gouki/icl-minima/runs/zdb5qxto' target=\"_blank\">https://wandb.ai/gouki/icl-minima/runs/zdb5qxto</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 197668 1.0 1.0 0.0234375 0.025 0.0 0.00007812539 0.2421875 0.015625 0.1640625 0.2890625 0.0703125 0.21875 0.1796875 0.0234375 0.2734375 0.2421875 0.1953125 0.015625 0.2578125 0.3046875 0.2421875 0.0390625 0.3515625 0.28906250.1953125 0.03125 0.265625 0.25 0.265625 0.2421875 0.1796875 0.0234375 0.28125 0.24218750.0234375 0.265625 0.31250.03125 0.296875 0.2890625 0.3046875 0.28125 0.242187526613 0.2578125 0.0625 0.296875 0.250.3046875 0.1796875 0.0390625 0.25 0.2890625 0.2265625 0.03125 0.3125 0.265625 0.2578125 0.078125 0.328125 0.2734375 0.2890625 0.343750.3125 0.03125 0.3671875 0.26562539063 0.265625 0.0546875 0.265625 0.2890625 0.2968750.0625 0.2421875 0.23437541675 0.375 0.0390625 0.3125 0.3125 0.6015625 0.2109375 0.1796875 0.1718750.1796875 0.1875 0.226562544411 0.6640625 0.2421875 0.1875 0.1718750.171875 0.375 0.1484375 0.078125 0.3359375 0.140625 0.09375 0.5859375 0.0546875 0.101562550913 0.8671875 0.734375 0.0390625 0.031250.859375 0.03125 0.015625 0.96875 0.0390625 0.0156250.953125 0.046875 0.0156251.0 0.984375 0.0234375 0.0 0.9921875 0.96875 0.0390625 0.01562558261 0.984375 0.9765625 0.015625 0.0234375 0.0078125 0.9921875 0.9921875 0.0234375 0.00781250.0546875 0.060943 1.0 0.984375 0.0390625 0.00.984375 0.015625 0.00.0234375 0.0078125 0.96875 0.9296875 0.0390625 0.0078125 1.0 1.0 0.03125 0.00781251.0 0.0234375 0.0078125 1.0 0.984375 0.03125 0.00781250.0078125 1.0 1.0 0.0546875 0.0 1.0 1.0 0.0234375 0.0 1.0 1.0 0.0546875 0.0 1.0 1.0 0.0390625 0.076905 1.0 1.0 0.0390625 0.077252 1.0 1.0 0.046875 0.01.0 0.0390625 0.00.01.0 1.0 0.0234375 0.007812586043 1.0 1.0 0.046875 0.00.015625 0.0 0.046875 0.0 0.03125 0.0 1.0 0.03125 0.00.0078125 0.096146 1.0 1.0 0.0390625 0.00.0 0.099350 1.0 1.0 0.0390625 0.0 0.0 1.0 0.0234375 0.0101431 1.0 1.0 0.03125 0.00.03125 0.0 0.0234375 0.0 0.01.0 0.03125 0.0 0.015625 0.0 1.0 1.0 0.015625 0.01.0 1.0 0.0234375 0.0 1.0 0.046875 0.0111013 0.9921875 1.0 0.0546875 0.0 1.0 1.0 0.046875 0.0 1.0 0.0234375 0.0 1.0 1.0 0.015625 0.01.0 0.0390625 0.0127537 1.0 1.0 0.03125 0.0 0.03125 0.0 0.0 1.0 0.0390625 0.0129157 1.0 1.0 0.0390625 0.00.03125 0.00.0132877 1.0 1.0 0.0390625 0.0 1.0 1.0 0.03125 0.0 0.0234375 0.0 0.0133845 1.0 1.0 0.0234375 0.0134304 1.0 1.0 0.0234375 0.00.00.0390625 0.0 1.0 0.0625 0.01.0 0.046875 0.0 0.00.0390625 0.01.0 1.0 0.0546875 0.0 1.0 0.03125 0.00.0390625 0.0 1.0 0.015625 0.00.0625 0.0 0.0390625 0.0142379 1.0 1.0 0.0234375 0.0 0.0625 0.00.0 1.0 0.0234375 0.0144962 1.0 1.0 0.03125 0.0 1.0 0.015625 0.0 1.0 1.0 0.03125 0.01.0 0.03125 0.0 0.0 1.0 1.0 0.0390625 0.00.0 0.0 1.0 0.03125 0.0 1.0 1.0 0.0546875 0.0148054 1.0 1.0 0.0078125 0.00.0 1.0 0.0078125 0.0 1.0 0.03125 0.0 1.0 0.0390625 0.00.03125 0.01.0 1.0 0.046875 0.0 0.0152125 1.0 1.0 0.0078125 0.0 1.0 1.0 0.0390625 0.00.0390625 0.0153930 1.0 1.0 0.0234375 0.0 1.0 0.0390625 0.01.0 1.0 0.03125 0.00.00.015625 0.0 1.0 1.0 0.0703125 0.0 0.0158381 0.9921875 1.0 0.03125 0.00.0546875 0.01.0 1.0 0.0234375 0.00.00781250.9921875 0.0625 0.00.0390625 0.0160788 1.0 1.0 0.0390625 0.01.0 1.0 0.015625 0.00.0546875 0.01.0 0.015625 0.00.046875 0.0 0.0 0.03125 0.0165271 1.0 1.0 0.0234375 0.01.0 1.0 0.0078125 0.0 0.0625 0.0 1.0 1.0 0.0234375 0.0 1.0 1.0 0.078125 0.0 1.0 1.0 0.0390625 0.00.00.0 1.0 0.0546875 0.01.0 0.0546875 0.0171365 1.0 1.0 0.0078125 0.01.0 0.015625 0.0173075 1.0 1.0 0.046875 0.01.0 0.03125 0.01.0 1.0 0.03125 0.0173284 1.0 1.0 0.0390625 0.0 0.0390625 0.01.0 0.046875 0.0 0.046875 0.01.0 0.0234375 0.00.01.0 0.0546875 0.00.0 0.0390625 0.0 0.0234375 0.0 0.0390625 0.0 1.0 1.0 0.0546875 0.0 1.0 1.0 0.0390625 0.0180134 1.0 1.0 0.046875 0.01.0 0.0546875 0.0 0.0181881 1.0 1.0 0.015625 0.01.0 0.0234375 0.01.0 0.0390625 0.01.0 0.0234375 0.0 0.0234375 0.00.0 0.00.015625 0.01.0 0.0703125 0.0 1.0 1.0 0.078125 0.0 0.0234375 0.01.0 0.0625 0.01.0 1.0 0.03125 0.0 0.01.0 0.015625 0.01.0 1.0 0.046875 0.0 1.0 0.0234375 0.0 0.0390625 0.0189601 1.0 1.0 0.0546875 0.00.015625 0.01.0 0.0546875 0.0191023 1.0 1.0 0.046875 0.00.046875 0.01.0 1.0 0.0078125 0.0 1.0 1.0 0.0234375 0.0192026 1.0 1.0 0.078125 0.0 1.0 1.0 0.046875 0.0192840 1.0 1.0 0.0234375 0.0 1.0 1.0 0.03125 0.0 1.0 1.0 0.0703125 0.0"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     44\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (data_dict, icl_data_dict, iwl_data_dict, icl2_data_dict) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_dataloader, icl_dataloader, iwl_dataloader, icl2_dataloader):\n\u001b[1;32m     46\u001b[0m   model\u001b[38;5;241m.\u001b[39mtrain()   \n\u001b[1;32m     47\u001b[0m   data_dict \u001b[38;5;241m=\u001b[39m to_gpu_dict(data_dict)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_name):\n\u001b[1;32m    627\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/profiler.py:648\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 648\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_ops.py:448\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# train\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "config = MainConfig()\n",
        "wandb.init(project=\"icl-minima\", config=asdict(config))\n",
        "# data\n",
        "Dataset = SamplingDataset(traindataconfig)\n",
        "\n",
        "trainloader = SamplingLoader(traindataconfig, dataset=Dataset)\n",
        "train_seq_generator = trainloader.get_seq\n",
        "train_dataset = IterDataset(train_seq_generator)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "iclloader = SamplingLoader(icldataconfig, dataset=Dataset)\n",
        "icl_seq_generator = iclloader.get_seq\n",
        "icl_dataset = IterDataset(icl_seq_generator)\n",
        "icl_dataloader = torch.utils.data.DataLoader(icl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "iwlloader = SamplingLoader(iwldataconfig, dataset=Dataset)\n",
        "iwl_seq_generator = iwlloader.get_seq\n",
        "iwl_dataset = IterDataset(iwl_seq_generator)\n",
        "iwl_dataloader = torch.utils.data.DataLoader(iwl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "icl2loader = SamplingLoader(icl2dataconfig, dataset=Dataset)\n",
        "icl2_seq_generator = icl2loader.get_seq\n",
        "icl2_dataset = IterDataset(icl2_seq_generator)\n",
        "icl2_dataloader = torch.utils.data.DataLoader(icl2_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "# model\n",
        "embedder = InputEmbedder(config.modelconfig)\n",
        "model = TransformerICL(embedder, config.modelconfig)\n",
        "model.to(config.device)\n",
        "\n",
        "# optimizer\n",
        "if config.trainconfig.optimizer == \"adam\":\n",
        "  optimizer =  torch.optim.Adam(model.parameters(), lr=config.trainconfig.lr)\n",
        "elif config.trainconfig.optimizer == \"sgd\":\n",
        "  optimizer =  torch.optim.SGD(model.parameters(), lr=config.trainconfig.lr)\n",
        "elif config.trainconfig.optimizer == \"adamw\":\n",
        "  optimizer =  torch.optim.AdamW(model.parameters(), lr=config.trainconfig.lr)\n",
        "\n",
        "# loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "step = 0\n",
        "for (data_dict, icl_data_dict, iwl_data_dict, icl2_data_dict) in zip(train_dataloader, icl_dataloader, iwl_dataloader, icl2_dataloader):\n",
        "  model.train()   \n",
        "  data_dict = to_gpu_dict(data_dict)\n",
        "  icl_data_dict = to_gpu_dict(icl_data_dict)\n",
        "  iwl_data_dict = to_gpu_dict(iwl_data_dict)\n",
        "  icl2_data_dict = to_gpu_dict(icl2_data_dict)\n",
        "  \n",
        "  logits = model(data_dict[\"examples\"], data_dict[\"labels\"])\n",
        "  query_logit = logits[:,-1,:]\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  # print(data_dict[\"labels\"][:,-1])\n",
        "  loss = criterion(query_logit, data_dict[\"labels\"][:,-1],)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  train_acc = cal_acc(data_dict[\"labels\"][:, -1], query_logit)\n",
        "  # print(\"train_sample\", data_dict[\"classes\"], data_dict[\"labels\"])\n",
        "  wandb.log({\"train/acc\":train_acc,\"train/loss\":loss}, step=step)\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    logits = model(icl_data_dict[\"examples\"], icl_data_dict[\"labels\"])\n",
        "    query_logit = logits[:,-1,:]\n",
        "    icl_acc = cal_acc(icl_data_dict[\"labels\"][:, -1], query_logit)\n",
        "    wandb.log({\"valid/icl_acc\":icl_acc}, step=step)\n",
        "    # print(\"icl_sample\", icl_data_dict[\"classes\"], icl_data_dict[\"labels\"])\n",
        "\n",
        "    logits = model(iwl_data_dict[\"examples\"], iwl_data_dict[\"labels\"])\n",
        "    query_logit = logits[:,-1,:]\n",
        "    iwl_acc = cal_acc(iwl_data_dict[\"labels\"][:, -1], query_logit)\n",
        "    wandb.log({\"valid/iwl_acc\":iwl_acc}, step=step)\n",
        "    # print(\"iwl_sample\", iwl_data_dict[\"classes\"], iwl_data_dict[\"labels\"])\n",
        "\n",
        "    logits = model(icl2_data_dict[\"examples\"], icl2_data_dict[\"labels\"])\n",
        "    query_logit = logits[:,-1,:]\n",
        "    icl2_acc = cal_acc(icl2_data_dict[\"labels\"][:, -1], query_logit)\n",
        "    wandb.log({\"valid/icl2_acc\":icl2_acc}, step=step)\n",
        "    # print(\"icl2_sample\", icl2_data_dict[\"classes\"], icl2_data_dict[\"labels\"])\n",
        "          \n",
        "  print(\"\\r\",step, train_acc.item(), iwl_acc.item(), icl_acc.item(), icl2_acc.item(), end=\"\")\n",
        "  step+=1\n",
        "  if step > config.trainconfig.optimize_step:\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4i_bXu6L5yF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "065c4cf26dd24c0d8caaa7165e800ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12621ac3883347eba74b077797cda2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b7930a79e344c358f1ca175e9be4fd1",
              "IPY_MODEL_d4e89f0d98b44eb4b76e59caa06aea14"
            ],
            "layout": "IPY_MODEL_494267ae30c44a3bb446eaee6dc43115"
          }
        },
        "1a6e230516ff4996bb34f8a4992e1a20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494267ae30c44a3bb446eaee6dc43115": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b7930a79e344c358f1ca175e9be4fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a6e230516ff4996bb34f8a4992e1a20",
            "placeholder": "",
            "style": "IPY_MODEL_065c4cf26dd24c0d8caaa7165e800ca7",
            "value": "0.010 MB of 0.010 MB uploaded\r"
          }
        },
        "9b3c46b8ff12462b9c4d2f0fbf12c767": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aea29e116b304946b7d98692deb2addb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4e89f0d98b44eb4b76e59caa06aea14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aea29e116b304946b7d98692deb2addb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b3c46b8ff12462b9c4d2f0fbf12c767",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
