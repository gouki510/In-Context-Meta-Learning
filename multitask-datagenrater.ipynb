{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torch\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class SamplingDataset(object):\n",
    "  def __init__(self,conf):\n",
    "    self.num_classes = conf.num_classes\n",
    "    self.dim = conf.dim\n",
    "    self.num_labels = conf.num_labels\n",
    "    self.mu, self.labels = self._get_data()\n",
    "\n",
    "  def _get_data(self):\n",
    "    mu = torch.normal(mean=0, std=math.sqrt(1/self.dim), size=(self.num_classes,self.dim))\n",
    "    labels = torch.randint(self.num_labels, size=(self.num_classes,1))\n",
    "    return mu, labels\n",
    "\n",
    "class SamplingLoader(DataLoader):\n",
    "\n",
    "  def __init__(self,conf, dataset):\n",
    "    self.dataset = dataset\n",
    "    self.mu, self.labels = self.dataset.mu, self.dataset.labels\n",
    "    self.data_type = conf.data_type\n",
    "    self.num_seq = conf.num_seq\n",
    "    self.alpha = conf.alpha\n",
    "    self.num_classes = conf.num_classes\n",
    "    self.num_labels = conf.num_labels\n",
    "    self.ways = conf.ways\n",
    "    self.p_bursty = conf.p_bursty\n",
    "    self.eps = conf.eps\n",
    "    self.dim = conf.dim\n",
    "    self.num_holdout_classes = conf.num_holdout_classes\n",
    "    self.holdout_classes = np.arange(self.num_classes-self.num_holdout_classes, self.num_classes)\n",
    "    self.burstiness_class = conf.burstiness_classes\n",
    "    if self.ways != 0:\n",
    "      assert self.num_seq % self.ways ==0\n",
    "    if self.ways == 0:\n",
    "      self.p_bursty = 0\n",
    "    prob = np.array([1/((k+1)**self.alpha) for k in range(self.num_classes-self.num_holdout_classes)])\n",
    "    self.prob = prob/prob.sum()\n",
    "\n",
    "  def get_seq(self):\n",
    "    while True:\n",
    "      if self.data_type==\"bursty\":\n",
    "        if self.p_bursty > np.random.rand():\n",
    "          # choise few shot example\n",
    "          num_few_shot_class = self.num_seq//self.ways\n",
    "          few_shot_class = np.random.choice(self.burstiness_class, num_few_shot_class, replace=False)\n",
    "          mus = self.mu[few_shot_class]\n",
    "          mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
    "          labels = self.labels[few_shot_class]\n",
    "          labels = np.repeat(labels, self.ways, axis=0) # expand ways\n",
    "          classes = np.repeat(few_shot_class, self.ways)\n",
    "          # add noise\n",
    "          x = self.add_noise(mus)\n",
    "          # permutation shuffle\n",
    "          ordering = np.random.permutation(self.num_seq)\n",
    "          x = x[ordering]\n",
    "          labels = labels[ordering]\n",
    "          classes = classes[ordering]\n",
    "          # select query labels\n",
    "          query_class = np.random.choice(few_shot_class, 1)\n",
    "          query_label = self.labels[query_class]\n",
    "          query_mu = self.mu[query_class]\n",
    "          query_x = self.add_noise(query_mu)\n",
    "          # concat\n",
    "          x = torch.cat([x, query_x])\n",
    "          labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
    "          yield {\n",
    "              \"examples\":x.to(torch.float32),\n",
    "              \"labels\":labels,\n",
    "              \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
    "          }\n",
    "        else:\n",
    "          # rank frequency\n",
    "          classes = np.random.choice(self.num_classes-self.num_holdout_classes, self.num_seq+1, p=self.prob)\n",
    "          mus = self.mu[classes]\n",
    "          labels = self.labels[classes]\n",
    "          x = self.add_noise(mus)\n",
    "          # permutation shuffle\n",
    "          ordering = np.random.permutation(self.num_seq+1)\n",
    "          x = x[ordering]\n",
    "          labels = labels[ordering]\n",
    "          classes = classes[ordering]\n",
    "\n",
    "          yield {\n",
    "              \"examples\":x.to(torch.float32),\n",
    "              \"labels\":labels.flatten(),\n",
    "              \"classes\" : torch.from_numpy(classes)\n",
    "          }\n",
    "\n",
    "      elif self.data_type == \"no_support\":\n",
    "          # rank frequency\n",
    "          classes = np.random.choice(self.num_classes-self.num_holdout_classes, self.num_seq+1, p=self.prob)\n",
    "          mus = self.mu[classes]\n",
    "          labels = self.labels[classes]\n",
    "          x = self.add_noise(mus)\n",
    "          # permutation shuffle\n",
    "          ordering = np.random.permutation(self.num_seq+1)\n",
    "          x = x[ordering]\n",
    "          labels = labels[ordering]\n",
    "          classes = classes[ordering]\n",
    "\n",
    "          yield {\n",
    "              \"examples\":x.to(torch.float32),\n",
    "              \"labels\":labels.flatten(),\n",
    "              \"classes\" : torch.from_numpy(classes)\n",
    "          }\n",
    "          \n",
    "      elif self.data_type == \"holdout\":\n",
    "          if 1 > np.random.rand():\n",
    "            # choise few shot example\n",
    "            num_few_shot_class = self.num_seq//self.ways\n",
    "            few_shot_class = np.random.choice(self.holdout_classes, num_few_shot_class, replace=False)\n",
    "            mus = self.mu[few_shot_class]\n",
    "            mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
    "            labels = self.labels[few_shot_class]\n",
    "            labels = np.repeat(labels, self.ways, axis=0) # expand ways\n",
    "            classes = np.repeat(few_shot_class, self.ways)\n",
    "            # add noise\n",
    "            x = self.add_noise(mus)\n",
    "            # permutation shuffle\n",
    "            ordering = np.random.permutation(self.num_seq)\n",
    "            x = x[ordering]\n",
    "            labels = labels[ordering]\n",
    "            classes = classes[ordering]\n",
    "            # select query labels\n",
    "            query_class = np.random.choice(few_shot_class, 1)\n",
    "            query_label = self.labels[query_class]\n",
    "            query_mu = self.mu[query_class]\n",
    "            query_x = self.add_noise(query_mu)\n",
    "            # concat\n",
    "            x = torch.cat([x, query_x])\n",
    "            labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
    "            \n",
    "            yield {\n",
    "                \"examples\":x.to(torch.float32),\n",
    "                \"labels\":labels,\n",
    "                \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
    "            }\n",
    "            \n",
    "          else:\n",
    "          # rank frequency\n",
    "            classes = np.random.choice(self.holdout_classes, self.num_seq)\n",
    "            mus = self.mu[classes]\n",
    "            labels = self.labels[classes]\n",
    "            x = self.add_noise(mus)\n",
    "            # permutation shuffle\n",
    "            ordering = np.random.permutation(self.num_seq)\n",
    "            x = x[ordering]\n",
    "            labels = labels[ordering]\n",
    "            classes = classes[ordering]\n",
    "            # query\n",
    "            query_class = np.random.choice(classes, 1)\n",
    "            query_label = self.labels[query_class]\n",
    "            query_mu = self.mu[query_class]\n",
    "            query_x = self.add_noise(query_mu)\n",
    "            # concat\n",
    "            x = torch.cat([x, query_x])\n",
    "            labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
    "\n",
    "            yield {\n",
    "                \"examples\":x.to(torch.float32),\n",
    "                \"labels\":labels,\n",
    "                \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class)])\n",
    "            }\n",
    "\n",
    "      elif self.data_type == \"flip\":\n",
    "          if self.p_bursty > np.random.rand():\n",
    "            # choise few shot example\n",
    "            num_few_shot_class = self.num_seq//self.ways\n",
    "            few_shot_class = np.random.choice(self.burstiness_class, num_few_shot_class, replace=False)\n",
    "            mus = self.mu[few_shot_class]\n",
    "            mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
    "            classes = np.repeat(few_shot_class, self.ways)\n",
    "            # label flip\n",
    "            labels = (self.labels[classes] + 1) % self.num_labels\n",
    "            # add noise\n",
    "            x = self.add_noise(mus)\n",
    "            # permutation shuffle\n",
    "            ordering = np.random.permutation(self.num_seq)\n",
    "            x = x[ordering]\n",
    "            labels = labels[ordering]\n",
    "            classes = classes[ordering]\n",
    "            # select query labels\n",
    "            query_class = np.random.choice(few_shot_class, 1)\n",
    "            query_label = (self.labels[query_class] + 1) % self.num_labels\n",
    "            query_mu = self.mu[query_class]\n",
    "            query_x = self.add_noise(query_mu)\n",
    "            # concat\n",
    "            x = torch.cat([x, query_x])\n",
    "            labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
    "            \n",
    "            yield {\n",
    "                \"examples\":x.to(torch.float32),\n",
    "                \"labels\":labels,\n",
    "                \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
    "            }\n",
    "            \n",
    "          else:\n",
    "            # rank frequency\n",
    "            classes = np.random.choice(self.num_classes-self.num_holdout_classes, self.num_seq)\n",
    "            mus = self.mu[classes]\n",
    "            # label flip\n",
    "            labels = (self.labels[classes] + 1) % self.num_labels\n",
    "            x = self.add_noise(mus)\n",
    "            # permutation shuffle\n",
    "            ordering = np.random.permutation(self.num_seq)\n",
    "            x = x[ordering]\n",
    "            labels = labels[ordering]\n",
    "            classes = classes[ordering]\n",
    "            # query\n",
    "            query_class = np.random.choice(classes, 1)\n",
    "            query_label = (self.labels[query_class]+1) % self.num_labels\n",
    "            query_mu = self.mu[query_class]\n",
    "            query_x = self.add_noise(query_mu)\n",
    "            # concat\n",
    "            x = torch.cat([x, query_x])\n",
    "            labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
    "            yield {\n",
    "                \"examples\":x.to(torch.float32),\n",
    "                \"labels\":labels.flatten(),\n",
    "                \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class)])\n",
    "            }\n",
    "        \n",
    "\n",
    "  def add_noise(self, x):\n",
    "    x = (x+self.eps*torch.normal(mean=0, std=math.sqrt(1/self.dim), size=(x.shape[0],1)))/(np.sqrt(1+self.eps**2))\n",
    "    # x = (x+self.eps*np.random.normal(mean=0, std=np.sqrt(1/self.dim), size=(x.shape[0],1)))/(np.sqrt(1+self.eps**2))\n",
    "    return x\n",
    "\n",
    "class IterDataset(IterableDataset):\n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.generator()\n",
    "    \n",
    "    \n",
    "class MultiTaskSamplingLoader(SamplingLoader):\n",
    "  \n",
    "    def __init__(self, conf, dataset):\n",
    "      self.dataset = dataset\n",
    "      self.mu, self.labels = self.dataset.mu, self.dataset.labels\n",
    "      self.data_type = conf.data_type\n",
    "      self.num_seq = conf.num_seq\n",
    "      self.alpha = conf.alpha\n",
    "      self.num_classes = conf.num_classes\n",
    "      self.num_labels = conf.num_labels\n",
    "      self.ways = conf.ways\n",
    "      self.p_bursty = conf.p_bursty\n",
    "      self.eps = conf.eps\n",
    "      self.dim = conf.dim\n",
    "      self.num_holdout_classes = conf.num_holdout_classes\n",
    "      self.holdout_classes = np.arange(self.num_classes-self.num_holdout_classes, self.num_classes)\n",
    "      self.burstiness_class = conf.burstiness_classes\n",
    "      if self.ways != 0:\n",
    "        assert self.num_seq % self.ways ==0\n",
    "      if self.ways == 0:\n",
    "        self.p_bursty = 0\n",
    "      prob = np.array([1/((k+1)**self.alpha) for k in range(self.num_classes-self.num_holdout_classes)])\n",
    "      self.prob = prob/prob.sum()\n",
    "      \n",
    "      # multi task\n",
    "      self.num_tasks = conf.num_tasks\n",
    "      self.num_seq_per_task = conf.num_seq_per_task\n",
    "    \n",
    "    def get_seq(self):\n",
    "      while True:\n",
    "        if self.data_type==\"bursty\":\n",
    "          if self.p_bursty > np.random.rand():\n",
    "              examples = []\n",
    "              labels = []\n",
    "              classes = []\n",
    "              tasks = np.random.choice(self.num_tasks, size=self.num_tasks)\n",
    "              for task_idx in tasks:\n",
    "                # choise few shot example\n",
    "                num_few_shot_class = self.num_seq_per_task//self.ways\n",
    "                few_shot_class = np.random.choice(self.num_classes-self.num_holdout_classes, num_few_shot_class, replace=False)\n",
    "                mus = self.mu[few_shot_class]\n",
    "                mus = np.repeat(mus, self.ways, axis=0)\n",
    "                tmp_labels = self.labels[few_shot_class]\n",
    "                tmp_labels = np.repeat(tmp_labels, self.ways, axis=0)\n",
    "                tmp_classes = np.repeat(few_shot_class, self.ways)\n",
    "                # change labels for multi task\n",
    "                tmp_labels = (tmp_labels + task_idx) % self.num_labels\n",
    "                # add noise\n",
    "                mus = self.add_noise(mus)\n",
    "                # permutation shuffle\n",
    "                ordering = np.random.permutation(self.num_seq_per_task)\n",
    "                mus = mus[ordering]\n",
    "                tmp_labels = tmp_labels[ordering]\n",
    "                tmp_classes = tmp_classes[ordering]\n",
    "                # select query labels\n",
    "                query_class = np.random.choice(few_shot_class, 1)\n",
    "                query_label = (self.labels[query_class] + task_idx) % self.num_labels\n",
    "                query_mu = self.mu[query_class]\n",
    "                query_x = self.add_noise(query_mu)\n",
    "                # concat\n",
    "                mus = torch.cat([mus, query_x])\n",
    "                tmp_labels = torch.cat([tmp_labels.flatten(), query_label.flatten()])\n",
    "                tmp_classes = torch.cat([torch.from_numpy(tmp_classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
    "                \n",
    "                #append\n",
    "                examples.append(mus)\n",
    "                labels.append(tmp_labels)\n",
    "                classes.append(tmp_classes)\n",
    "                \n",
    "              yield {\n",
    "                  \"task\" : tasks,\n",
    "                  \"examples\": torch.stack(examples).to(torch.float32),\n",
    "                  \"labels\": torch.stack(labels),\n",
    "                  \"classes\" : torch.stack(classes)\n",
    "              }\n",
    "            \n",
    "          else:\n",
    "              examples = []\n",
    "              labels = []\n",
    "              classes = []\n",
    "              tasks = np.random.choice(self.num_tasks, size=self.num_tasks)\n",
    "              for task_idx in tasks:\n",
    "                # rank frequency\n",
    "                tmp_classes = np.random.choice(self.num_classes-self.num_holdout_classes, self.num_seq_per_task+1, p=self.prob)\n",
    "                mus = self.mu[classes]\n",
    "                tmp_labels = self.labels[classes]\n",
    "                mus = self.add_noise(mus)\n",
    "                # permutation shuffle\n",
    "                ordering = np.random.permutation(self.num_seq_per_task+1)\n",
    "                mus = mus[ordering]\n",
    "                tmp_labels = tmp_labels[ordering]\n",
    "                tmp_classes = tmp_classes[ordering]\n",
    "              \n",
    "                #append\n",
    "                examples.append(mus)\n",
    "                labels.append(tmp_labels.flatten())\n",
    "                classes.append(torch.from_numpy(tmp_classes))\n",
    "              yield {\n",
    "                  \"task\" : tasks,\n",
    "                  \"examples\": torch.stack(examples).to(torch.float32),\n",
    "                  \"labels\": torch.stack(labels),\n",
    "                  \"classes\" : torch.stack(classes)\n",
    "              }\n",
    "            \n",
    "        elif self.data_type == \"no_support\":\n",
    "            examples = []\n",
    "            labels = []\n",
    "            classes = []\n",
    "            tasks = np.random.choice(self.num_tasks, size=self.num_tasks)\n",
    "            for task_idx in tasks:\n",
    "              # rank frequency\n",
    "              tmp_classes = np.random.choice(self.num_classes-self.num_holdout_classes, self.num_seq_per_task+1, p=self.prob)\n",
    "              mus = self.mu[tmp_classes]\n",
    "              tmp_labels = self.labels[tmp_classes]\n",
    "              mus = self.add_noise(mus)\n",
    "              # permutation shuffle\n",
    "              ordering = np.random.permutation(self.num_seq_per_task+1)\n",
    "              mus = mus[ordering]\n",
    "              tmp_labels = tmp_labels[ordering]\n",
    "              tmp_classes = tmp_classes[ordering]\n",
    "\n",
    "              #append\n",
    "              examples.append(mus)\n",
    "              labels.append(tmp_labels.flatten())\n",
    "              classes.append(torch.from_numpy(tmp_classes))\n",
    "            yield {\n",
    "                \"task\" : tasks,\n",
    "                \"examples\": torch.stack(examples).to(torch.float32),\n",
    "                \"labels\": torch.stack(labels),\n",
    "                \"classes\" : torch.stack(classes)\n",
    "            }\n",
    "        \n",
    "        elif self.data_type == \"holdout\":\n",
    "            if 1 > np.random.rand():\n",
    "              examples = []\n",
    "              labels = []\n",
    "              classes = []\n",
    "              tasks = np.random.choice(self.num_tasks, size=self.num_tasks)\n",
    "              for task_idx in tasks:\n",
    "                # choise few shot example\n",
    "                num_few_shot_class = self.num_seq_per_task//self.ways\n",
    "                few_shot_class = np.random.choice(self.holdout_classes, num_few_shot_class, replace=False)\n",
    "                mus = self.mu[few_shot_class]\n",
    "                mus = np.repeat(mus, self.ways, axis=0)\n",
    "                tmp_labels = self.labels[few_shot_class]\n",
    "                tmp_labels = np.repeat(tmp_labels, self.ways, axis=0)\n",
    "                tmp_classes = np.repeat(few_shot_class, self.ways)\n",
    "                # change labels for multi task\n",
    "                tmp_labels = (tmp_labels + task_idx) % self.num_labels\n",
    "                # add noise\n",
    "                mus = self.add_noise(mus)\n",
    "                # permutation shuffle\n",
    "                ordering = np.random.permutation(self.num_seq_per_task)\n",
    "                mus = mus[ordering]\n",
    "                tmp_labels = tmp_labels[ordering]\n",
    "                tmp_classes = tmp_classes[ordering]\n",
    "                # select query labels\n",
    "                query_class = np.random.choice(few_shot_class, 1)\n",
    "                query_label = (self.labels[query_class] + task_idx) % self.num_labels\n",
    "                query_mu = self.mu[query_class]\n",
    "                query_x = self.add_noise(query_mu)\n",
    "                # concat\n",
    "                mus = torch.cat([mus, query_x])\n",
    "                tmp_labels = torch.cat([tmp_labels.flatten(), query_label.flatten()])\n",
    "                tmp_classes = torch.cat([torch.from_numpy(tmp_classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
    "                #append\n",
    "                examples.append(mus)\n",
    "                labels.append(tmp_labels)\n",
    "                classes.append(tmp_classes)\n",
    "              yield {\n",
    "                  \"task\" : tasks,\n",
    "                  \"examples\": torch.stack(examples).to(torch.float32),\n",
    "                  \"labels\": torch.stack(labels),\n",
    "                  \"classes\" : torch.stack(classes)\n",
    "              }\n",
    "              \n",
    "            \n",
    "            else:\n",
    "              examples = []\n",
    "              labels = []\n",
    "              classes = []\n",
    "              tasks = np.random.choice(self.num_tasks, size=self.num_tasks)\n",
    "              for task_idx in tasks:\n",
    "                # rank frequency\n",
    "                classes = np.random.choice(self.holdout_classes, self.num_seq_per_task)\n",
    "                mus = self.mu[classes]\n",
    "                labels = self.labels[classes]\n",
    "                mus = self.add_noise(mus)\n",
    "                # permutation shuffle\n",
    "                ordering = np.random.permutation(self.num_seq_per_task)\n",
    "                mus = mus[ordering]\n",
    "                labels = labels[ordering]\n",
    "                classes = classes[ordering]\n",
    "                # query\n",
    "                query_class = np.random.choice(classes, 1)\n",
    "                query_label = (self.labels[query_class]+task_idx) % self.num_labels\n",
    "                query_mu = self.mu[query_class]\n",
    "                query_x = self.add_noise(query_mu)\n",
    "                \n",
    "                mus = torch.cat([mus, query_x])\n",
    "                labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
    "                classes = torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class)])\n",
    "                \n",
    "                #append\n",
    "                examples.append(mus)\n",
    "                labels.append(labels)\n",
    "                classes.append(classes)\n",
    "              yield {\n",
    "                  \"task\" : tasks,\n",
    "                  \"examples\": torch.stack(examples).to(torch.float32),\n",
    "                  \"labels\": torch.stack(labels),\n",
    "                  \"classes\" : torch.stack(classes)\n",
    "              }\n",
    "              \n",
    "        elif self.data_type == \"flip\":\n",
    "            if 1 > np.random.rand():\n",
    "              examples = []\n",
    "              labels = []\n",
    "              classes = []\n",
    "              tasks = np.random.choice(self.num_tasks, size=self.num_tasks)\n",
    "              for task_idx in tasks:\n",
    "                # choise few shot example\n",
    "                num_few_shot_class = self.num_seq_per_task//self.ways\n",
    "                few_shot_class = np.random.choice(self.burstiness_class, num_few_shot_class, replace=False)\n",
    "                mus = self.mu[few_shot_class]\n",
    "                mus = np.repeat(mus, self.ways, axis=0)\n",
    "                tmp_classes = np.repeat(few_shot_class, self.ways)\n",
    "                # label flip\n",
    "                tmp_labels = (self.labels[tmp_classes] + task_idx) % self.num_labels\n",
    "                # add noise\n",
    "                mus = self.add_noise(mus)\n",
    "                # permutation shuffle\n",
    "                ordering = np.random.permutation(self.num_seq_per_task)\n",
    "                mus = mus[ordering]\n",
    "                tmp_labels = tmp_labels[ordering]\n",
    "                tmp_classes = tmp_classes[ordering]\n",
    "                # select query labels\n",
    "                query_class = np.random.choice(few_shot_class, 1)\n",
    "                query_label = (self.labels[query_class] + task_idx) % self.num_labels\n",
    "                query_mu = self.mu[query_class]\n",
    "                query_x = self.add_noise(query_mu)\n",
    "                # concat\n",
    "                mus = torch.cat([mus, query_x])\n",
    "                tmp_labels = torch.cat([tmp_labels.flatten(), query_label.flatten()])\n",
    "                tmp_classes = torch.cat([torch.from_numpy(tmp_classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
    "                #append\n",
    "                examples.append(mus)\n",
    "                labels.append(tmp_labels)\n",
    "                classes.append(tmp_classes)\n",
    "              yield {\n",
    "                  \"task\" : tasks,\n",
    "                  \"examples\": torch.stack(examples).to(torch.float32),\n",
    "                  \"labels\": torch.stack(labels),\n",
    "                  \"classes\" : torch.stack(classes)\n",
    "              }\n",
    "            \n",
    "            else:\n",
    "              examples = []\n",
    "              labels = []\n",
    "              classes = []\n",
    "              tasks = np.random.choice(self.num_tasks, size=self.num_tasks)\n",
    "              for task_idx in tasks:\n",
    "                # rank frequency\n",
    "                classes = np.random.choice(self.num_classes-self.num_holdout_classes, self.num_seq_per_task)\n",
    "                mus = self.mu[classes]\n",
    "                # label flip\n",
    "                labels = (self.labels[classes] + task_idx) % self.num_labels\n",
    "                mus = self.add_noise(mus)\n",
    "                # permutation shuffle\n",
    "                ordering = np.random.permutation(self.num_seq_per_task)\n",
    "                mus = mus[ordering]\n",
    "                labels = labels[ordering]\n",
    "                classes = classes[ordering]\n",
    "                # query\n",
    "                query_class = np.random.choice(classes, 1)\n",
    "                query_label = (self.labels[query_class]+task_idx) % self.num_labels\n",
    "                query_mu = self.mu[query_class]\n",
    "                query_x = self.add_noise(query_mu)\n",
    "                mus = torch.cat([mus, query_x])\n",
    "                labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
    "                classes = torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class)])\n",
    "                #append\n",
    "                examples.append(mus)\n",
    "                labels.append(labels)\n",
    "                classes.append(classes)\n",
    "              yield {\n",
    "                  \"task\" : tasks,\n",
    "                  \"examples\": torch.stack(examples).to(torch.float32),\n",
    "                  \"labels\": torch.stack(labels),\n",
    "                  \"classes\" : torch.stack(classes)\n",
    "              }\n",
    "                \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "  num_layers: int = 2\n",
    "  d_vocab: int = 32 + 3\n",
    "  d_model: int = 128\n",
    "  d_mlp: int = 128\n",
    "  d_head: int = 128\n",
    "  num_heads: int = 1\n",
    "  n_ctx: int = int((9*2+1)*3-1)\n",
    "  act_type: str = \"ReLU\"\n",
    "  use_cache: bool = False\n",
    "  use_ln: bool = True\n",
    "  p_dim: int = 65\n",
    "  d_emb: int = 128\n",
    "  num_tasks:int = 3\n",
    "  num_seq_per_task:int = 8\n",
    "\n",
    "@dataclass\n",
    "class TrainDataConfig:\n",
    "  num_classes: int = 6\n",
    "  dim: int = 63\n",
    "  num_labels: int = 6\n",
    "  eps: float = 0.1\n",
    "  alpha: float = 0\n",
    "  ways: int = 4\n",
    "  num_seq: int = 8*3\n",
    "  burstiness_classes: int = range(0, num_seq//ways)\n",
    "  p_bursty: float = 1\n",
    "  data_type: str = \"bursty\" # bursty, holdout, no_support, flip\n",
    "  num_holdout_classes: int = 2\n",
    "  num_tasks: int = 3\n",
    "  num_seq_per_task: int = 8\n",
    "\n",
    "@dataclass\n",
    "class IWLDataConfig(TrainDataConfig):\n",
    "  data_type: str = \"no_support\" # bursty, holdout, no_support, flip\n",
    "\n",
    "@dataclass\n",
    "class ICLDataConfig(TrainDataConfig):\n",
    "  data_type: str = \"holdout\" # bursty, holdout, no_support, flip\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ICL2DataConfig(TrainDataConfig):\n",
    "  data_type: str = \"flip\" # bursty, holdout, no_support, flip\n",
    "  \n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "  batch_size: int = 2\n",
    "  optimize_step: int = int(5e4)\n",
    "  lr: float = 0.01\n",
    "  optimizer: str = \"sgd\" # adam, sgd, adamw\n",
    "\n",
    "@dataclass\n",
    "class MainConfig:\n",
    "  traindataconfig : TrainDataConfig = TrainDataConfig()\n",
    "  icldataconfig: ICLDataConfig = ICLDataConfig()\n",
    "  iwldataconfig: IWLDataConfig = IWLDataConfig()\n",
    "  icl2dataconfig: ICL2DataConfig = ICL2DataConfig()\n",
    "  modelconfig: TransformerConfig = TransformerConfig()\n",
    "  trainconfig: TrainConfig = TrainConfig()\n",
    "  device: str = \"cuda:1\"\n",
    "# define config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "task torch.Size([2, 3])\n",
      "train_class torch.Size([2, 3, 9])\n",
      "train_label torch.Size([2, 3, 9])\n",
      "ICL\n",
      "task tensor([[2, 1, 2],\n",
      "        [0, 2, 1]])\n",
      "train_class tensor([[[4, 4, 5, 5, 4, 5, 5, 4, 5],\n",
      "         [4, 4, 5, 4, 5, 5, 5, 4, 5],\n",
      "         [4, 5, 4, 4, 5, 5, 5, 4, 4]],\n",
      "\n",
      "        [[4, 4, 5, 4, 5, 5, 4, 5, 4],\n",
      "         [4, 4, 4, 5, 5, 5, 4, 5, 4],\n",
      "         [4, 4, 4, 5, 5, 5, 5, 4, 5]]])\n",
      "train_label tensor([[[5, 5, 0, 0, 5, 0, 0, 5, 0],\n",
      "         [4, 4, 5, 4, 5, 5, 5, 4, 5],\n",
      "         [5, 0, 5, 5, 0, 0, 0, 5, 5]],\n",
      "\n",
      "        [[3, 3, 4, 3, 4, 4, 3, 4, 3],\n",
      "         [5, 5, 5, 0, 0, 0, 5, 0, 5],\n",
      "         [4, 4, 4, 5, 5, 5, 5, 4, 5]]])\n",
      "IWL\n",
      "example torch.Size([2, 3, 9, 63])\n",
      "task tensor([[1, 1, 2],\n",
      "        [2, 0, 2]])\n",
      "train_class tensor([[[2, 1, 2, 1, 1, 2, 1, 1, 0],\n",
      "         [1, 1, 2, 3, 2, 3, 3, 1, 3],\n",
      "         [0, 2, 2, 2, 0, 3, 3, 1, 3]],\n",
      "\n",
      "        [[0, 3, 1, 1, 3, 1, 1, 3, 2],\n",
      "         [2, 1, 3, 3, 3, 1, 3, 2, 3],\n",
      "         [0, 0, 3, 1, 3, 2, 2, 3, 0]]])\n",
      "train_label tensor([[[4, 0, 4, 0, 0, 4, 0, 0, 4],\n",
      "         [0, 0, 4, 0, 4, 0, 0, 0, 0],\n",
      "         [4, 4, 4, 4, 4, 0, 0, 0, 0]],\n",
      "\n",
      "        [[4, 0, 0, 0, 0, 0, 0, 0, 4],\n",
      "         [4, 0, 0, 0, 0, 0, 0, 4, 0],\n",
      "         [4, 4, 0, 0, 0, 4, 4, 0, 4]]])\n",
      "ICL2\n",
      "task tensor([[2, 2, 1],\n",
      "        [1, 0, 1]])\n",
      "train_class tensor([[[0, 0, 0, 2, 2, 2, 0, 2, 0],\n",
      "         [2, 2, 2, 0, 0, 2, 0, 0, 2],\n",
      "         [5, 5, 0, 5, 0, 0, 0, 5, 5]],\n",
      "\n",
      "        [[1, 2, 1, 2, 2, 2, 1, 1, 1],\n",
      "         [1, 3, 3, 1, 1, 3, 1, 3, 3],\n",
      "         [4, 4, 4, 2, 2, 4, 2, 2, 2]]])\n",
      "train_label tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [5, 5, 5, 5, 5, 5, 5, 5, 5]],\n",
      "\n",
      "        [[1, 5, 1, 5, 5, 5, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [4, 4, 4, 5, 5, 4, 5, 5, 5]]])\n"
     ]
    }
   ],
   "source": [
    "traindataconfig = MainConfig.traindataconfig\n",
    "icldataconfig = MainConfig.icldataconfig\n",
    "iwldataconfig = MainConfig.iwldataconfig\n",
    "icl2dataconfig = MainConfig.icl2dataconfig\n",
    "trainconfig = MainConfig.trainconfig\n",
    "\n",
    "Dataset = SamplingDataset(traindataconfig)\n",
    "\n",
    "trainloader = MultiTaskSamplingLoader(traindataconfig, dataset=Dataset)\n",
    "train_seq_generator = trainloader.get_seq\n",
    "train_dataset = IterDataset(train_seq_generator)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "iclloader = MultiTaskSamplingLoader(icldataconfig, dataset=Dataset)\n",
    "icl_seq_generator = iclloader.get_seq\n",
    "icl_dataset = IterDataset(icl_seq_generator)\n",
    "icl_dataloader = torch.utils.data.DataLoader(icl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "iwlloader = MultiTaskSamplingLoader(iwldataconfig, dataset=Dataset)\n",
    "iwl_seq_generator = iwlloader.get_seq\n",
    "iwl_dataset = IterDataset(iwl_seq_generator)\n",
    "iwl_dataloader = torch.utils.data.DataLoader(iwl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "icl2loader = MultiTaskSamplingLoader(icl2dataconfig, dataset=Dataset)\n",
    "icl2_seq_generator = icl2loader.get_seq\n",
    "icl2_dataset = IterDataset(icl2_seq_generator)\n",
    "icl2_dataloader = torch.utils.data.DataLoader(icl2_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "print(\"Train\")\n",
    "cnt = 0\n",
    "for data in train_dataloader:\n",
    "    # print(data.shape)\n",
    "    task = data[\"task\"]\n",
    "    examples = data[\"examples\"]\n",
    "    labels = data[\"labels\"]\n",
    "    classes = data[\"classes\"]\n",
    "    # print(examples)\n",
    "    print(\"task\", task.shape)\n",
    "    print(\"train_class\", classes.shape)\n",
    "    print(\"train_label\", labels.shape)\n",
    "    cnt += 1\n",
    "    if cnt > 0:\n",
    "        break\n",
    "print(\"ICL\")\n",
    "cnt = 0\n",
    "for data in icl_dataloader:\n",
    "    task = data[\"task\"]\n",
    "    examples = data[\"examples\"]\n",
    "    labels = data[\"labels\"]\n",
    "    classes = data[\"classes\"]\n",
    "    # print(examples)\n",
    "    print(\"task\", task)\n",
    "    print(\"train_class\", classes)\n",
    "    print(\"train_label\", labels)\n",
    "    cnt += 1\n",
    "    if cnt > 0:\n",
    "        break\n",
    "print(\"IWL\")\n",
    "cnt = 0\n",
    "for data in iwl_dataloader:\n",
    "    task = data[\"task\"]\n",
    "    examples = data[\"examples\"]\n",
    "    labels = data[\"labels\"]\n",
    "    classes = data[\"classes\"]\n",
    "    print(\"example\",examples.shape)\n",
    "    print(\"task\", task)\n",
    "    print(\"train_class\", classes)\n",
    "    print(\"train_label\", labels)\n",
    "    cnt += 1\n",
    "    if cnt > 0:\n",
    "        break\n",
    "print(\"ICL2\")\n",
    "cnt = 0\n",
    "for data in icl2_dataloader:\n",
    "    task = data[\"task\"]\n",
    "    examples = data[\"examples\"]\n",
    "    labels = data[\"labels\"]\n",
    "    classes = data[\"classes\"]\n",
    "    # print(examples)\n",
    "    print(\"task\", task)\n",
    "    print(\"train_class\", classes)\n",
    "    print(\"train_label\", labels)\n",
    "    cnt += 1\n",
    "    if cnt > 0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import einops\n",
    "\n",
    "\n",
    "class Unembed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(d_vocab, d_model) / np.sqrt(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.einsum(\"pe,bse->bsp\", self.W, x)\n",
    "\n",
    "\n",
    "class HookPoint(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fwd_hooks = []\n",
    "        self.bwd_hooks = []\n",
    "\n",
    "    def give_name(self, name):\n",
    "        # Called by the model at initialisation\n",
    "        self.name = name\n",
    "\n",
    "    def add_hook(self, hook, dir=\"fwd\"):\n",
    "        # Hook format is fn(activation, hook_name)\n",
    "        # Change it into PyTorch hook format (this includes input and output,\n",
    "        # which are the same for a HookPoint)\n",
    "        def full_hook(module, module_input, module_output):\n",
    "            return hook(module_output, name=self.name)\n",
    "\n",
    "        if dir == \"fwd\":\n",
    "            handle = self.register_forward_hook(full_hook)\n",
    "            self.fwd_hooks.append(handle)\n",
    "        elif dir == \"bwd\":\n",
    "            handle = self.register_backward_hook(full_hook)\n",
    "            self.bwd_hooks.append(handle)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid direction {dir}\")\n",
    "\n",
    "    def remove_hooks(self, dir=\"fwd\"):\n",
    "        if (dir == \"fwd\") or (dir == \"both\"):\n",
    "            for hook in self.fwd_hooks:\n",
    "                hook.remove()\n",
    "            self.fwd_hooks = []\n",
    "        if (dir == \"bwd\") or (dir == \"both\"):\n",
    "            for hook in self.bwd_hooks:\n",
    "                hook.remove()\n",
    "            self.bwd_hooks = []\n",
    "        if dir not in [\"fwd\", \"bwd\", \"both\"]:\n",
    "            raise ValueError(f\"Invalid direction {dir}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, max_ctx, d_model, weight_scale=1):\n",
    "        super().__init__()\n",
    "        self.W_pos = nn.Parameter(torch.randn(max_ctx, d_model) * weight_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.W_pos[: x.shape[-2]]\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, epsilon=1e-4, model=[None]):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.w_ln = nn.Parameter(torch.ones(d_model))\n",
    "        self.b_ln = nn.Parameter(torch.zeros(d_model))\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.model[0].use_ln:\n",
    "            x = x - x.mean(axis=-1)[..., None]\n",
    "            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n",
    "            x = x * self.w_ln\n",
    "            x = x + self.b_ln\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "# Attention\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    b : batch size\n",
    "    d : embedding size of token\n",
    "    p : vocabraly size (113 or 3)\n",
    "    i : number of heads\n",
    "    h : embedding size of each heads\n",
    "    n_ctx : token size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_head, n_ctx):\n",
    "        super().__init__()\n",
    "        self.W_K = nn.Parameter(\n",
    "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
    "        )\n",
    "        self.W_Q = nn.Parameter(\n",
    "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
    "        )\n",
    "        self.W_V = nn.Parameter(\n",
    "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
    "        )\n",
    "        self.W_O = nn.Parameter(\n",
    "            torch.randn(d_model, d_head * num_heads) / np.sqrt(d_model)\n",
    "        )\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones((n_ctx, n_ctx))))\n",
    "        self.d_head = d_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = torch.einsum(\"ihd,bpd->biph\", self.W_K, x)\n",
    "        q = torch.einsum(\"ihd,bpd->biph\", self.W_Q, x)\n",
    "        v = torch.einsum(\"ihd,bpd->biph\", self.W_V, x)\n",
    "        attn_scores_pre = torch.einsum(\"biph,biqh->biqp\", k, q)\n",
    "        attn_scores_masked = torch.tril(attn_scores_pre) - 1e10 * (\n",
    "            1 - self.mask[: x.shape[-2], : x.shape[-2]]\n",
    "        )\n",
    "        attn_matrix = F.softmax(attn_scores_masked / np.sqrt(self.d_head), dim=-1)\n",
    "        z = torch.einsum(\"biph,biqp->biqh\", v, attn_matrix)\n",
    "        z_flat = einops.rearrange(z, \"b i q h -> b q (i h)\")\n",
    "        out = torch.einsum(\"df,bqf->bqd\", self.W_O, z_flat)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    def __init__(self, d_in, d_out, act_type, weight_scale=1):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(d_out, d_in))\n",
    "        torch.nn.init.normal_(self.W, mean=0, std=weight_scale / np.sqrt(d_in))\n",
    "\n",
    "    def set_weight_ratio(self, weight_ratio):\n",
    "        self.W = nn.Parameter(self.W * weight_ratio)\n",
    "\n",
    "    def set_weight_ratio_l2(self, weight_ratio):\n",
    "        self.W = nn.Parameter(self.W * torch.sqrt(weight_ratio))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.W.T\n",
    "\n",
    "\n",
    "# for Transformer\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    b : batch size\n",
    "    d : embedding size of token\n",
    "    p : vocabraly size (114 or 3)\n",
    "    i : number of heads\n",
    "    h : embedding size of each heads\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_mlp, act_type):\n",
    "        super().__init__()\n",
    "        # bias & layer norm are removed.\n",
    "        self.W_in = nn.Parameter(torch.randn(d_mlp, d_model) / np.sqrt(d_model))\n",
    "        self.b_in = nn.Parameter(torch.zeros(d_mlp))\n",
    "        self.W_out = nn.Parameter(torch.randn(d_model, d_mlp) / np.sqrt(d_model))\n",
    "        self.b_out = nn.Parameter(torch.zeros(d_model))\n",
    "        self.act_type = act_type\n",
    "        # self.ln = LayerNorm(d_mlp, model=self.model)\n",
    "        assert act_type in [\"ReLU\", \"GeLU\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.einsum(\"md,bpd->bpm\", self.W_in, x) + self.b_in\n",
    "        if self.act_type == \"ReLU\":\n",
    "            x = F.relu(x)\n",
    "        elif self.act_type == \"GeLU\":\n",
    "            x = F.gelu(x)\n",
    "        x = torch.einsum(\"dm,bpm->bpd\", self.W_out, x) + self.b_out\n",
    "        return x\n",
    "\n",
    "    def set_weight_ratio(self, weight_ratio):\n",
    "        self.W_in = nn.Parameter(self.W_in * weight_ratio)\n",
    "        self.W_out = nn.Parameter(self.W_out * weight_ratio)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    b : batch size\n",
    "    d : embedding size of token\n",
    "    p : vocabraly size\n",
    "    i : number of heads\n",
    "    h : embedding size of each heads\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n",
    "        super().__init__()\n",
    "        # self.ln1 = LayerNorm(d_model, model=self.model)\n",
    "        self.model = model\n",
    "        self.attn = Attention(d_model, num_heads, d_head, n_ctx)\n",
    "        # self.ln2 = LayerNorm(d_model, model=self.model)\n",
    "        self.mlp = MLPBlock(d_model, d_mlp, act_type)\n",
    "        self.layer_norm = LayerNorm(d_model, model=self.model)\n",
    "        self.hook_attn_out = HookPoint()\n",
    "        self.hook_mlp_out = HookPoint()\n",
    "        self.hook_resid_pre = HookPoint()\n",
    "        self.hook_resid_mid = HookPoint()\n",
    "        self.hook_resid_post = HookPoint()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hook_resid_mid(\n",
    "            x + self.hook_attn_out(self.attn((self.hook_resid_pre(x))))\n",
    "        )\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.hook_resid_post(x + self.hook_mlp_out(self.mlp((x))))\n",
    "        return x\n",
    "\n",
    "    def set_weight_ratio(self, weight_ratio):\n",
    "        self.attn.set_weight_ratio(weight_ratio)\n",
    "        self.mlp.set_weight_ratio(weight_ratio)\n",
    "\n",
    "\n",
    "class InputEmbedder(nn.Module):\n",
    "    \"\"\"Input embedder.\"\"\"\n",
    "\n",
    "    def __init__(self, conf):\n",
    "\n",
    "        \"\"\"Initialize the input embedder.\n",
    "\n",
    "    Args:\n",
    "      num_classes: Total number of output classes.\n",
    "      emb_dim: Dimensionality of example and label embeddings.\n",
    "      example_encoding: How to encode example inputs.\n",
    "        'resnet': simple resnet encoding\n",
    "        'linear': flatten and pass through a linear layer\n",
    "        'embedding': pass through an embedding layer\n",
    "      flatten_superpixels: Whether to flatten the output of the resnet (instead\n",
    "        of taking a mean over superpixels).\n",
    "      example_dropout_prob: Dropout probability on example embeddings. Note that\n",
    "        these are applied at both train and test.\n",
    "      concatenate_labels: Whether to concatenate example and label embeddings\n",
    "        into one token for each (example, label) pair, rather than being fed to\n",
    "        the transformer as two separate tokens.\n",
    "      use_positional_encodings: Whether to use positional encoding.\n",
    "      positional_dropout_prob: Positional dropout probability.\n",
    "      name: Optional name for the module.\n",
    "    \"\"\"\n",
    "        super(InputEmbedder, self).__init__()\n",
    "        self._num_classes = conf.d_vocab\n",
    "        self._emb_dim = conf.d_emb\n",
    "        self.p_dim = conf.p_dim\n",
    "        self.num_tasks = conf.num_tasks\n",
    "        self.num_seq_per_task = conf.num_seq_per_task\n",
    "\n",
    "        self.Emb = nn.Linear(self._emb_dim, self._emb_dim)\n",
    "\n",
    "        self.label_embs = nn.Parameter(\n",
    "            torch.randn(self._num_classes, self._emb_dim) / np.sqrt(self._emb_dim)\n",
    "        )\n",
    "        \n",
    "        self.task_embs = nn.Parameter(\n",
    "            torch.randn(self.num_tasks, self._emb_dim) / np.sqrt(self._emb_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, examples, labels, tasks):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            examples (_type_): _description_\n",
    "            labels (_type_): _description_\n",
    "            tasks (_type_): _description_\n",
    "            is_training (bool): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # Encode the example inputs into shape (B, T, SS, E)\n",
    "        B, T, SS, D = examples.shape\n",
    "        examples = examples.view(B, T*SS, D)\n",
    "        # pos encoding\n",
    "        pos_enc = F.one_hot(torch.arange(T*SS), num_classes=self.p_dim).repeat(B,1,1).to(examples.device)\n",
    "        h_example = torch.cat([examples, pos_enc], dim=2) # (B, T*SS, E)\n",
    "        \n",
    "        # Embed the labels. (B, T, SS, 1) -> (B, T*SS, E)\n",
    "        h_label = self.label_embs[labels]  # (B, T, SS, E)\n",
    "        h_label = h_label.view(B, T*SS, self._emb_dim) #(B, T*SS, E)\n",
    "        \n",
    "        # task embedding (B, T) -> (B, T, 1, E)\n",
    "        task_embs = self.task_embs[tasks] # (B, T, E)\n",
    "        \n",
    "        hh = torch.empty(\n",
    "            (B, (SS * 2 +1) * T ,  h_example.shape[2]), # (B, S, E),  S = T*(SS*2 + task) \n",
    "            dtype=h_example.dtype, \n",
    "        ).to(h_example.device)\n",
    "        hh[:, 0::(SS*2+1)] = task_embs\n",
    "        for t in range(T):\n",
    "            hh[:, t*(SS*2+1)+1: t*(SS*2+1)+1 + SS*2:2] = h_example[:, t*SS:(t+1)*SS]\n",
    "            hh[:, t*(SS*2+1)+2:t*(SS*2+1)+1 + SS*2:2] = h_label[:, t*SS:(t+1)*SS]\n",
    "\n",
    "        # last label remove\n",
    "        hh = hh[:, :-1]\n",
    "        \n",
    "\n",
    "        return hh\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedder, config):\n",
    "        super().__init__()\n",
    "        num_layers = config.num_layers\n",
    "        d_model = config.d_emb\n",
    "        d_mlp = config.d_emb * 4\n",
    "        d_head = config.d_emb // config.num_heads\n",
    "        num_heads = config.num_heads\n",
    "        n_ctx = config.n_ctx\n",
    "        act_type = config.act_type\n",
    "        use_cache = config.use_cache\n",
    "        use_ln = config.use_ln\n",
    "        self.cache = {}\n",
    "        self.use_cache = use_cache\n",
    "        d_vocab = config.d_vocab\n",
    "\n",
    "        self.embedder = embedder\n",
    "        # self.pos_embed = PosEmbed(n_ctx, d_model)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model=[self]\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        # self.ln = LayerNorm(d_model, model=[self])\n",
    "        self.unembed = Unembed(d_vocab, d_model)\n",
    "        self.use_ln = use_ln\n",
    "\n",
    "        for name, module in self.named_modules():\n",
    "            if type(module) == HookPoint:\n",
    "                module.give_name(name)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        x = self.embedder(x, labels,)\n",
    "        # x = self.pos_embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.unembed(x)\n",
    "        return x\n",
    "\n",
    "    def set_use_cache(self, use_cache):\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "    def hook_points(self):\n",
    "        return [module for name, module in self.named_modules() if \"hook\" in name]\n",
    "\n",
    "    def remove_all_hooks(self):\n",
    "        for hp in self.hook_points():\n",
    "            hp.remove_hooks(\"fwd\")\n",
    "            hp.remove_hooks(\"bwd\")\n",
    "\n",
    "    def cache_all(self, cache, incl_bwd=False):\n",
    "        # Caches all activations wrapped in a HookPoint\n",
    "        def save_hook(tensor, name):\n",
    "            cache[name] = tensor.detach()\n",
    "\n",
    "        def save_hook_back(tensor, name):\n",
    "            cache[name + \"_grad\"] = tensor[0].detach()\n",
    "\n",
    "        for hp in self.hook_points():\n",
    "            hp.add_hook(save_hook, \"fwd\")\n",
    "            if incl_bwd:\n",
    "                hp.add_hook(save_hook_back, \"bwd\")\n",
    "\n",
    "class TransformerICL(nn.Module):\n",
    "    def __init__(self, embedder, config):\n",
    "        super().__init__()\n",
    "        num_layers = config.num_layers\n",
    "        d_model = config.d_emb\n",
    "        d_mlp = config.d_emb * 4\n",
    "        d_head = config.d_emb // config.num_heads\n",
    "        num_heads = config.num_heads\n",
    "        n_ctx = config.n_ctx\n",
    "        act_type = config.act_type\n",
    "        use_cache = config.use_cache\n",
    "        use_ln = config.use_ln\n",
    "        self.cache = {}\n",
    "        self.use_cache = use_cache\n",
    "        d_vocab = config.d_vocab\n",
    "\n",
    "        self.embedder = embedder\n",
    "        # self.pos_embed = PosEmbed(n_ctx, d_model)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Attention(d_model, num_heads, d_head, n_ctx),\n",
    "                Attention(d_model, num_heads, d_head, n_ctx),\n",
    "                Dense(d_model, d_model, act_type),\n",
    "                Dense(d_model, d_model, act_type),\n",
    "                Dense(d_model, d_model, act_type),\n",
    "            ]\n",
    "        )\n",
    "        # self.ln = LayerNorm(d_model, model=[self])\n",
    "        self.unembed = Unembed(d_vocab, d_model)\n",
    "        self.use_ln = use_ln\n",
    "\n",
    "        for name, module in self.named_modules():\n",
    "            if type(module) == HookPoint:\n",
    "                module.give_name(name)\n",
    "\n",
    "    def forward(self, examples, labels, tasks):\n",
    "        x = self.embedder(examples, labels, tasks)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.unembed(x)\n",
    "        return x\n",
    "\n",
    "    def set_use_cache(self, use_cache):\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "    def hook_points(self):\n",
    "        return [module for name, module in self.named_modules() if \"hook\" in name]\n",
    "\n",
    "    def remove_all_hooks(self):\n",
    "        for hp in self.hook_points():\n",
    "            hp.remove_hooks(\"fwd\")\n",
    "            hp.remove_hooks(\"bwd\")\n",
    "\n",
    "    def cache_all(self, cache, incl_bwd=False):\n",
    "        # Caches all activations wrapped in a HookPoint\n",
    "        def save_hook(tensor, name):\n",
    "            cache[name] = tensor.detach()\n",
    "\n",
    "        def save_hook_back(tensor, name):\n",
    "            cache[name + \"_grad\"] = tensor[0].detach()\n",
    "\n",
    "        for hp in self.hook_points():\n",
    "            hp.add_hook(save_hook, \"fwd\")\n",
    "            if incl_bwd:\n",
    "                hp.add_hook(save_hook_back, \"bwd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(t,p):\n",
    "    p_arg = torch.argmax(p,dim=1)\n",
    "    return torch.sum(t == p_arg) / p.shape[0]\n",
    "def to_gpu_dict(dic):\n",
    "    dic = {k:v.to(\"cuda:1\") for k,v in dic.items()}\n",
    "    return dic\n",
    "def to_gpu_dict_list(dic_list):\n",
    "    return np.array([to_gpu_dict(dic) for dic in dic_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:g6v9ok7s) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gallant-spaceship-28</strong> at: <a href='https://wandb.ai/gouki/icl-minima/runs/g6v9ok7s' target=\"_blank\">https://wandb.ai/gouki/icl-minima/runs/g6v9ok7s</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240308_083919-g6v9ok7s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:g6v9ok7s). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/induction-head/wandb/run-20240308_084031-3kqwdlud</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gouki/icl-minima/runs/3kqwdlud' target=\"_blank\">likely-brook-29</a></strong> to <a href='https://wandb.ai/gouki/icl-minima' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gouki/icl-minima' target=\"_blank\">https://wandb.ai/gouki/icl-minima</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gouki/icl-minima/runs/3kqwdlud' target=\"_blank\">https://wandb.ai/gouki/icl-minima/runs/3kqwdlud</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1622 0.0 1.0 0.0 0.50.0 0.0 0.0 0.0 0.0 0.00.00.5 1.0"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     50\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (data_dict_list, icl_data_dict_list, iwl_data_dict_list, icl2_data_dict_list) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_dataloader, icl_dataloader, iwl_dataloader, icl2_dataloader):\n\u001b[1;32m     52\u001b[0m   model\u001b[38;5;241m.\u001b[39mtrain()   \n\u001b[1;32m     53\u001b[0m   data_dict \u001b[38;5;241m=\u001b[39m to_gpu_dict(data_dict_list)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:626\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_name):\n\u001b[1;32m    627\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/profiler.py:648\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 648\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_ops.py:448\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "config = MainConfig()\n",
    "wandb.init(project=\"icl-minima\", config=asdict(config))\n",
    "# data\n",
    "traindataconfig = MainConfig.traindataconfig\n",
    "icldataconfig = MainConfig.icldataconfig\n",
    "iwldataconfig = MainConfig.iwldataconfig\n",
    "icl2dataconfig = MainConfig.icl2dataconfig\n",
    "trainconfig = MainConfig.trainconfig\n",
    "\n",
    "Dataset = SamplingDataset(traindataconfig)\n",
    "\n",
    "trainloader = MultiTaskSamplingLoader(traindataconfig, dataset=Dataset)\n",
    "train_seq_generator = trainloader.get_seq\n",
    "train_dataset = IterDataset(train_seq_generator)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "iclloader = MultiTaskSamplingLoader(icldataconfig, dataset=Dataset)\n",
    "icl_seq_generator = iclloader.get_seq\n",
    "icl_dataset = IterDataset(icl_seq_generator)\n",
    "icl_dataloader = torch.utils.data.DataLoader(icl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "iwlloader = MultiTaskSamplingLoader(iwldataconfig, dataset=Dataset)\n",
    "iwl_seq_generator = iwlloader.get_seq\n",
    "iwl_dataset = IterDataset(iwl_seq_generator)\n",
    "iwl_dataloader = torch.utils.data.DataLoader(iwl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "icl2loader = MultiTaskSamplingLoader(icl2dataconfig, dataset=Dataset)\n",
    "icl2_seq_generator = icl2loader.get_seq\n",
    "icl2_dataset = IterDataset(icl2_seq_generator)\n",
    "icl2_dataloader = torch.utils.data.DataLoader(icl2_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "# model\n",
    "embedder = InputEmbedder(config.modelconfig)\n",
    "model = TransformerICL(embedder, config.modelconfig)\n",
    "model.to(config.device)\n",
    "\n",
    "# optimizer\n",
    "if config.trainconfig.optimizer == \"adam\":\n",
    "  optimizer =  torch.optim.Adam(model.parameters(), lr=config.trainconfig.lr)\n",
    "elif config.trainconfig.optimizer == \"sgd\":\n",
    "  optimizer =  torch.optim.SGD(model.parameters(), lr=config.trainconfig.lr)\n",
    "elif config.trainconfig.optimizer == \"adamw\":\n",
    "  optimizer =  torch.optim.AdamW(model.parameters(), lr=config.trainconfig.lr)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "step = 0\n",
    "for (data_dict_list, icl_data_dict_list, iwl_data_dict_list, icl2_data_dict_list) in zip(train_dataloader, icl_dataloader, iwl_dataloader, icl2_dataloader):\n",
    "  model.train()   \n",
    "  data_dict = to_gpu_dict(data_dict_list)\n",
    "  icl_data_dict = to_gpu_dict(icl_data_dict_list)\n",
    "  iwl_data_dict = to_gpu_dict(iwl_data_dict_list)\n",
    "  icl2_data_dict = to_gpu_dict(icl2_data_dict_list)\n",
    "  \n",
    "  # print(\"data_dict\", data_dict)\n",
    "  \n",
    "  logits = model(data_dict[\"examples\"], data_dict[\"labels\"], data_dict[\"task\"])\n",
    "  query_logit = logits[:,-1,:]\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  # print(data_dict[\"labels\"][:,-1])\n",
    "  loss = criterion(query_logit, data_dict[\"labels\"][:,-1,-1],)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  train_acc = cal_acc(data_dict[\"labels\"][:,-1, -1], query_logit)\n",
    "  # print(\"train_sample\", data_dict[\"classes\"], data_dict[\"labels\"])\n",
    "  wandb.log({\"train/acc\":train_acc,\"train/loss\":loss}, step=step)\n",
    "  with torch.no_grad():\n",
    "          model.eval()\n",
    "          logits = model(icl_data_dict[\"examples\"], icl_data_dict[\"labels\"], icl_data_dict[\"task\"])\n",
    "          query_logit = logits[:,-1,:]\n",
    "          icl_acc = cal_acc(icl_data_dict[\"labels\"][:,-1, -1], query_logit)\n",
    "          # wandb.log({\"valid/icl_acc\":icl_acc}, step=step)\n",
    "          # print(\"\\r\",step, icl_acc, end=\"\")\n",
    "          # print(\"icl_sample\", icl_data_dict)\n",
    "\n",
    "          logits = model(iwl_data_dict[\"examples\"], iwl_data_dict[\"labels\"], iwl_data_dict[\"task\"])\n",
    "          query_logit = logits[:,-1,:]\n",
    "          iwl_acc = cal_acc(iwl_data_dict[\"labels\"][:,-1, -1], query_logit)\n",
    "          # wandb.log({\"valid/iwl_acc\":iwl_acc}, step=step)\n",
    "          # print(\"\\r\",step, iwl_acc, end=\"\")\n",
    "          # print(\"iwl_sample\", iwl_data_dict[\"classes\"], iwl_data_dict[\"labels\"])\n",
    "\n",
    "          logits = model(icl2_data_dict[\"examples\"], icl2_data_dict[\"labels\"], icl2_data_dict[\"task\"])\n",
    "          query_logit = logits[:,-1,:]\n",
    "          icl2_acc = cal_acc(icl2_data_dict[\"labels\"][:,-1, -1], query_logit)\n",
    "          # wandb.log({\"valid/icl2_acc\":icl2_acc}, step=step)\n",
    "          # print(\"\\r\",step, icl2_acc, end=\"\")\n",
    "          # print(\"icl2_sample\", icl2_data_dict[\"classes\"], icl2_data_dict[\"labels\"])\n",
    "          \n",
    "  print(\"\\r\",step, train_acc.item(), iwl_acc.item(), icl_acc.item(), icl2_acc.item(), end=\"\")\n",
    "  step+=1\n",
    "  if step > config.trainconfig.optimize_step:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
